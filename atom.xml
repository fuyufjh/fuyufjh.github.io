<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding Husky</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ericfu.me/"/>
  <updated>2019-06-04T02:19:09.416Z</updated>
  <id>https://ericfu.me/</id>
  
  <author>
    <name>Eric Fu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SQL 子查询的优化</title>
    <link href="https://ericfu.me/subquery-optimization/"/>
    <id>https://ericfu.me/subquery-optimization/</id>
    <published>2019-03-20T09:04:53.000Z</published>
    <updated>2019-06-04T02:19:09.416Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/04/banner-subquery-optimization.png"></p><style type="text/css">.image-captain {    margin-top: -20px;}.marked-code {    color:red;    font-weight:bold;}</style><p><strong>子查询</strong>（Subquery）的优化一直以来都是 SQL 查询优化中的难点之一。关联子查询的基本执行方式类似于 Nested-Loop，但是这种执行方式的效率常常低到难以忍受。当数据量稍大时，必须在优化器中对其进行<strong>去关联化</strong>（Decoorelation 或 Unnesting），将其改写为类似于 Semi-Join 这样的更高效的算子。</p><p>前人已经总结出一套完整的方法论，理论上能对任意一个查询进行去关联化。本文结合 SQL Server 以及 HyPer 的几篇经典论文，由浅入深地讲解一下这套去关联化的理论体系。它们二者所用的方法大同小异，基本思想是想通的。</p><a id="more"></a><p>本文的例子都基于 TPC-H 的表结构，<a href="/images/2019/04/The-TPC-H-Schema.png">这里</a> 有一份供你参考。</p><h2 id="子查询简介">子查询简介</h2><p>子查询是定义在 SQL 标准中一种语法，它可以出现在 SQL 的几乎任何地方，包括 SELECT, FROM, WHERE 等子句中。</p><p>总的来说，子查询可以分为<strong>关联子查询（Correlated Subquery）</strong>和<strong>非关联子查询（Non-correlated Subquery）</strong>。后者非关联子查询是个很简单的问题，最简单地，只要先执行它、得到结果集并物化，再执行外层查询即可。下面是一个例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_count, <span class="keyword">count</span>(*) <span class="keyword">AS</span> custdist</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">     <span class="keyword">SELECT</span> c_custkey, <span class="keyword">count</span>(o_orderkey) <span class="keyword">AS</span> c_count</span><br><span class="line">     <span class="keyword">FROM</span> CUSTOMER</span><br><span class="line">     <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> ORDERS <span class="keyword">ON</span> c_custkey = o_custkey</span><br><span class="line">     <span class="keyword">AND</span> o_comment <span class="keyword">NOT</span> <span class="keyword">LIKE</span> <span class="string">'%pending%deposits%'</span></span><br><span class="line">     <span class="keyword">GROUP</span> <span class="keyword">BY</span> c_custkey</span><br><span class="line">     ) c_orders</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> c_count</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> custdist <span class="keyword">DESC</span>, c_count <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><p class="image-captain">▲ TPCH-13 是一个非关联子查询</p><p><strong>非关联子查询不在本文讨论范围之列</strong>，除非特别声明，以下我们说的子查询都是指关联子查询。</p><p>关联子查询的特别之处在于，其本身是不完整的：<strong>它的闭包中包含一些外层查询提供的参数</strong>。显然，只有知道这些参数才能运行该查询，所以我们不能像对待非关联子查询那样。</p><p>根据产生的数据来分类，子查询可以分成以下几种：</p><p><strong>标量（Scalar-valued）</strong>子查询：输出一个只有一行一列的结果表，这个标量值就是它的结果。如果结果为空（0 行），则输出一个 NULL。但是注意，超过 1 行结果是不被允许的，会产生一个运行时异常。</p><p>标量子查询可以出现在任意包含标量的地方，例如 SELECT、WHERE 等子句里。下面是一个例子：</p><pre><code>SELECT c_custkeyFROM CUSTOMERWHERE 1000000 < (    SELECT SUM(o_totalprice)    FROM ORDERS    WHERE o_custkey = <span class="marked-code">c_custkey</span>)</code></pre><p class="image-captain">▲ Query 1: 一个出现在 WHERE 子句中的标量子查询，关联参数用红色字体标明了</p><pre><code>SELECT o_orderkey, (    SELECT c_name    FROM CUSTOMER    WHERE c_custkey = <span class="marked-code">o_custkey</span>) AS c_name FROM ORDERS</code></pre><p class="image-captain">▲ Query 2: 一个出现在 SELECT 子句中的标量子查询</p><p><strong>存在性检测（Existential Test）</strong>子查询：特指 EXISTS 的子查询，返回一个布尔值。如果出现在 WHERE 中，这就是我们熟悉的 Semi-Join。当然，它可能出现在任何可以放布尔值的地方。</p><pre><code>SELECT c_custkeyFROM CUSTOMERWHERE c_nationkey = 86 AND EXISTS(    SELECT * FROM ORDERS    WHERE o_custkey = <span class="marked-code">c_custkey</span>)</code></pre><p class="image-captain">▲ Query 3: 一个 Semi-Join 的例子</p><p><strong>集合比较（Quantified Comparision）</strong>子查询：特指 IN、SOME、ANY 的查询，返回一个布尔值，常用的形式有：<code>x = SOME(Q)</code> （等价于 <code>x IN Q</code>）或 <code>X &lt;&gt; ALL(Q)</code>（等价于 <code>x NOT IN Q</code>）。同上，它可能出现在任何可以放布尔值的地方。</p><pre><code>SELECT c_nameFROM CUSTOMERWHERE c_nationkey <> ALL (SELECT s_nationkey FROM SUPPLIER)</code></pre><p class="image-captain">▲ Query 4: 一个集合比较的非关联子查询</p><h2 id="原始执行计划">原始执行计划</h2><p>我们以 Query 1 为例，直观地感受一下，为什么说关联子查询的去关联化是十分必要的。</p><p>下面是 Query 1 的未经去关联化的原始查询计划（Relation Tree）。与其他查询计划不一样的是，我们特地画出了表达式树（Expression Tree），可以清晰地看到：子查询是实际上是挂在 Filter 的条件表达式下面的。</p><p><img src="/images/2019/04/subquery-optimization-query-1-relnode-1.png"></p><p>实际执行时，查询计划执行器（Executor）在执行到 Filter 时，调用表达式执行器（Evaluator）；由于这个条件表达式中包含一个标量子查询，所以 Evaluator 又会调用 Executor 计算标量子查询的结果。</p><p><strong>这种 Executor - Evaluator - Executor 的交替调用十分低效</strong>！考虑到 Filter 上可能会有上百万行数据经过，如果为每行数据都执行一次子查询，那查询执行的总时长显然是不可接受的。</p><h2 id="apply-算子">Apply 算子</h2><p>上文说到的 Relation - Expression - Relation 这种交替引用不仅执行性能堪忧，而且，对于优化器也是个麻烦的存在——我们的优化规则都是在匹配并且对 Relation 进行变换，而这里的子查询却藏在 Expression 里，令人无从下手。</p><p>为此，在开始去关联化之前，我们引入 Apply 算子：</p><p><strong>Apply 算子</strong>（也称作 Correlated Join）接收两个关系树的输入，与一般 Join 不同的是，Apply 的 Inner 输入（图中是右子树）是一个带有参数的关系树。</p><p>Apply 的含义用下图右半部分的集合表达式定义：对于 Outer Relation <span class="math inline">\(R\)</span> 中的每一条数据 <span class="math inline">\(r\)</span>，计算 Inner Relation <span class="math inline">\(E(r)\)</span>，输出它们连接（Join）起来的结果 <span class="math inline">\({r} \otimes E(r)\)</span>。Apply 的结果是所有这些结果的并集（本文中说的并集指的是 Bag 语义下的并集，也就是 UNION ALL）。</p><p><img src="/images/2019/04/subquery-optimization-query-apply-operator.png"></p><blockquote><p>Apply 是 SQL Server 的命名，它在 HyPer 的文章中叫做 Correlated Join。它们是完全等价的。考虑到 SQL Server 的文章发表更早、影响更广，本文中都沿用它的命名。</p></blockquote><p>根据连接方式（<span class="math inline">\(\otimes\)</span>）的不同，Apply 又有 4 种形式：</p><ul><li><strong>Cross Apply</strong> <span class="math inline">\(A^{\times}\)</span>：这是最基本的形式，行为刚刚我们已经描述过了；</li><li><strong>Left Outer Apply</strong> <span class="math inline">\(A^{LOJ}\)</span>：即使 <span class="math inline">\(E(r)\)</span> 为空，也生成一个 <span class="math inline">\(r \circ \{NULLs\}\)</span>。</li><li><strong>Semi Apply</strong> <span class="math inline">\(A^{\exists}\)</span>：如果 <span class="math inline">\(E(r)\)</span> 不为空则返回 <span class="math inline">\(r\)</span>，否则丢弃；</li><li><strong>Anti-Semi Apply</strong> <span class="math inline">\(A^{\nexists}\)</span>：如果 <span class="math inline">\(E(r)\)</span> 为空则返回 <span class="math inline">\(r\)</span>，否则丢弃；</li></ul><p>我们用刚刚定义的 Apply 算子来改写之前的例子：把子查询从 Expression 内部提取出来。结果如下：</p><p><img src="/images/2019/04/subquery-optimization-subquery-using-apply.png"></p><p>上面的例子中，我们可以肯定 Scalar Agg 子查询<strong>有且只有</strong>一行结果，所以可以直接转成 Apply。但某些情况下，可能无法肯定子查询一定能返回 0 或 1 行结果（例如，想象一下 Query 2 如果 c_custkey 不是唯一的），为了确保 SQL 语义，还要在 Apply 右边加一个 <span class="math inline">\(\textit{Max1Row}\)</span> 算子：</p><p><span class="math display">\[\textit{Max1Row}(E)=\begin{cases}    \textit{Null}, &amp; \text{if}\ |E|=0 \\    E, &amp; \text{if}\ |E|=1 \\    \textit{error}, &amp; \text{otherwise}\end{cases}\]</span></p><p>理论上，我们<strong>可以将所有的子查询转换成 Apply 算子</strong>，一个通用的方法如下：</p><ol type="1"><li>如果某个算子的表达式中出现了子查询，我们就把这个子查询提取到该算子下面（留下一个子查询的结果变量），构成一个 <span class="math inline">\(A^{LOJ}\)</span> 算子。如果不止一个子查询，则会产生多个 <span class="math inline">\(A^{LOJ}\)</span>。必要的时候加上 <span class="math inline">\(\textit{Max1Row}\)</span> 算子。</li><li>然后应用其他一些规则，将 <span class="math inline">\(A^{LOJ}\)</span> 转换成 <span class="math inline">\(A^{\times}\)</span>、<span class="math inline">\(A^{\exists}\)</span>、<span class="math inline">\(A^{\nexists}\)</span>。例如上面例子中的子查询结果 <span class="math inline">\(X\)</span> 被用作 Filter 的过滤条件，NULL 值会被过滤掉，因此可以安全地转换成 <span class="math inline">\(A^{\times}\)</span>。</li></ol><p>下面这个例子中，Filter 条件表达式中包含 <span class="math inline">\(Q_1\)</span>、<span class="math inline">\(Q_2\)</span> 两个子查询。转换之后分别生成了对应的 Apply 算子。其中 <span class="math inline">\(Q_2\)</span> 无法确定只会生成恰好一条记录，所以还加上了 <span class="math inline">\(\textit{Max1Row}\)</span> 算子。</p><p><img src="/images/2019/04/convert-subquery-to-apply.png"></p><h2 id="基本消除规则">基本消除规则</h2><p>第一组规则是最基本的规则，等式中的 <span class="math inline">\(\otimes\)</span> 说明它不限制连接类型，可以是 <span class="math inline">\(\{ \times, LOJ, \exists, \nexists \}\)</span> 中的任意一个。</p><p><img src="/images/2019/04/rule-set-1.png"></p><p>这两条规则是非常显而易见的，翻译成大白话就是：如果 Apply 的右边不包含来自左边的参数，那它就和直接 Join 是等价的。</p><p>下面是对 Query 3 应用规则 (2) 的例子：</p><p><img src="/images/2019/04/subquery-decorrlation-rule-set-1-example.png"></p><h2 id="project-和-filter-的去关联化">Project 和 Filter 的去关联化</h2><p>第二组规则描述了如何处理子查询中的 Project 和 Filter，其思想可以用一句话来描述：<strong>尽可能把 Apply 往下推、把 Apply 下面的算子向上提</strong>。</p><p><img src="/images/2019/04/rule-set-2-1.png"></p><figure><img src="/images/2019/04/subquery-decorration-project-filter-rules%20-1-.png" alt="subquery-decorration-project-filter-rules -1-"><figcaption>subquery-decorration-project-filter-rules -1-</figcaption></figure><p>注意这些规则仅处理 Cross Apply 这一种情况。其他 3 种 Apply 的变体，理论上都可以转换成 Cross Apply，暂时我们只要知道这个事实就可以了。</p><p>你可能会问：通常我们都是尽可能把 Filter、Project 往下推，为什么这里会反其道而行呢？关键在于：Filter、Project 里面原本包含了带有关联变量的表达式，但是把它提到 Apply 上方之后，<strong>关联变量就变成普通变量了！</strong>这正是我们想要的。</p><p>我们稍后就会看到这样做的巨大收益：<strong>当 Apply 被推最下面时，就可以应用第一组规则，直接把 Apply 变成 Join</strong>，也就完成了子查询去关联化的优化过程。</p><p>下面是对 Query 2 应用规则 (3) 的例子。之后再应用规则 (1)，就完成了去关联化过程。</p><p><img src="/images/2019/04/subquery-decorrlation-rule-set-2-example.png"></p><h2 id="aggregate-的去关联化">Aggregate 的去关联化</h2><p>第三组规则描述如何处理子查询中的 Aggregate（即 Group By）。和上一组一样，我们的指导思想仍然是：<strong>尽可能把 Apply 往下推、把 Apply 下面的算子向上提</strong>。</p><p>下面等式中，<span class="math inline">\(G_{A,F}\)</span> 表示带有 Group By 分组的聚合（Group Agg），其中 <span class="math inline">\(A\)</span> 表示分组的列，<span class="math inline">\(F\)</span> 表示聚合函数的列；<span class="math inline">\(G_{F}^{1}\)</span> 表示不带有分组的聚合（Scalar Agg）。</p><p><img src="/images/2019/04/rule-set-3.png"></p><p>这一组规则不像之前那么简单直白，我们先看一个例子找找感觉。下面是对 Query 1 运用规则 (9) 的结果：</p><p><img src="/images/2019/04/subquery-decorrelation-agg-example.png"></p><p>规则 (9) 在下推 Apply 的同时，还将 ScalarAgg 变成了 GroupAgg，其中，<strong>分组列就是 R 的 key</strong>，在这里也就是 CUSTOMER 的主键 c_custkey。</p><blockquote><p>如果 R 没有主键或唯一键，理论上，我们可以在 Scan 时生成一个。</p></blockquote><p>为什么变换前后是等价的呢？变换前，我们是给每个 R 的行做了一次 ScalarAgg 聚合计算，然后再把聚合的结果合并起来；变换后，我们先是将所有要聚合的数据准备好（这被称为 augment），然后使用 GroupAgg 一次性地做完所有聚合。</p><p>这也解释了为什么我们要用 <span class="math inline">\(A^{LOJ}\)</span> 而不是原本的 <span class="math inline">\(A^{\times}\)</span> ：原来的 ScalarAgg 上，即使输入是空集，也会输出一个 NULL。如果我们这里用 <span class="math inline">\(A^{LOJ}\)</span>，恰好也会得到一样的行为（＊）；反之，如果用 <span class="math inline">\(A^{\times}\)</span> 就有问题了——没有对应 ORDERS 的客户在结果中消失了！</p><p>规则 (8) 处理的是 GroupAgg，道理也是一样的，只不过原来的分组列也要留着。</p><p><strong>ScalarAgg 转换中的细节＊</strong></p><p>细心的读者可能注意到，规则 (9) 右边产生的聚合函数是 <span class="math inline">\(F&#39;\)</span>，多了一个单引号，这暗示它和原来的聚合函数 <span class="math inline">\(F\)</span> 可能是有些不同的。那什么情况下会不同呢？这个话题比较深入了，不感兴趣的同学可以跳过。</p><p>首先我们思考下，GroupAgg 以及 <span class="math inline">\(A^{LOJ}\)</span> 的行为真的和变换前一模一样吗？其实不然。举个反例：</p><pre><code class="language-text">SELECT c_custkey, (    SELECT <span class="marked-code">COUNT(*)</span>    FROM ORDERS    WHERE o_custkey = c_custkey) AS count_ordersFROM CUSTOMER</code></pre><p>设想一下：客户 Eric 没有任何订单，那么这个查询应当返回一个 <code>['Eric', 0]</code> 的行。但是，当我们应用了规则 (9) 做变换之后，却得到了一个 <code>['Eric', 1]</code> 的值，结果出错了！</p><p>为何会这样呢？变换之后，我们是先用 LeftOuterJoin 准备好中间数据（augment），然后用 GroupAgg 做聚合。LeftOuterJoin 为客户 Eric 生成了一个 <code>['Eric', NULL, NULL, ...]</code> 的行；之后的 GroupAgg 中，聚合函数 <code>COUNT(*)</code> 认为 Eric 这个分组有 1 行数据，所以输出了 <code>['Eric', 1]</code>。</p><p>下面是个更复杂的例子，也有类似的问题：</p><pre><code class="language-text">SELECT c_custkeyFROM CUSTOMERWHERE 200000 < (    SELECT <span class="marked-code">MAX(IF_NULL(o_totalprice, 42))</span> -- o_totalprice may be NULL    FROM ORDERS    WHERE o_custkey = c_custkey)</code></pre><p>作为总结，问题的根源在于：<span class="math inline">\(F(\emptyset) \neq F(\{\textit{NULL}\})\)</span>，这样的聚合函数 <span class="math inline">\(F\)</span> 都有这个问题。</p><p><strong>变换后的 GroupAgg 无法区分它看到的 NULL 数据到底是 OuterJoin 产生的，还是原本就存在的</strong>，有时候，这两种情形在变换前的 ScalarAgg 中会产生不同的结果。</p><p>幸运的是，SQL 标准中定义的聚合函数 <span class="math inline">\(F(col)\)</span> 都是 OK 的——它们都满足 <span class="math inline">\(F(\emptyset) = F(\{\textit{NULL}\})\)</span>，我们只要对 <span class="math inline">\(F\)</span> 稍加变换就能解决这个问题。</p><ul><li>对于例子一，将 <code>COUNT(*)</code> 替换成一个对非空列（例如主键）的 Count 即可，例如：<code>COUNT(o_orderkey)</code>；</li><li>对于例子二，需要把 <code>MIN(IF_NULL(o_totalprice, 42))</code> 分成两步来做：定义中间变量 <code>X</code>，先用 Project 计算 <code>X = IF_NULL(o_totalprice, 42)</code>，再对聚合函数 <code>MIN(X)</code> 进行去关联化即可。</li></ul><h2 id="集合运算的去关联化">集合运算的去关联化</h2><p>最后一组优化规则用来处理带有 Union（对应 <code>UNION ALL</code>）、Subtract（对应 <code>EXCEPT ALL</code>） 和 Inner Join 算子的子查询。再强调一遍，我们的指导思想是：<strong>尽可能把 Apply 往下推、把 Apply 下面的算子向上提</strong>。</p><p>下面的等式中，<span class="math inline">\(\times\)</span> 表示 Cross Join，<span class="math inline">\(\bowtie_{R.key}\)</span> 表示按照 <span class="math inline">\(R\)</span> 的 Key 做自然连接：<span class="math inline">\(r \circ e_1 \circ e_2\)</span> 。和之前一样，我们假设 <span class="math inline">\(R\)</span> 存在主键或唯一键，如果没有也可以在 Scan 的时候加上一个。</p><p><img src="/images/2019/04/rule-set-4.png"></p><p><img src="/images/2019/04/subquery-decorration-set-op-rules.png"></p><p>注意到，这些规则与之前我们见过的规则有个显著的不同：等式右边 <span class="math inline">\(R\)</span> 出现了两次。这样一来，要么我们把这颗子树拷贝一份，要么做成一个 DAG 的执行计划，总之会麻烦许多。</p><p>事实上，这一组规则很少能派上用场。在 [2] 中提到，在 TPC-H 的 Schema 下甚至很难写出一个带有 Union All 的、有意义的子查询。</p><h2 id="其他">其他</h2><p>有几个我认为比较重要的点，用 FAQ 的形式列在下面。</p><p><strong>► 是否任意的关联子查询都可以被去关联化？</strong></p><p>可以说是这样的，在加上少量限定之后，理论上可以证明：任意的关联子查询都可以被去关联化。</p><p>证明方法在 [1]、[3] 中都有提及。以 [1] 中为例，思路大致是：</p><ol type="1"><li>对于任意的查询关系树，首先将关联子查询从表达式中提取出来，用 Apply 算子表示；</li><li>一步步去掉其中非基本关系算子，首先，通过等价变换去掉 Union 和 Subtract；</li><li>进一步缩小算子集合，去掉 OuterJoin、<span class="math inline">\(A^{LOJ}\)</span>、<span class="math inline">\(A^{\exists}\)</span>、<span class="math inline">\(A^{\nexists}\)</span>；</li><li>最后，去掉所有的 <span class="math inline">\(A^{\times}\)</span>，剩下的关系树仅包含基本的一些关系算子，即完成了去关联化。</li></ol><p>另一方面，现实世界中用户使用的子查询大多是比较简单的，本文中描述的这些规则可能已经覆盖到 99% 的场景。虽然理论上任意子查询都可以处理，但是实际上，没有任何一个已知的 DBMS 实现了所有这些变换规则。</p><p><strong>► HyPer 和 SQL Server 的做法有什么异同？</strong></p><p>HyPer 的理论覆盖了更多的去关联化场景。例如各种 Join 等算子，[3] 中都给出了相应的等价变换规则（作为例子，下图是对 Outer Join 的变换）。而在 [1] 中仅仅是证明了这些情况都可以被规约到可处理的情形（实际上嘛，可想而知，一定是没有处理的）。</p><p><img src="/images/2019/04/hyper-outer-join-example.png"></p><p>另一个细节是，HyPer 中还存在这样一条规则：</p><p><img src="/images/2019/04/hyper-general-unnesting.png"></p><p>其中，<span class="math inline">\(D=\Pi_{F(T_2)\cap A(T_1)} (T_1)\)</span>，表示对 <span class="math inline">\(T_1\)</span> 的 Distinct Project 结果。直接看等式比较晦涩，看下面的例子就容易理解了：</p><p><img src="/images/2019/04/hyper-general-unnesting-example.png"></p><p>图中，在做 Apply 之前，先拿到需要 Apply 的列的 Distinct 值集合，拿这些值做 Apply，之后再用普通的 Join 把 Apply 的结果连接上去。</p><p>这样做的好处是：如果被 Apply 的数据存在大量重复，则 Distinct Project 之后需要 Apply 的行数大大减少。这样一来，即使之后 Apply 没有被优化掉，迭代执行的代价也会减小不少。</p><p><strong>► 本文说的这些变换规则，应该用在 RBO 还是 CBO 中呢？换句话说，去关联化后之后的执行计划一定比去关联化之前更好吗？</strong></p><p>答案是，不一定。</p><p>直观的看，如果 Apply 的左边数据量比较少（例如，仅有 1 条数据），那直接带入 Apply 的右边计算反而是更好的方式。另一种情况是，右边有合适的索引，这种情况下，多次 Apply 的代价也并非不可接受。</p><p>所以把这些规则放进一个 CBO 的优化器是更合适的，优化器根据代价估计选出最优的计划来。甚至，在某些情况下，我们还会自右向左地运用这些等式，做“加关联化”。</p><p>这和用 HashJoin 还是 NestedLoopJoin 是同样的道理。事实上，NestedLoopJoin 就是 Apply 的一个特例。如果存在合适的索引，NestedLoopJoin 效率高于 HashJoin 是很常见的事情。</p><h2 id="references">References</h2><ol type="1"><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2000-31.pdf" target="_blank" rel="noopener">Parameterized Queries and Nesting Equivalencies - C Galindo-Legaria</a></li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.563.8492&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Orthogonal Optimization of Subqueries and Aggregation - C Galindo-Legaria, M Joshi</a></li><li><a href="https://dl.gi.de/bitstream/handle/20.500.12116/2418/383.pdf?sequence=1" target="_blank" rel="noopener">Unnesting Arbitrary Queries - T Neumann, A Kemper</a></li><li><a href="https://dl.gi.de/bitstream/handle/20.500.12116/657/paper04.pdf?sequence=1&amp;isAllowed=y" target="_blank" rel="noopener">The Complete Story of Joins (inHyPer) - T Neumann, V Leis, A Kemper</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2019/04/banner-subquery-optimization.png&quot;&gt;&lt;/p&gt;
&lt;style type=&quot;text/css&quot;&gt;
.image-captain {
    margin-top: -20px;
}
.marked-code {
    color:red;
    font-weight:bold;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;子查询&lt;/strong&gt;（Subquery）的优化一直以来都是 SQL 查询优化中的难点之一。关联子查询的基本执行方式类似于 Nested-Loop，但是这种执行方式的效率常常低到难以忍受。当数据量稍大时，必须在优化器中对其进行&lt;strong&gt;去关联化&lt;/strong&gt;（Decoorelation 或 Unnesting），将其改写为类似于 Semi-Join 这样的更高效的算子。&lt;/p&gt;
&lt;p&gt;前人已经总结出一套完整的方法论，理论上能对任意一个查询进行去关联化。本文结合 SQL Server 以及 HyPer 的几篇经典论文，由浅入深地讲解一下这套去关联化的理论体系。它们二者所用的方法大同小异，基本思想是想通的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
      <category term="optimizer" scheme="https://ericfu.me/tags/optimizer/"/>
    
      <category term="sql" scheme="https://ericfu.me/tags/sql/"/>
    
  </entry>
  
  <entry>
    <title>JIT 代码生成技术（二）查询编译执行</title>
    <link href="https://ericfu.me/code-gen-of-query/"/>
    <id>https://ericfu.me/code-gen-of-query/</id>
    <published>2019-02-28T12:14:27.000Z</published>
    <updated>2019-06-04T02:19:09.414Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/03/code-gen-2-banner.jpg"></p><p><strong>代码生成（Code Generation）</strong>技术广泛应用于现代的数据系统中。代码生成是将用户输入的表达式、查询、存储过程等现场编译成二进制代码再执行，相比解释执行的方式，运行效率要高得多。</p><p>上一篇文章 <a href="/code-gen-of-expression">代码生成技术（一）表达式编译</a> 中提到，虽然表面上都叫“代码生成”，但是实际可以分出几种粒度的实现方式，比如表达式的代码生成、查询的代码生成、存储过程的代码生成等。今天我们要讲的是<strong>查询级别</strong>的代码生成，有时也称作<strong>算子间</strong>（intra-operator）级别，这也是主流数据系统所用的编译执行方式。</p><a id="more"></a><p>本文主要参考了 HyPer 团队发表在 VLDB'11 的文章 <a href="https://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">Efficiently Compiling Efficient Query Plans for Modern Hardware</a>。</p><h2 id="volcano-经典执行模型">Volcano 经典执行模型</h2><blockquote><p>为什么要用编译执行？编译执行有哪几种实现？这些问题的答案都写在前一篇文章里，还有困惑的同学务必先看完 <a href="/code-gen-of-expression">前一篇文章</a> 再回来。</p></blockquote><p>今天说的主角是查询（Query）的编译执行，在讲它之前，看看经典 Volcano 模型是怎么做的。Volcano 模型十分简单（这也是它流行的主要原因）：<strong>每个算子需要实现一个 <code>next()</code> 接口，意为返回下一个 Tuple</strong>。</p><p><img src="/images/2019/03/volcano-execution-example.png"></p><p><strong>Query 1</strong> 是一个很简单的查询，Project 会调用 Filter 的 <code>next()</code> 获得数据，Filter 的 <code>next()</code> 又会调用 TableScan 的 <code>next()</code>，TableScan 读出表中的一行数据并返回。如此往复，直到数据全部处理完。</p><p><strong>Query 2</strong> 复杂一些，它包含一个 HashJoin。我们知道 HashJoin 的两个子节点是不对称的，一边称为 build-side，另一边称为 probe 或 stream-side。执行时，必须等待 build-side 处理完<strong>全部</strong>数据、构建出哈希表之后，才能运行 stream-side。</p><p>因为这个原因，执行的过程其实被分成了两个阶段（图中浅灰色的背景）。在 Volcano 模型中，这也很容易实现，我们试着写一下 HashJoin 的伪代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Row HashJoin::next() &#123;</span><br><span class="line">    <span class="comment">// Stage 1: Build Hash Table (HT)</span></span><br><span class="line">    <span class="keyword">if</span> (HT is not built yet) <span class="comment">// 注意：Build 仅在第一次调用 next() 时发生</span></span><br><span class="line">        <span class="keyword">while</span> ((r = left.next()) != END)</span><br><span class="line">            ht.put(buildKey(r), buildValue(r))</span><br><span class="line">    <span class="comment">// Stage 2: Probe tuples one by one</span></span><br><span class="line">    <span class="keyword">while</span> (r = right.next())</span><br><span class="line">        <span class="keyword">if</span> (HT contains r)</span><br><span class="line">            output joined row;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个构建哈希表的过程，我们称为<strong>物化（Materialize）</strong>，意味着 Tuple 不能继续往上传递，而是被暂存到某个 buffer 里。而大多数时候，例如执行 Filter 等算子时，Tuple 是被一路传上去的，这被称为 <strong>Pipeline</strong>。显然物化的代价是比较高的，我们希望尽可能多的 Pipeline 而避免物化。</p><p><strong>Query 3</strong> 中的 Aggregate 算子也有类似的情况：在 Aggregate 返回第一条结果之前，我们要把下面所有的数据都聚合完成才行。</p><p><strong>我们称 HashJoin、HashAgg 这种打断 Pipeline 的算子为 Pipeline Breaker</strong>，它们的存在使得执行过程被分成了不止一个阶段。值得注意的是，这里之所以分成多个阶段，是因为 HashJoin 或 HashAgg 算法本身决定的，跟 Volcano 执行模型无关。</p><h2 id="volcano-的性能问题">Volcano 的性能问题</h2><p>Volcano 执行模型胜在简单易懂，在那个硬盘速度跟不上 CPU 的时代，性能方面并不需要考虑太多。然而随着硬件的进步，IO 很多时候已经不再是瓶颈，这时候人们就开始重新审视 Volcano 模型，于是产生了两种改进思路：</p><ol type="1"><li>将 Volcano 迭代模型和向量化模型结合，每次返回一批而不是一个 Tuple；</li><li>利用代码生成技术，消除迭代计算的性能损耗。</li></ol><blockquote><p>关于这两个方案哪个更优，这里有一篇<a href="http://www.vldb.org/pvldb/vol11/p2209-kersten.pdf" target="_blank" rel="noopener">非常棒的论文</a>做了很详尽的实验和分析。</p></blockquote><p>当然，作为今天的主题，我们只看第二个思路。就像表达式解析执行一样，Volcano 其实是对算子树的解释执行，它也同样存在这些问题：</p><ul><li>每产生一条结果就要做很多次虚函数调用，消耗了大量的 CPU 时间；</li><li>过多的函数调用导致不能很好的利用寄存器。</li></ul><p>我们来思考一个问题，<strong>如果让你去把 Query 1 写成代码来执行，会是什么样的呢</strong>？答案非常短，短的令人惊讶：</p><p><img src="/images/2019/03/code-gen-example-1.png"></p><p>右图中用不同颜色标出了原来的算子，其中 <code>condition = true</code> 是一个表达式，<strong>按照上一篇文章讲解的方法就能生成出代码</strong>，然后放到这边 <code>if</code> 的条件上即可。</p><p>这两个的执行效率应该很容易看出差距吧！生成出的代码完全消除了虚函数调用，而且 Tuple 几乎一直在高速缓存甚至寄存器中。论文中也提到，随便找个本科生手写代码，执行性能都能甩迭代模型几条街。</p><p>再看个更复杂的例子找找感觉，以下查询（记作 <strong>Query 4</strong>）混合了 Join、Aggragate 甚至子查询，之前我们说到，这些算子是 Pipeline Breaker，执行过程被不可避免的分成几个阶段；除此以外，我们希望其他部分尽可能地做到 Pipeline 执行。</p><p><img src="/images/2019/03/code-gen-example-2.png"></p><p>这个例子有点长，但如果你能花上两三分钟看懂它，相信你对代码生成已经有了些直觉上的理解，这对你理解掌握下一章节的内容大有帮助。</p><p>图中我用不同颜色出了 HashJoin、HashAgg 三个算子各自的代码，可以看出，它们各自的代码逻辑被“分散”到了不止一处地方，甚至代码中已经很难分辨出各个算子，而是全都<strong>融合</strong>（Fusion）到一块儿了。</p><p>这就是我们想要的结果！好了，下一步终于进入了正题：如何自动生成出这样的代码呢？</p><blockquote><p>很多人有个错觉，以为数据库查询过程那么复杂，生成的代码一定也很复杂吧。其实不然，查询中复杂的部分，例如 HashJoin 中哈希表实现、TableScan 读取数据的实现等，<strong>这些并不用生成很多代码，仅仅只是调用现有的函数即可</strong>，比如 LLVM IR 可以调用已存在的任何函数。</p><p>换个角度看，生成的代码不过是把这些算子的实现以更高效的方式串联在了一起：算子自身逻辑就像齿轮，生成的代码好比连接齿轮的链条。</p></blockquote><h2 id="hyper-的解决方案">HyPer 的解决方案</h2><p>代码生成是个纯粹的工程问题。工程问题没有什么不能解的，难就难在找到其中最漂亮的解。比如现在这个问题，为了编程的优雅，我们希望造一个可扩展的框架：<strong>不论哪个算子，只要实现某种接口</strong>（就像 Volcano 模型要求实现 <code>next()</code> 接口一样）<strong>，就能参与到代码生成中</strong>。</p><p>论文中给出的解法可以说是十分优雅了，模型要求所有算子实现以下两个接口函数：</p><ul><li><code>produce()</code></li><li><code>consume(attributes, node)</code></li></ul><p>代码生成的过程总是从调用根节点的 <code>produce()</code> 开始；而 <code>consume()</code> 类似于一个回调函数，当下层的算子完成自己的使命之后，调用上层的 <code>consume()</code> 来消费刚刚产生的 tuples——注意这里并不是真的消费。</p><p>用例子来说明。下面是一个伪代码版本的若干算子实现。<code>produce()</code> 和 <code>consume()</code> 返回的类型都是生成的代码片段，这里为了方便演示直接用字符串表示。真实世界中当然要更复杂一些。</p><p><img src="/images/2019/03/code-gen-interface-example.png"></p><p>表中红色的字符串是生成的代码，黑色的则是 code-gen 本身的代码。回忆一下：代码生成其实就是用各种手段拼出代码（字符串）来，没什么神秘的。</p><figure><img src="/images/2019/03/code-gen-demo-animated.gif" alt="ezgif.com-gif-maker -1-"><figcaption>ezgif.com-gif-maker -1-</figcaption></figure><p>不满足于伪代码的同学可以尝试阅读 HyPer 的 <a href="https://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">论文</a>（生成 LLVM IR） 或者 Spark SQL 中的 <a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala" target="_blank" rel="noopener">CodeGenerator 实现</a>（生成 Java 代码），后者的代码相对更容易理解些。</p><h2 id="思考这是唯一的解法吗">思考：这是唯一的解法吗？*</h2><p>为什么是 produce/consume 呢？是否存在更简单的解呢？<strong>这里给出我的推导思路，你可以跳过这一段</strong>，毕竟每个人的脑回路是不一样的。</p><p>首先，如果只有一个接口函数，不妨叫它 <code>produce()</code>，一定是不够用的。为什么这么说呢？一个函数充其量只能做出类似 DFS 的效果：每个算子只会被经过一次。这对 Query 1 还不是问题，但对于上文中复杂的 Query 4，HashJoin 的两部分代码离得那么远，用 DFS 就很难做到了。</p><p>为了处理 HashJoin，我们该增加一个怎样的函数呢？<strong>我认为它应该类似于一个回调</strong>，比如 Query 4 中，当 DFS 进行到 <span class="math inline">\(\Join_{a=b}\)</span> 时，我希望通过一种某种方式告诉下面的 <span class="math inline">\(\sigma_{x=7}\)</span>：当你拿到结果后，只要用我传给你的方法去消费这些 Tuples（生成消费这些 Tuples 的代码）。这个方法，不妨叫做 <code>consume()</code>。</p><p>顺理成章的，<code>consume()</code> 至少有个参数来传递需要消费的 tuples 有哪些列。另外，还需要一个参数用来指示：调用者是左孩子还是右孩子？这等价于传 <code>this</code>。</p><p>以上。因此我倾向于了认为，论文提出的 produce/consume 模式可能是唯一正确的方法，即使存在其他算法，我猜想也是大同小异。</p><h2 id="references">References</h2><ol type="1"><li><a href="https://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">Efficiently Compiling Efficient Query Plans for Modern Hardware - VLDB'11</a></li><li><a href="https://issues.apache.org/jira/browse/SPARK-12795" target="_blank" rel="noopener">SPARK-12795 - Whole stage codegen</a></li><li><a href="http://www.vldb.org/pvldb/vol11/p2209-kersten.pdf" target="_blank" rel="noopener">Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask - VLDB'18</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2019/03/code-gen-2-banner.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码生成（Code Generation）&lt;/strong&gt;技术广泛应用于现代的数据系统中。代码生成是将用户输入的表达式、查询、存储过程等现场编译成二进制代码再执行，相比解释执行的方式，运行效率要高得多。&lt;/p&gt;
&lt;p&gt;上一篇文章 &lt;a href=&quot;/code-gen-of-expression&quot;&gt;代码生成技术（一）表达式编译&lt;/a&gt; 中提到，虽然表面上都叫“代码生成”，但是实际可以分出几种粒度的实现方式，比如表达式的代码生成、查询的代码生成、存储过程的代码生成等。今天我们要讲的是&lt;strong&gt;查询级别&lt;/strong&gt;的代码生成，有时也称作&lt;strong&gt;算子间&lt;/strong&gt;（intra-operator）级别，这也是主流数据系统所用的编译执行方式。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
      <category term="jit" scheme="https://ericfu.me/tags/jit/"/>
    
  </entry>
  
  <entry>
    <title>从 Weld 论文看执行器的优化技术</title>
    <link href="https://ericfu.me/weld-the-query-exeution-engine/"/>
    <id>https://ericfu.me/weld-the-query-exeution-engine/</id>
    <published>2019-01-31T01:50:17.000Z</published>
    <updated>2019-06-04T02:19:09.414Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/02/weld-banner.jpg"></p><p><a href="https://weld.rs/" target="_blank" rel="noopener">Weld</a> 是一个用于数据计算分析的高性能 Runtime（<em>High-performance runtime for data analytics applications</em>），使用 Rust 编写，可以很容易地集成到各种大数据计算框架中，比如 Spark SQL、NumPy &amp; Pandas、TensorFlow 等，带来大幅的性能提升。</p><p>除了 Weld 本身的贡献，论文中提到的各种用于执行阶段的优化技术也很有意思，其中的大部分都借鉴自关系型数据库或编译器。本文除了介绍 Weld 之外，也是想对这些技术做个梳理。</p><p>本文主要内容来自于 Weld 发表在 <a href="https://anilshanbhag.in/static/papers/weld_vldb18.pdf" target="_blank" rel="noopener">VLDB'18 的论文</a>。</p><a id="more"></a><h2 id="整体架构">整体架构</h2><p>之前说到，Weld 是一个用于数据计算的 Runtime，它的上层通常是一些计算框架，例如 Spark SQL、NumPy 等。用户用这些计算框架编写程序，这些框架将用户需要的计算翻译成 Weld 中间表示（IR），然后 Weld 对其进行一系列的优化，最后生成代码并编译运行。</p><p>做个类比，这就像 LLVM 的工作方式一样：各种语言的编译前端将高级语言翻译成 LLVM IR，LLVM 再对 IR 做一系列的优化，最后再编译成二进制。</p><p>虽然都是 IR，但实际上 Weld IR 和 LLVM IR 有很大不同：</p><ul><li><strong>Weld IR 是声明式的</strong>：只表达计算流程，不包含具体的实现。比如下面会提到的 Builder，上层不需要指定用什么方式构建数组或是哈希表等数据结构，这些是由 Weld 优化器决定的；</li><li><strong>Weld IR 是 Lazy 的</strong>：只有当需要输出结果时，相应的 DAG 计算才会真正开始运行。</li></ul><p><img src="/images/2019/02/weld-architecture.png"></p><p>上图是 Weld 的整体工作过程：</p><ol type="1"><li>上层调用 Weld 的 API 输入需要计算的 IR 程序，它会被解析成 AST；</li><li>当需要执行时，相关的函数 IR 会被拼在一起，方便进行整体优化；</li><li>Weld 优化器使用一系列的启发式规则进行优化，注意结果仍然是 AST；</li><li>最后生成代码并借助 LLVM 编译成二进制。</li></ol><p>Weld 主要由两个部分组成：IR 和 Runtime，接下来我们依次进行介绍。</p><h2 id="weld-ir">Weld IR</h2><p>Weld IR 支持 <code>int</code>、<code>float</code> 等基本数据类型、<code>struct</code> 类型，以及两种容器类型：<code>vec</code> 和 <code>dict</code>，顾名思义，分别是（变长）数组和字典。另外还支持他们的各种组合，就像 JSON 那样。</p><p>和数据库的执行器不同，Weld 不考虑数据拉取之类的问题。它假设输入数据都在内存中以数组形式存在，例如： <code>int[100]</code>、<code>struct{int, float}[100]</code>。</p><p><strong>Weld IR 的计算都通过 Builder 和 Merger 来完成</strong>，由于 Merger 和 Builder 的接口是一样的，Weld 论文中并没有把二者区分开来。下面我们统称为 Builder。</p><table><thead><tr class="header"><th>Builder</th><th>输入</th><th>输出</th><th>备注</th></tr></thead><tbody><tr class="odd"><td><code>vecbuilder[T]</code></td><td><code>T</code></td><td><code>vec[T]</code></td><td>通过 append 构建数组</td></tr><tr class="even"><td><code>dictmerger[K,V,op]</code></td><td><code>(K,V)</code></td><td><code>dict[K,V]</code></td><td>通过 put 构建字典</td></tr><tr class="odd"><td><code>merger[T,op]</code></td><td><code>T</code></td><td><code>T</code></td><td>聚合计算（例如 add）</td></tr><tr class="even"><td><code>vecmerger[T, op]</code></td><td><code>(idx,T)</code></td><td><code>vec[T]</code></td><td>把 T 填在给定位置 idx 上</td></tr><tr class="odd"><td><code>groupbuilder[K,V]</code></td><td><code>(K,V)</code></td><td><code>dict[K,vec[V]]</code></td><td>对数据分组 Group by K</td></tr></tbody></table><p>Builder 提供两个接口方法：</p><ul><li><code>merge(b, v)</code>：向 Builder <code>b</code> 添加新的元素；</li><li><code>result(b)</code>：拿到 <code>b</code> 的结果，注意之后不能再添加元素了。</li></ul><p>下面是使用 Builder 的例子：</p><p><img src="/images/2019/02/weld-example-of-builders.png"></p><p>代码中还有个 <code>for</code>，它的语法是 <code>for(vector, builders, (builders, index, elem) =&gt; builders)</code>，用来<strong>并行地</strong>对数据做处理——也就是往 Builder 里加元素，这是 Weld 中唯一的计算方式。</p><p><code>for</code> 还可以同时处理多个 Builder，这个特性在优化的时候很有用，可以避免同一个数据扫描多次。</p><p>Weld IR 还有些别的特性（比如方便编程的 macro），但不是本文的重点，有兴趣的同学自己看原文吧。</p><h2 id="weld-runtime">Weld Runtime</h2><p><img src="/images/2019/02/weld-optimizing.png"></p><p>当上层输入 IR 并发出开始计算的指令时，就轮到 Weld Runtime 登场了。在代码生成之前，Weld Runtime 会对 IR 做优化，优化可以分为两种：</p><ol type="1"><li>Rule-Based Optimizer (RBO)：和我们熟悉的 RBO 优化类似，是基于规则匹配的优化；</li><li>Adaptive Optimizer：运行时 sample 数据，然后决定用哪种算法执行，勉强可以对应 CBO。</li></ol><p>为什么不是 CBO？关系型数据库的 CBO 是需要以统计信息为基础的，但是 Weld 作为一个通用的 Runtime，上层框架不一定能提供统计信息（比如 NumPy）。</p><p><strong>Weld 应用规则是依次进行的，每次运行一种优化规则，称为一个 pass</strong>。Pass 之间会进行剪枝，去掉无用的代码。以下我们逐条看看 Weld 做了哪些优化。</p><h2 id="pipeline">Pipeline</h2><p>Pipeline 在 OLAP 系统中很常见，最经典的是 HyPer 团队提出的 <a href="http://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">consume/produce 代码生成机制</a>，可以在代码生成时尽可能生成 Pipeline 的代码。</p><p><img src="/images/2019/02/hyper-pipeline-codegen.png"></p><p>为什么需要 Pipeline？设想一下使用代码生成、但是不使用 Pipeline 会怎么样，那么 <span class="math inline">\(R_1\)</span> 和 <span class="math inline">\(\sigma_{x=7}\)</span> 就会分成独立的两步，<span class="math inline">\(R_1\)</span> (即 TableScan）的结果被物化到内存中，再进行 <span class="math inline">\(\sigma_{x=7}\)</span>（Filter）。</p><p>而 Pipeline 的代码省略了中间的物化，仅仅用了一个 <code>if</code> 就解决了 filter，这个代价要低得多：计算 if 表达式时相关数据基本还在寄存器或 Cache 里，充分利用 Data Locality，这比去内存取数据快 1～2 个数量级。</p><p>Pipeline 优化规则会在 AST 中匹配这样的模式：<strong>A 的输出就是 B 的输入</strong>，对匹配到的节点应用 pipeline 优化，下面是一个简单例子：</p><p><img src="/images/2019/02/weld-pipeline-optimize.png"></p><h2 id="horizontal-fusion">Horizontal Fusion</h2><p>Fusion 意为把两段代码融合成一段更精炼的代码，刚刚说的 Pipeline 也是一种 Fusion。所谓 Horizontal Fusion 是找出<strong>被重复处理的数据</strong>，然后将几次处理合在一起。</p><p>例如下面图中的 IR，<code>v0</code> 原本被 loop over 了两次，如果把两次循环合成一次，能尽可能利用 Data Locality，减少一半的内存读取代价。</p><p><img src="/images/2019/02/weld-horizontal-fusion.png"></p><p>硬要说的话，这个规则与关系代数优化中的 Project Merge 规则最相似。论文中给了一个更好的例子来说明它的用处：像 Pandas 这类的计算框架，由于 API 设计一次只能处理一列，必须借助 Horizontal Fusion 实现一次处理多列。</p><h2 id="向量化和-adaptive-优化">向量化和 Adaptive 优化</h2><p>向量化（Vectorization）优化也不是新鲜事，很多编译器（比如 LLVM）都能自动把循环编译成 SIMD 指令，JVM 甚至可以在运行时生成 SIMD 代码。</p><p>SIMD 全称是<strong>单条指令、多个数据</strong>，即用一条指令处理多个数据计算，比如原本计算 4 个整数加法要用 4 次加法指令，用了 SIMD 之后只要 1 次。没错，就这么简单！</p><p><img src="/images/2019/02/simd-illustrate.png"></p><p>在这个 pass 中仅处理简单的、没有条件分支的 for 循环，如果满足这一条件，优化器会将被循环的数据从 <code>T</code> 转换成 <code>simd[T]</code>，最后 code-gen 的时候为其生成 SIMD 代码。</p><p>那对于带有条件分支的 for 循环，能否进行向量化呢？答案是，<strong>可以，但是不一定有用。</strong></p><p>我们先设想一下：对于有条件分支的 for 循环，它向量化之后是什么样的？SIMD 指令本身是没法处理分支的（compare 这种特别简单的除外），如果一定要用 SIMD，可以<strong>假设分支条件全都为 true 或 false</strong>，最后根据条件表达式的计算结果（true 或 false），利用 <code>select</code> 指令选出相应的结果。</p><p>这种方式相比普通的带分支的指令，有得有失：</p><ul><li>优势：用 SIMD 指令集可以加速计算；</li><li>劣势：原本只要算一个分支，现在两个分支都要算。</li></ul><blockquote><p>注：另一个优势是，SIMD 去掉了条件跳转，不存在打断 CPU 流水线的问题。但是论文中没有提到这一点，我猜测可能是它的影响因素比较小，或是作者没有找到一个合适的代价计算方式。</p></blockquote><p>论文只给出了对 <code>if(cond, merge(b, body), b)</code> 这样单分支条件的代价建模，有兴趣的同学可以看原论文上的式子。这里只说一个粗糙的结论：当选择率（即进入 if-body 的概率）很小时，有分支的代码更优；当选择率比较大时，SIMD 代码更优。</p><p>我们之前说过，Weld 假设上层无法提供统计信息，因而在这一步，<strong>由于缺乏关键的选择率信息</strong>，它只能采取一种 Adaptive 的思路：<strong>同时生成有分支的代码和 SIMD 代码，运行时，首先对输入数据做个 Sample 估算一下选择率，再决定走哪个算法。</strong></p><p>选择率（selectivity）这个概念在数据库优化器中也很常用，比如估算 Row Count 时就频繁用到了选择率估计。如果能在优化时直接拿到这个信息，想必不需要这么折腾。</p><h2 id="adaptive-hash-table">Adaptive Hash Table</h2><p>Weld 的 <code>dictbuilder</code> 和 <code>groupbuilder</code> 中都需要构建哈希表，这里也有个 trade-off：是用 Partitioned Hash Table 还是 Global Hash Table？</p><ul><li>Partitioned Hash Table 是将 build 过程分成两步，先各个线程本地做 build，最后再 merge 成完整的结果；</li><li>Global Hash Table 只有一张全局的哈希表，通过加锁等方式做了控制并发写入。</li></ul><p>一般而言，如果 Group by 的基数（Cardinality）比较小，Partitioned 方式更有优势，因为并发冲突会很多；相反，如果基数很大，Global 更占优势，因为无需再做多一次 merge。</p><p>Weld 的做法很巧妙地实现了二者取折中：<strong>各线程先写到本地的哈希表，但如果大小超过阈值，就写到全局的哈希表</strong>。最后把本地数据再 merge 进全局哈希表。这个实现被它称为 Adaptive Hash Table。</p><p><img src="/images/2019/02/weld-adaptive-hash-table.png"></p><h2 id="misc.">Misc.</h2><p>Weld 中还有还有一些优化手段，比较简单：</p><p><strong>循环展开</strong>（Loop Unrolling）是编译器中很常见的优化，如果编译期已知 for 循环的次数很小（例如，对于一个 N*3 的矩阵，第二维度长度仅为 3），就将循环展开，避免条件跳转指令打断 CPU 流水线。</p><p><strong>数组预分配</strong>（Preallocation）在矩阵运算中也很有用，例如，默认 <code>vecbuiler</code> 的实现是自动生长的动态数组。如果预先知道数组长度，就能避免数组生长的拷贝代价。</p><h2 id="评估和总结">评估和总结</h2><p>下面是 Weld 官网放出的性能评估，对于文中提到的这几个框架，的确做到了可观的性能提升。</p><p><img src="/images/2019/02/weld-evaluation.png"></p><blockquote><p>注：这里 TensorFlow 性能是用 CPU 运行的，而非 GPU。</p></blockquote><p><strong>Weld 的最大贡献是抽象出了一个通用的执行器 Runtime</strong>。这个抽象的层级要比“代码生成”中的“代码”（比如 LLVM IR）高级（high-level）不少，但又比关系代数或是线性代数低级（low-level），从而有更好的通用性。更可贵的是，Weld IR 仅仅包含 Builder 以及 for、if 这些最基本的语句，极其之简单。</p><p>上文提到的很多优化规则，不少来源于编译器或关系型数据库。例如 Pipeline Fusion 的思想，在编译器中其实也有体现——编译器会尽可能连续的利用寄存器、避免 store/load。但是 Weld IR 独特的抽象层级令它能做层级更高的优化，达到和数据库的 Pipeline 一样的效果。</p><h2 id="references">References</h2><ol type="1"><li><a href="https://anilshanbhag.in/static/papers/weld_vldb18.pdf" target="_blank" rel="noopener">Evaluating End-to-End Optimization for Data Analytics Applications in Weld (VLDB'18)</a></li><li><a href="http://www.vldb.org/pvldb/vol4/p539-neumann.pdf" target="_blank" rel="noopener">Efficiently Compiling Efficient Query Plans for Modern Hardware (VLDB'11)</a></li><li><a href="https://weld.rs/" target="_blank" rel="noopener">Weld - Official Website</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2019/02/weld-banner.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://weld.rs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weld&lt;/a&gt; 是一个用于数据计算分析的高性能 Runtime（&lt;em&gt;High-performance runtime for data analytics applications&lt;/em&gt;），使用 Rust 编写，可以很容易地集成到各种大数据计算框架中，比如 Spark SQL、NumPy &amp;amp; Pandas、TensorFlow 等，带来大幅的性能提升。&lt;/p&gt;
&lt;p&gt;除了 Weld 本身的贡献，论文中提到的各种用于执行阶段的优化技术也很有意思，其中的大部分都借鉴自关系型数据库或编译器。本文除了介绍 Weld 之外，也是想对这些技术做个梳理。&lt;/p&gt;
&lt;p&gt;本文主要内容来自于 Weld 发表在 &lt;a href=&quot;https://anilshanbhag.in/static/papers/weld_vldb18.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VLDB&#39;18 的论文&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>CompletableFuture 也没有那么废柴嘛！</title>
    <link href="https://ericfu.me/completable-future-not-so-bad/"/>
    <id>https://ericfu.me/completable-future-not-so-bad/</id>
    <published>2019-01-08T16:09:05.000Z</published>
    <updated>2019-01-17T09:14:24.382Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/several-ways-to-aync">上篇文章</a>中提到，Java 里把 Promise 叫作 CompletableFuture，相比那个只能用于线程同步的 Future，它新增了很多方法用于串联异步事件，比如常用的 <code>thenApply</code>、<code>thenCompose</code>、<code>thenAccept</code> 等。</p><p>如果不引入任何第三方库，CompletableFuture 仍是目前 Java 上最好的异步编程方式。之前一直觉得这个东西难用，直到我想明白一件事，证明了 CompletableFuture 虽然麻烦了点但是能做任何事情，然后用它的时候心里就没那么膈应了。</p><p>本文会以一个例子来讲解：<strong>如何把任意函数转换成异步调用风格</strong>。其实不一定要用 CompletableFuture，任何语言和框架都是适用的。</p><a id="more"></a><p>这篇文章不会涉及 CompletableFuture 的用法，你可以参考 Javadoc 或者<a href="https://colobu.com/2016/02/29/Java-CompletableFuture/" target="_blank" rel="noopener">这篇文章</a>。</p><h2 id="证明-completablefuture-是足够的">证明 CompletableFuture 是足够的</h2><p>首先来（极不严谨地）说明一件事情，<strong>为什么 CompletableFuture 是足够用的</strong>，换句话说，证明 <strong>CompletableFuture 能表达一切计算流程</strong>。</p><p>如果你有一些函数式编程的基础，比如会一点 Haskell，这就是一句话的事情：CompletableFuture 其实是一个 Monad —— 因为它的 <code>thenCompose</code> 实现了 Monad 的 <code>&gt;&gt;=</code> 操作符。既然 Monad 能用来表示任何计算过程，CompletableFuture 当然也能。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Applicative</span> m =&gt; <span class="type">Monad</span> (<span class="title">m</span> :: * -&gt; *) <span class="keyword">where</span></span></span><br><span class="line">  (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b  <span class="comment">-- thenCompose 实现了它 </span></span><br><span class="line">  (&gt;&gt;) :: m a -&gt; m b -&gt; m b</span><br><span class="line">  return :: a -&gt; m a</span><br><span class="line">  fail :: <span class="type">String</span> -&gt; m a</span><br><span class="line">  <span class="meta">&#123;-# MINIMAL (&gt;&gt;=) #-&#125;</span> <span class="comment">-- 这是在说：只要实现 (&gt;&gt;=) 就够了</span></span><br></pre></td></tr></table></figure><p>其实想想也很明白，Monad 表示一个带 context 的计算过程，比如可能抛异常之类的（纯函数是不会抛异常的）。CompletableFuture 也一样，他包裹一串计算过程并且处理异常。</p><p>如果看不懂上面的也没关系，我们用另一种方式再说明一下：</p><p>任何程序的流程控制都可以用 <code>if</code> 和 <code>goto</code> 来组合起来。无论是 <code>for</code> 还是 <code>while</code> 循环，desurge 之后不过就是 <code>if</code> 和 <code>goto</code> 的组合。<strong>通过 <code>thenCompose</code> 就可以表达 <code>if</code> 和 <code>goto</code></strong>：</p><blockquote><p>这里说的不够严谨，其实 if 也是 surge，最终会变成条件跳转指令。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cf.thenCompose(v -&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (v &lt; <span class="number">100</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> doStage1(); <span class="comment">// doStage1() 返回一个 CompletableFuture，决定下一步做什么，相当于 goto</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> doStage2(); <span class="comment">// 同上</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>你看这个例子，<code>if</code> 和 <code>goto</code> 都有了，所以无论程序的控制流多复杂，我们都能组合出来。怎么组合？别急，下面我们就来讲这个。</p><h2 id="completablefuture-in-practice">CompletableFuture in Practice</h2><p>我们从一个普通的函数开始。考虑到复杂性和完整性，我们用 <em>Merge 2 Sorted Streams</em> 作为演示，如果你不清楚这个是干嘛的，可以先做一下<a href="https://leetcode.com/problems/merge-sorted-array/" target="_blank" rel="noopener">这道算法题</a>。</p><p>下面是最普通的实现，输入两个数组，输出一个数组：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Stream <span class="title">merge</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">    List&lt;Integer&gt; results = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Integer headA = inputA.next();</span><br><span class="line">    Integer headB = inputB.next();</span><br><span class="line">    <span class="keyword">while</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">            results.add(headB);</span><br><span class="line">            headB = inputB.next();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            results.add(headA);</span><br><span class="line">            headA = inputA.next();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Stream(results);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stream</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Queue&lt;Integer&gt; numbers;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Stream</span><span class="params">(List&lt;Integer&gt; numbers)</span> </span>&#123; <span class="keyword">this</span>.numbers = <span class="keyword">new</span> LinkedList&lt;&gt;(numbers); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">next</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> numbers.poll(); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个实现有什么问题呢？作为算法足够 OK。但是从工程意义上说，如果输入的 Stream 很大，包含 million 级的元素，那更好的方式是把 Stream 的输入输出作为 Iterator，只在 <code>next()</code> 的时候计算下一个需要的元素。这样内存占用是常数级的，完全不用担心数据量过大呢！</p><p>为了看清一步一步的变化过程，我们先假装 Java 有 <a href="https://wiki.python.org/moin/Generators" target="_blank" rel="noopener">Generator 语法</a>。标记为 Generator 的函数不再是一个函数，而是类似一个 Iterator。一旦调用 <code>next()</code>，“函数”代码运行到 <code>yield</code> 返回一个值，然后函数似乎<strong>停在</strong>了这里。下次 <code>next()</code>，“函数”又接着刚刚的地方运行。</p><p>如果有 Generator 的话，函数应该长下面这样，注意 <code>[yield]</code>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Stream <span class="title">merge</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">    Integer headA = inputA.next();</span><br><span class="line">    Integer headB = inputB.next();</span><br><span class="line">    <span class="keyword">while</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">            [yield] headB;</span><br><span class="line">            headB = inputB.next();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            [yield] headA;</span><br><span class="line">            headA = inputA.next();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    [yield] <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>哇，这个函数几乎没有改动，真是太方便了！（然而并没有卵用）</p><h2 id="function-iterator">Function → Iterator</h2><p>现在我们回到现实：Java 并没有 Generator 语法，所以我们要人肉实现一个 Generator。</p><p>为了通用性，首先做一个 desurge，把 while 循环改成 <code>if</code> 和 <code>goto</code> 的组合，这太简单了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Stream <span class="title">merge</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">    Integer headA = inputA.next();</span><br><span class="line">    Integer headB = inputB.next();</span><br><span class="line">    WHILE_LOOP:</span><br><span class="line">    <span class="keyword">if</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">            [yield] headB;</span><br><span class="line">            headB = inputB.next();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            [yield] headA;</span><br><span class="line">            headA = inputA.next();</span><br><span class="line">        &#125;</span><br><span class="line">        goto WHILE_LOOP; <span class="comment">// again，假设 Java 也有 goto</span></span><br><span class="line">    &#125;</span><br><span class="line">    [yield] <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下一步是去掉 <code>yield</code>，刚刚说到 Generator 的每次 <code>next()</code> 似乎会让函数<strong>停在</strong>一个地方，如何实现<strong>停在</strong>一个地方？记下来呗！加一个标记<strong>状态</strong>的变量，这个状态会告诉我下次 <code>next()</code> 的时候从哪里继续运行。</p><p>首先画出函数的控制流图，然后做一件事：想象所有的 <code>yield</code> 之后都有一个断点，我们在断点处切开，标记它为某个 State，这样下次 <code>next()</code> 的时候就能从断点继续。</p><p>下图的 S0 ～ S2 是我标记好的断点，S0 就是起始位置，S1 是两个 <code>yield result</code> 之后断下来的地方（恰好是同一个地方），S2 是 <code>yield null</code> 之后断下来的地方。</p><figure><img src="/images/2019/01/flow-graph-iterator.png" alt="flow-graph -1-"><figcaption>flow-graph -1-</figcaption></figure><p>我们按照图中的 <code>State</code> 标记机械地把它切开，就得到了下面这个类，它就是由 <code>merge()</code> 变换得到的 Generator：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Merger</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">Integer</span>&gt; </span>&#123; </span><br><span class="line">    <span class="comment">// Arguments</span></span><br><span class="line">    <span class="keyword">final</span> Iterator inputA;</span><br><span class="line">    <span class="keyword">final</span> Iterator inputB;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Internal states</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> state = <span class="number">0</span>; <span class="comment">// 我们加上的状态变量</span></span><br><span class="line">    <span class="keyword">private</span> Integer headA; <span class="comment">// 变换前的局部变量，因为跨了多次 next() 调用，不能再是局部变量了</span></span><br><span class="line">    <span class="keyword">private</span> Integer headB; <span class="comment">// 同上</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Merger</span><span class="params">(Iterator inputA, Iterator inputB)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.inputA = inputA;</span><br><span class="line">        <span class="keyword">this</span>.inputB = inputB;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123; <span class="comment">// 这个循环是有用的，往下看几行</span></span><br><span class="line">            <span class="keyword">switch</span> (state) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                headA = inputA.next();</span><br><span class="line">                headB = inputB.next();</span><br><span class="line">                state = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>; <span class="comment">// 这里就用上了外层的循环</span></span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB ) &#123;</span><br><span class="line">                        <span class="keyword">final</span> <span class="keyword">int</span> result = headB;</span><br><span class="line">                        headB = inputB.next();</span><br><span class="line">                        state = <span class="number">1</span>; <span class="comment">// 可以省略</span></span><br><span class="line">                        <span class="keyword">return</span> result; <span class="comment">// 变换前是 yield result</span></span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">final</span> <span class="keyword">int</span> result = headA;</span><br><span class="line">                        headA = inputA.next();</span><br><span class="line">                        state = <span class="number">1</span>; <span class="comment">// 可以省略</span></span><br><span class="line">                        <span class="keyword">return</span> result; <span class="comment">// 变换前是 yield result</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    state = <span class="number">2</span>;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>; <span class="comment">// 变换前是 yield null</span></span><br><span class="line">                &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                <span class="comment">// Generator 已经终结了（变换前：函数已经走到底了）</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Generator has been exhausted!"</span>);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Unreachable!"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>别急，最后我们会简化这些充满废话的代码。</p></blockquote><p>阶段性总结一下：到现在为止，我们做了一件伟大的事情——<strong>把一个函数变成了 Iterator，函数已经不再是函数，而是一个状态机，这个状态记录了下次调用 <code>next()</code> 需要从哪继续</strong>。</p><blockquote><p>套用一下术语：“从哪继续”就是 <a href="https://en.wikipedia.org/wiki/Continuation" target="_blank" rel="noopener">Continuation</a>，把 Continuation 搞出来的这个过程称为 <a href="https://en.wikipedia.org/wiki/Continuation-passing_style" target="_blank" rel="noopener">CPS 变换</a>。</p></blockquote><h2 id="iterator-asynciterator">Iterator → AsyncIterator</h2><p>呃…… 说好的 CompletableFuture 呢？离 CompletableFuture 只有一步之遥了！</p><p>先从接口下手。想象两个 Stream Input 都是从 IO 拿到的数据，所以每次 <code>next()</code> 其实背后都是一次 IO，应该把它用 CompletableFuture 包成异步的，接口大概长这样：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">AsyncIterator</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function">CompletableFuture&lt;T&gt; <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类似刚刚引入 Generator 一样，我们再假装有 <code>await</code> 关键字。<code>await</code> 关键字表示异步地等待结果返回，有了它，函数就魔法般的暂停在等待异步 IO 的地方：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Stream <span class="title">merge</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">    Integer headA = inputA.next();</span><br><span class="line">    Integer headB = inputB.next();</span><br><span class="line">    WHILE_LOOP:</span><br><span class="line">    <span class="keyword">if</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">            Integer result = headB;</span><br><span class="line">            headB = [await] inputB.next(); <span class="comment">// await 会魔法般地等待 next() 完成再继续运行</span></span><br><span class="line">            [yield] result;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Integer result = headA;</span><br><span class="line">            headA = [await] inputA.next();</span><br><span class="line">            [yield] result;</span><br><span class="line">        &#125;</span><br><span class="line">        goto WHILE_LOOP;</span><br><span class="line">    &#125;</span><br><span class="line">    [yield] <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为 <code>await</code> 也会暂停这个“函数”，所以和刚刚对 <code>yield</code> 的处理一样，我们想象 <code>await</code> 这里有一个断点，我们也要为它设置 State 标记：</p><figure><img src="/images/2019/01/flow-graph-async.png" alt="flow-graph -1-"><figcaption>flow-graph -1-</figcaption></figure><p>糟糕！这状态数有点多啊！好在 Java 8 提供了 Lambda 表达式，和 CompletableFuture 搭配食用口味更佳。图中的大多数状态都可以借助 Lambda 表达式来实现，节约了不少代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Merger</span> <span class="keyword">implements</span> <span class="title">AsyncIterator</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// Arguments</span></span><br><span class="line">    <span class="keyword">final</span> Stream inputA;</span><br><span class="line">    <span class="keyword">final</span> Stream inputB;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Internal states</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> state = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> Integer headA;</span><br><span class="line">    <span class="keyword">private</span> Integer headB;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Merger</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.inputA = inputA;</span><br><span class="line">        <span class="keyword">this</span>.inputB = inputB;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> CompletableFuture&lt;Integer&gt; <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">switch</span> (state) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> inputA.next().thenCompose(a -&gt; &#123; <span class="comment">// State 1 在这里！</span></span><br><span class="line">                headA = a;</span><br><span class="line">                <span class="keyword">return</span> inputB.next();</span><br><span class="line">            &#125;).thenCompose(b -&gt; &#123; <span class="comment">// State 2 在这里！</span></span><br><span class="line">                headB = b;</span><br><span class="line">                state = <span class="number">3</span>;</span><br><span class="line">                <span class="keyword">return</span> next(); <span class="comment">// 相当于原来的外层循环</span></span><br><span class="line">            &#125;);</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">                    <span class="keyword">final</span> Integer result = headB;</span><br><span class="line">                    <span class="keyword">return</span> inputB.next().thenCompose(b -&gt; &#123; <span class="comment">// State 4 在这里！</span></span><br><span class="line">                        headB = b;</span><br><span class="line">                        state = <span class="number">3</span>; <span class="comment">// 可以省略</span></span><br><span class="line">                        <span class="keyword">return</span> CompletableFuture.completedFuture(result);</span><br><span class="line">                    &#125;);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">final</span> Integer result = headA;</span><br><span class="line">                    <span class="keyword">return</span> inputA.next().thenCompose(a -&gt; &#123; <span class="comment">// State 5 在这里！</span></span><br><span class="line">                        headA = a;</span><br><span class="line">                        state = <span class="number">3</span>; <span class="comment">// 可以省略</span></span><br><span class="line">                        <span class="keyword">return</span> CompletableFuture.completedFuture(result);</span><br><span class="line">                    &#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                state = <span class="number">6</span>;</span><br><span class="line">                <span class="keyword">return</span> CompletableFuture.completedFuture(<span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Generator has been exhausted!"</span>);</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Unreachable!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="refinement">Refinement</h2><p>上面我们只用了 <code>thenCompose</code>，理论上这是 OK 的，但是实际上 CompletableFuture 有上百个方法，最合适的才是坠吼的。</p><ul><li>如果仅仅是返回一个值（而非阶段），可以用 <code>thenApply</code>；</li><li><code>thenCombine</code> 等待两个 CompletableFuture 都完成了再去调用 BiFunction <code>(T, U) -&gt; R</code> 来消费。</li></ul><blockquote><p>思考题：有兴趣的读者可以思考一下 <code>thenCombine</code> 的实现。</p></blockquote><p>整理一下上面的代码，比如这样：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Merger</span> <span class="keyword">implements</span> <span class="title">AsyncIterator</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// States</span></span><br><span class="line">    <span class="keyword">enum</span> State &#123; START, ITERATING, DONE &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Arguments</span></span><br><span class="line">    <span class="keyword">final</span> Stream inputA;</span><br><span class="line">    <span class="keyword">final</span> Stream inputB;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Internal states</span></span><br><span class="line">    <span class="keyword">private</span> State state = State.START;</span><br><span class="line">    <span class="keyword">private</span> Integer headA;</span><br><span class="line">    <span class="keyword">private</span> Integer headB;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Merger</span><span class="params">(Stream inputA, Stream inputB)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.inputA = inputA;</span><br><span class="line">        <span class="keyword">this</span>.inputB = inputB;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> CompletableFuture&lt;Integer&gt; <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">switch</span> (state) &#123;</span><br><span class="line">        <span class="keyword">case</span> START:</span><br><span class="line">            <span class="comment">// 这里做了小小的优化：这两个 next() 可以并行等待</span></span><br><span class="line">            <span class="keyword">return</span> inputA.next().thenCombine(inputB.next(), (a, b) -&gt; &#123;</span><br><span class="line">                headA = a;</span><br><span class="line">                headB = b;</span><br><span class="line">                state = State.ITERATING;</span><br><span class="line">                <span class="keyword">return</span> (Void)<span class="keyword">null</span>;</span><br><span class="line">            &#125;).thenCompose(__ -&gt; next());</span><br><span class="line">        <span class="keyword">case</span> ITERATING:</span><br><span class="line">            <span class="keyword">if</span> (headA != <span class="keyword">null</span> || headB != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB != <span class="keyword">null</span> &amp;&amp; headA &gt; headB) &#123;</span><br><span class="line">                    <span class="keyword">final</span> Integer result = headB;</span><br><span class="line">                    <span class="keyword">return</span> inputB.next().thenApply(b -&gt; &#123; <span class="comment">// thenCompose 某个值 &lt;=&gt; thenApply</span></span><br><span class="line">                        headB = b;</span><br><span class="line">                        <span class="keyword">return</span> result;</span><br><span class="line">                    &#125;);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">final</span> Integer result = headA;</span><br><span class="line">                    <span class="keyword">return</span> inputA.next().thenApply(a -&gt; &#123; <span class="comment">// 同上</span></span><br><span class="line">                        headA = a;</span><br><span class="line">                        <span class="keyword">return</span> result;</span><br><span class="line">                    &#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                state = State.DONE;</span><br><span class="line">                <span class="keyword">return</span> CompletableFuture.completedFuture(<span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">case</span> DONE:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Generator has been exhausted!"</span>);</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Unreachable!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><p>任何函数都可以用 CompletableFuture 实现异步化，最通用的方式如下：</p><ol type="1"><li>在函数里加上 <code>yield</code>（返回下一个结果）和 <code>await</code>（等待输入值）来标记断点；</li><li>画出控制流图，注意要在 <code>yield</code> 和 <code>await</code> 处断开，断开处标记为状态；</li><li>实现一个状态机类，把控制流图中的代码块、状态都无脑填进去，搞定。</li></ol><p>这一刻，我们都是（人肉）编译器。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/several-ways-to-aync&quot;&gt;上篇文章&lt;/a&gt;中提到，Java 里把 Promise 叫作 CompletableFuture，相比那个只能用于线程同步的 Future，它新增了很多方法用于串联异步事件，比如常用的 &lt;code&gt;thenApply&lt;/code&gt;、&lt;code&gt;thenCompose&lt;/code&gt;、&lt;code&gt;thenAccept&lt;/code&gt; 等。&lt;/p&gt;
&lt;p&gt;如果不引入任何第三方库，CompletableFuture 仍是目前 Java 上最好的异步编程方式。之前一直觉得这个东西难用，直到我想明白一件事，证明了 CompletableFuture 虽然麻烦了点但是能做任何事情，然后用它的时候心里就没那么膈应了。&lt;/p&gt;
&lt;p&gt;本文会以一个例子来讲解：&lt;strong&gt;如何把任意函数转换成异步调用风格&lt;/strong&gt;。其实不一定要用 CompletableFuture，任何语言和框架都是适用的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
      <category term="async" scheme="https://ericfu.me/tags/async/"/>
    
  </entry>
  
  <entry>
    <title>异步编程的几种方式</title>
    <link href="https://ericfu.me/several-ways-to-aync/"/>
    <id>https://ericfu.me/several-ways-to-aync/</id>
    <published>2019-01-03T10:12:43.000Z</published>
    <updated>2019-06-04T02:19:09.414Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/01/banner-async-await.png"></p><p>近期尝试在搬砖专用语言 Java 上实现异步，起因和过程就不再详述了，总而言之，心中一万头草泥马奔过。但这个过程也没有白白浪费，趁机回顾了一下各种异步编程的实现。</p><p>这篇文章会涉及到回调、Promise、反应式、async/await、用户态线程等异步编程的实现方案。如果你熟悉它们中的一两种，那应该也能很快理解其他几个。</p><a id="more"></a><h2 id="为什么需要异步">为什么需要异步？</h2><p>操作系统可以看作是个虚拟机（VM），进程生活在操作系统创造的虚拟世界里。进程不用知道到底有多少 core 多少内存，只要进程不要索取的太过分，操作系统就假装有无限多的资源可用。</p><p>基于这个思想，线程（Thread）的个数并不受硬件限制：你的程序可以只有一个线程、也可以有成百上千个。操作系统会默默做好调度，让诸多线程共享有限的 CPU 时间片。这个调度的过程对线程是<strong>完全透明</strong>的。</p><p>那么，操作系统是怎样做到在线程无感知的情况下调度呢？答案是<strong>上下文切换（Context Switch）</strong>，简单来说，操作系统利用软中断机制，把程序从任意位置打断，然后保存当前所有寄存器——包括最重要的指令寄存器 PC 和栈顶指针 SP，还有一些线程控制信息（TCB），整个过程会产生<a href="https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html" target="_blank" rel="noopener">数个微秒的 overhead</a>。</p><p><img src="/images/2019/01/cpu-threads-illustration.jpg"></p><p>然而作为一位合格的程序员，你一定也听说过，线程是昂贵的：</p><ul><li>线程的上下文切换有不少的代价，占用宝贵的 CPU 时间；</li><li>每个线程都会占用一些（至少 1 页）内存。</li></ul><p>这两个原因驱使我们<strong>尽可能避免创建太多的线程</strong>，而异步编程的目的就是消除 IO wait 阻塞——绝大多数时候，这是我们创建一堆线程、甚至引入线程池的罪魁祸首。</p><h2 id="continuation">Continuation</h2><p>回调函数知道的人很多，但了解 Continuation 的人不多。Continuation 有时被晦涩地翻译成“计算续体”，咱们还是直接用单词好了。</p><p><strong>把一个计算过程在中间打断，剩下的部分用一个对象表示，这就是 Continuation</strong>。操作系统暂停一个线程时保存的那些现场数据，也可以看作一个 Continuation。有了它，我们就能在这个点接着刚刚的断点继续执行。</p><p>打断一个计算过程听起来很厉害吧！实际上它每时每刻都在发生——假设函数 <code>f()</code> 中间调用了 <code>g()</code>，那 <code>g()</code> 运行完成时，要返回到 <code>f()</code> 刚刚调用 <code>g()</code> 的地方接着执行。这个过程再自然不过了，以至于所有编程语言（汇编除外）都把它掩藏起来，让你在编程中感觉不到调用栈的存在。</p><p><img src="/images/2019/01/call-stack-and-continuation.jpg"></p><p>操作系统用昂贵的软中断机制实现了栈的保存和恢复。那有没有别的方式实现 Continuation 呢？最朴素的想法就是，把所有用得到的信息包成一个函数对象，在调用 <code>g()</code> 的时候一起传进去，并约定：一旦 <code>g()</code> 完成，就拿着结果去调用这个 Continuation。</p><p>这种编程模式被称为 Continuation-passing style（CPS）：</p><ol type="1"><li>把调用者 <code>f()</code> 还未执行的部分包成一个函数对象 <code>cont</code>，一同传给被调用者 <code>g()</code>；</li><li>正常运行 <code>g()</code> 函数体；</li><li><code>g()</code> 完成后，连同它的结果一起回调 <code>cont</code>，从而继续执行 <code>f()</code> 里剩余的代码。</li></ol><p>再拿 Wikipedia 上的定义巩固一下：</p><blockquote><p>A function written in continuation-passing style takes an extra argument: an explicit "continuation", i.e. a function of one argument. When the CPS function has computed its result value, it "returns" it by calling the continuation function with this value as the argument.</p><p>CPS 风格的函数带一个额外的参数：一个显式的 Continuation，具体来说就是个仅有一个参数的函数。当 CPS 函数计算完返回值时，它“返回”的方式就是拿着返回值调用那个 Continuation。</p></blockquote><p>你应该已经发现了，这也就是回调函数，我只是换了个名字而已。</p><h2 id="异步的朴素实现callback">异步的朴素实现：Callback</h2><p>光有回调函数其实并没有卵用。对于纯粹的计算工作，Call Stack 就很好，为何要费时费力用回调来做 Continuation 呢？你说的对，但仅限于没有 IO 的情况。我们知道 IO 通常要比 CPU 慢上好几个数量级，在 BIO 中，线程发起 IO 之后只能暂停，然后等待 IO 完成再由操作系统唤醒。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var input = recv_from_socket()  <span class="comment">// Block at syscall recv()</span></span><br><span class="line">var result = calculator.calculate(input)</span><br><span class="line">send_to_socket(result) <span class="comment">// Block at syscall send()</span></span><br></pre></td></tr></table></figure><p>而异步 IO 中，进程发起 IO 操作时也会一并输入回调（也就是 Continuation），这大大解放了生产力——现场无需等待，可以立即返回去做其他事情。一旦 IO 成功后，AIO 的 Event Loop 会调用刚刚设置的回调函数，把剩下的工作完成。这种模式有时也被称为 Fire and Forget。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">recv_from_socket((input) -&gt; &#123;</span><br><span class="line">    var result = calculator.calculate(input)</span><br><span class="line">    send_to_socket(result) <span class="comment">// ignore result</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>就这么简单，通过我们自己实现的 Continuation，线程不再受 IO 阻塞，可以自由自在地跑满 CPU。</p><h2 id="一颗语法糖promise">一颗语法糖：Promise</h2><p>回调函数哪里都好，就是不大好用，以及太丑了。</p><p>第一个问题是可读性大大下降，由于我们绕开操作系统自制 Continuation，所有函数调用都要传入一个 lambda 表达式，你的代码看起来就像要起飞一样，缩进止不住地往右挪（the "Callback Hell"）。</p><p>第二个问题是各种细节处理起来很麻烦，比如，考虑下异常处理，看来传一个 Continuation 还不够，最好再传个异常处理的 callback。</p><p><strong>Promise 是对异步调用结果的一个封装</strong>，在 Java 中它叫作 CompletableFuture (JDK8) 或者 ListenableFuture (Guava)。Promise 有两层含义：</p><p>第一层含义是：<strong>我现在还不是真正的结果，但是承诺以后会拿到这个结果</strong>。这很容易理解，异步的任务迟早会完成，调用者如果比较蠢萌，他也可以用 <code>Promise.get()</code> 强行要拿到结果，顺便阻塞了当前线程，异步变成了同步。</p><p>第二层含义是：<strong>如果你（调用者）有什么吩咐，就告诉我好了</strong>。这就有趣了，换句话说，回调函数不再是传给 <code>g()</code>，而是 <code>g()</code> 返回的 Promise，比如之前那段代码，我们用 Promise 来书写，看起来顺眼了不少。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var promise_input = recv_from_socket()</span><br><span class="line">promise_input.then((input) -&gt; &#123;</span><br><span class="line">    var result = calculator.calculate(input)</span><br><span class="line">    send_to_socket(result) <span class="comment">// ignore result</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>Promise 改善了 Callback 的可读性，也让异常处理稍稍优雅了些，但终究是颗语法糖。</p><h2 id="反应式编程">反应式编程</h2><p>反应式（Reactive）最早源于函数式编程中的一种模式，随着微软发起 ReactiveX 项目并一步步壮大，被移植到各种语言和平台上。Reactive 最初在 GUI 编程中有广泛的应用，由于异步调用的高性能，很快也在服务器后端领域遍地开花。</p><p><strong>Reactive 可以看作是对 Promise 的极大增强，相比 Promise，反应式引入了流（Flow）的概念</strong>。ReactiveX 中的事件流从一个 Observable 对象流出，这个对象可以是一个按钮，也可以是 Restful API，总之，它能被外界触发。与 Promise 不同的是，事件可能被触发多次，所以处理代码也会被多次调用。</p><p>一旦允许调用多次，<strong>从数据流动的角度看，事实上模型已经是 Push 而非 Pull</strong>。那么问题来了，如果调用频率非常高，以至于我们处理速度跟不上了怎么办？所以 RX 框架又引入了 Backpressure 机制来进行流控，最简单的流控方式就是：一旦 buffer 满，就丢弃掉之后的事件。</p><p>ReactiveX 框架的另一个优点是内置了很多好用的算子，比如：<code>merge</code>（Flow 合并），<code>debounce</code>（开关除颤）等等，方便了业务开发。下面是一个 RxJava 的例子：</p><p><img src="/images/2019/01/rxjava-example.gif"></p><h2 id="cps-变换coroutine-与-asyncawait">CPS 变换：Coroutine 与 async/await</h2><p>无论是反应式还是 Promise，说到底仍然没有摆脱手工构造 Continuation：开发者要把业务逻辑写成回调函数。对于线性的逻辑基本可以应付自如，但是如果逻辑复杂一点呢？（比如，考虑下包含循环的情况）</p><p><img src="/images/2019/01/csp-background-problems.jpg"></p><p>有些语言例如 C#，JavaScript 和 Python 提供了 <code>async/await</code> 关键字。与 Reactive 一样，这同样出自微软 C# 语言。在这些语言中，你会感到前所未有的爽感：异步编程终于摆脱了回调函数！唯一要做的只是在异步函数调用时加上 <code>await</code>，编译器就会自动把它转化为协程（Coroutine），而非昂贵的线程。</p><p>魔法的背后是 CPS 变换，<strong>CPS 变换把普通函数转换成一个 CPS 的函数，即 Continuation 也能作为一个调用参数</strong>。函数不仅能从头运行，还能根据 Continuation 的指示继续某个点（比如调用 IO 的地方）运行。</p><p>例子可以参见我的<a href="/completable-future-not-so-bad">下一篇文章</a>。由于代码太长，就不贴在这儿了。</p><p>可以看到，<strong>函数已经不再是一个函数了，而是变成一个状态机</strong>。每次 call 它、或者它 call 其他异步函数时，状态机都会做一些计算和状态轮转。说好的 Continuation 在哪呢？就是对象自己（<code>this</code>）啊。</p><p>CPS 变换实现非常复杂，尤其是考虑到 try-catch 之后。但是没关系，复杂性都在编译器里，用户只要学两个关键词即可。这个特性非常优雅，比 Java 那个废柴的 <code>CompletableFuture</code> 不知道高到哪去了。（更新：<a href="/completable-future-not-so-bad">也没有那么废柴啦</a>）</p><blockquote><p>JVM 上也有一个实现：<a href="https://github.com/electronicarts/ea-async" target="_blank" rel="noopener">electronicarts/ea-async</a>，原理和 C# 的 async/await 类似，在编译期修改 Bytecode 实现 CPS 变换。</p></blockquote><h2 id="终极方案用户态线程">终极方案：用户态线程</h2><p>有了 <code>async/await</code>，代码已经简洁很多了，基本上和同步代码无异。是否有可能让异步代码和同步代码完全一样呢？听起来就像免费午餐，但是的确可以做到！</p><p>用户态线程的代表是 Golang。JVM 上也有些实现，比如 <a href="http://docs.paralleluniverse.co/quasar/" target="_blank" rel="noopener">Quasar</a>，不过因为 JDBC、Spring 这些周边生态（它们占据了大部分 IO 操作）的缺失基本没有什么用。</p><p><strong>用户态线程是把操作系统提供的线程机制完全抛弃</strong>，换句话说，不去用这个 VM 的虚拟化机制。比如硬件有 8 个核心，那就创建 8 个系统线程，然后把 N 个用户线程调度到这 8 个系统线程上跑。N 个用户线程的调度在用户进程里实现，由于一切都在进程内部，切换代价要远远小于操作系统 Context Switch。</p><p><img src="/images/2019/01/goroutine-illustration.png"></p><p>另一方面，所有可能阻塞系统级线程的事情，例如 <code>sleep()</code>、<code>recv()</code> 等，用户态线程一定不能碰，否则它一旦阻塞住也就带着那 8 个系统线程中的一个阻塞了。Go Runtime 接管了所有这样的系统调用，并用一个统一的 Event loop 来轮询和分发。</p><p>另外，由于用户态线程很轻量，我们完全没必要再用线程池，如果需要开线程就直接创建。比如 Java 中的 WebServer 几乎一定有个线程池，而 Go 可以给每个请求开辟一个 goroutine 去处理。并发编程从未如此美好！</p><h2 id="总结">总结</h2><p>以上方案中，Promise、Reactive 本质上还是回调函数，只是框架的存在一定程度上降低了开发者的心智负担。而 <code>async/await</code> 和用户态线程的解决方案要优雅和彻底的多，前者通过编译期的 CPS 变换帮用户创造出 CPS 式的函数调用；后者则绕开操作系统、重新实现一套线程机制，一切调度工作由 Runtime 接管。</p><p>不知道是不是因为历史包袱太重，Java 语言本身提供的异步编程支持弱得可怜，即便是 CompletableFuture 还是在 Java 8 才引入，其后果就是很多库都没有异步的支持。虽然 Quasar 在没有语言级支持的情况下引入了 CPS 变换，但是由于缺少周边生态的支持，实际很难用在项目中。</p><h2 id="references">References</h2><ol type="1"><li><a href="https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html" target="_blank" rel="noopener">How long does it take to make a context switch?</a></li><li><a href="http://reactivex.io/" target="_blank" rel="noopener">ReactiveX</a></li><li><a href="https://zhuanlan.zhihu.com/p/25964339" target="_blank" rel="noopener">考不上三本也能给自己心爱的语言加上 Coroutine</a></li><li><a href="http://docs.paralleluniverse.co/quasar/" target="_blank" rel="noopener">Quasar</a></li><li><a href="http://morsmachine.dk/go-scheduler" target="_blank" rel="noopener">The Go scheduler</a></li><li><a href="https://medium.com/@ThatGuyTinus/callbacks-vs-promises-vs-async-await-f65ed7c2b9b4" target="_blank" rel="noopener">Callbacks VS Promises VS Async/Await</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2019/01/banner-async-await.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;近期尝试在搬砖专用语言 Java 上实现异步，起因和过程就不再详述了，总而言之，心中一万头草泥马奔过。但这个过程也没有白白浪费，趁机回顾了一下各种异步编程的实现。&lt;/p&gt;
&lt;p&gt;这篇文章会涉及到回调、Promise、反应式、async/await、用户态线程等异步编程的实现方案。如果你熟悉它们中的一两种，那应该也能很快理解其他几个。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
      <category term="async" scheme="https://ericfu.me/tags/async/"/>
    
      <category term="socket" scheme="https://ericfu.me/tags/socket/"/>
    
  </entry>
  
  <entry>
    <title>JIT 代码生成技术（一）表达式编译</title>
    <link href="https://ericfu.me/code-gen-of-expression/"/>
    <id>https://ericfu.me/code-gen-of-expression/</id>
    <published>2018-11-28T17:42:39.000Z</published>
    <updated>2019-06-04T02:19:09.413Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/12/code-generation-banner.png"></p><p><strong>代码生成（Code Generation）</strong>技术广泛应用于现代的数据系统中。代码生成是将用户输入的表达式、查询、存储过程等现场编译成二进制代码再执行，相比解释执行的方式，运行效率要高得多。尤其是对于计算密集型查询、或频繁重复使用的计算过程，运用代码生成技术能达到数十倍的性能提升。</p><a id="more"></a><h2 id="当我们谈论代码生成时我们在谈论什么">当我们谈论代码生成时我们在谈论什么</h2><p>很多大数据产品都将代码生成技术作为卖点，然而事实上他们往往谈论的不是一件事情。比如，之前就有人提问：Spark 1.x 就已经有代码生成技术，为什么 Spark 2.0 又把代码生成吹了一番？其中的原因在于，虽然都是代码生成，但是各个产品生成代码的粒度是不同的：</p><ul><li>最简单的，例如 Spark 1.4，使用代码生成技术加速<strong>表达式计算</strong>；</li><li>Spark 2.0 支持将同一个 Stage 的<strong>多个算子组合编译</strong>成一段二进制；</li><li>更有甚者，支持将<strong>自定义函数、存储过程</strong>等编译成一段二进制，例如 SQL Server。</li></ul><p><img src="/images/2018/12/three-levels-of-code-gen.png"></p><p>本文主要讲上面最简单的表达式编译。让我们通过一个简单的例子，初步了解代码生成的流程。</p><h2 id="解析执行的缺陷">解析执行的缺陷</h2><p>在讲代码生成之前，我们回顾一下解释执行。以上面图中的表达式 <span class="math inline">\(X \times 5 + \log (10)\)</span> 为例，计算过程是一个深度优先搜索（DFS）的过程：</p><ol type="1"><li>调用根节点 <code>+</code> 的 <code>visit()</code> 函数：分别调用左、右子节点的 <code>visit()</code> 再相加；</li><li>调用乘法节点 <code>*</code> 的 <code>visit()</code> 函数：分别调用左、右子节点的 <code>visit()</code> 再相乘；</li><li>调用变量节点 <code>X</code> 的 <code>visit()</code> 函数：从环境中读取 <span class="math inline">\(X\)</span> 的值以及类型。</li></ol><p>（……略）最终，DFS 回到根节点，得到最终结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Object <span class="title">visitPlus</span><span class="params">(CalculatorParser.PlusContext ctx)</span> </span>&#123;</span><br><span class="line">    Object left = visit(ctx.plusOrMinus());</span><br><span class="line">    Object right = visit(ctx.multOrDiv());</span><br><span class="line">    <span class="keyword">if</span> (left <span class="keyword">instanceof</span> Long &amp;&amp; right <span class="keyword">instanceof</span> Long) &#123;</span><br><span class="line">        <span class="keyword">return</span> (Long) left + (Long) right;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left <span class="keyword">instanceof</span> Long &amp;&amp; right <span class="keyword">instanceof</span> Double) &#123;</span><br><span class="line">        <span class="keyword">return</span> (Long) left + (Double) right;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left <span class="keyword">instanceof</span> Double &amp;&amp; right <span class="keyword">instanceof</span> Long) &#123;</span><br><span class="line">        <span class="keyword">return</span> (Double) left + (Long) right;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left <span class="keyword">instanceof</span> Double &amp;&amp; right <span class="keyword">instanceof</span> Double) &#123;</span><br><span class="line">        <span class="keyword">return</span> (Double) left + (Double) right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述过程中有几个显而易见的性能问题：</p><ul><li>涉及到大量的<strong>虚函数调用</strong>、即函数绑定的过程，例如 <code>visit()</code> 函数，虚函数调用是一个非确定性的跳转指令， CPU 无法做预测分支，从而导致打断 CPU 流水线；</li><li>在计算之前不能确定类型，因而各个算子的实现中会出现很多<strong>动态类型判断</strong>，例如：如果 <code>+</code> 左边是 DECIMAL 类型，而右边是 DOUBLE，需要先把左边转换成 DOUBLE 再相加；</li><li>递归中的<strong>函数调用打断了计算过程</strong>，不仅调用本身需要额外的指令，而且函数调用传参是通过栈完成的，不能很好的利用寄存器（这一点在现代的编译器和硬件体系中已经有所缓解，但显然比不上连续的计算指令）。</li></ul><h2 id="代码生成基本过程">代码生成基本过程</h2><p>代码生成执行，顾名思义，最核心的部分是生成出我们需要的执行代码。</p><p>拜编译器所赐，我们并不需要写难懂的汇编或字节码。在 native 程序中，通常用 LLVM 的中间语言（IR）作为生成代码的语言。而 JVM 上更简单，因为 Java 编译本身很快，利用运行在 JVM 上的轻量级编译器 janino，我们可以直接生成 Java 代码。</p><p>无论是 LLVM IR 还是 Java 都是静态类型的语言，在生成的代码中再去判断类型显然不是个明智的选择。<strong>通常的做法是在编译之前就确定所有值的类型</strong>。幸运的是，表达式和 SQL 执行计划都可以事先做类型推导。</p><p>所以，综上所述，代码生成往往是个 2-pass 的过程：<strong>先做类型推导，再做真正的代码生成</strong>。第一步中，类型推导的同时其实也是在检查表达式是否合法，因此很多地方也称之为<strong>验证（Validate）</strong>。</p><p>在代码生成完成后，<strong>调用编译器编译</strong>，我们得到了所需的函数（类），调用它即可得到计算结果。如果函数包含参数，例如上面例子中的 <code>X</code>，每次计算可以传入不同的参数，<strong>编译一次、计算多次</strong>。</p><p><strong>以下的代码实现都可以在 GitHub 项目 <a href="https://github.com/fuyufjh/calculator" target="_blank" rel="noopener">fuyufjh/calculator</a> 找到。</strong></p><h2 id="验证validate">验证（Validate）</h2><blockquote><p>为了尽可能简单，例子中仅涉及两种类型：Long 和 Double</p></blockquote><p><img src="/images/2018/12/ast-to-algebra-tree.png"></p><p>这一步中，我们将合法的表达式 AST 转换成 Algebra Node，这是一个递归语法树的过程，下面是一个例子（由于 Plus 接收 Long/Double 的任意类型组合，所以此处没有做类型检查）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> AlgebraNode <span class="title">visitPlus</span><span class="params">(CalculatorParser.PlusContext ctx)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> PlusNode(visit(ctx.plusOrMinus()), visit(ctx.multOrDiv()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AlgebraNode 接口定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">AlgebraNode</span> </span>&#123;</span><br><span class="line">    <span class="function">DataType <span class="title">getType</span><span class="params">()</span></span>; <span class="comment">// Validate 和 CodeGen 都会用到</span></span><br><span class="line">    <span class="function">String <span class="title">generateCode</span><span class="params">()</span></span>; <span class="comment">// CodeGen 使用</span></span><br><span class="line">    <span class="function">List&lt;AlgebraNode&gt; <span class="title">getInputs</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现类大致与 AST 的中的节点相对应，如下图。</p><figure><img src="/images/2018/12/algebra-node-uml.png" alt="Package algebra"><figcaption>Package algebra</figcaption></figure><p>对于加法，类型推导的过程很简单——如果两个操作数都是 Long 则结果为 Long，否则为 Double。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> DataType <span class="title">getType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (dataType == <span class="keyword">null</span>) &#123;</span><br><span class="line">        dataType = inferTypeFromInputs();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dataType;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> DataType <span class="title">inferTypeFromInputs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (AlgebraNode input : getInputs()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (input.getType() == DataType.DOUBLE) &#123;</span><br><span class="line">            <span class="keyword">return</span> DataType.DOUBLE;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> DataType.LONG;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="生成代码">生成代码</h2><p>依旧以加法为例，利用上面实现的 <code>getType()</code>，我们可以确定输入、输出的类型，生成出强类型的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> String <span class="title">generateCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (getLeft().getType() == DataType.DOUBLE &amp;&amp; getRight().getType() == DataType.DOUBLE) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"("</span> + getLeft().generateCode() + <span class="string">" + "</span> + getRight().generateCode() + <span class="string">")"</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (getLeft().getType() == DataType.DOUBLE &amp;&amp; getRight().getType() == DataType.LONG) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"("</span> + getLeft().generateCode() + <span class="string">" + (double)"</span> + getRight().generateCode() + <span class="string">")"</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (getLeft().getType() == DataType.LONG &amp;&amp; getRight().getType() == DataType.DOUBLE) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"((double)"</span> + getLeft().generateCode() + <span class="string">" + "</span> + getRight().generateCode() + <span class="string">")"</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (getLeft().getType() == DataType.LONG &amp;&amp; getRight().getType() == DataType.LONG) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"("</span> + getLeft().generateCode() + <span class="string">" + "</span> + getRight().generateCode() + <span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意，目前代码还是以 String 形式存在的，递归调用的过程中通过字符串拼接，一步步拼成完整的表达式函数。</p><p>以表达式 <code>a + 2*3 - 2/x + log(x+1)</code> 为例，最终生成的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(((<span class="keyword">double</span>)(a + (<span class="number">2</span> * <span class="number">3</span>)) - ((<span class="keyword">double</span>)<span class="number">2</span> / x)) + java.lang.Math.log((x + (<span class="keyword">double</span>)<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><p>其中，<code>a</code>、<code>x</code> 都是未知数，但类型是已经确定的，分别是 Long 型和 Double 型。</p><h2 id="编译器编译">编译器编译</h2><p><a href="https://janino-compiler.github.io/janino/" target="_blank" rel="noopener">Janino</a> 是一个流行的轻量级 Java 编译器，与常用的 <code>javac</code> 相比它最大的优势是：可以在 JVM 上直接调用，直接在进程内存中运行编译，速度很快。</p><p>上述代码仅仅是一个表达式、并不是完整的 Java 代码，但 janino 提供了方便的 API 能直接编译表达式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ExpressionEvaluator evaluator = <span class="keyword">new</span> ExpressionEvaluator();</span><br><span class="line">evaluator.setParameters(parameterNames, parameterTypes); <span class="comment">// 输入参数名及类型</span></span><br><span class="line">evaluator.setExpressionType(rootNode.getType() == DataType.DOUBLE ? <span class="keyword">double</span>.class : <span class="keyword">long</span>.class); <span class="comment">// 输出类型</span></span><br><span class="line">evaluator.cook(code); <span class="comment">// 编译代码</span></span><br></pre></td></tr></table></figure><p>实际上，你也可以手工拼接出如下的类代码，交给 janino 编译，效果是完全相同的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGeneratedClass</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">calculate</span><span class="params">(<span class="keyword">long</span> a, <span class="keyword">double</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (((<span class="keyword">double</span>)(a + (<span class="number">2</span> * <span class="number">3</span>)) - ((<span class="keyword">double</span>)<span class="number">2</span> / x)) + java.lang.Math.log((x + (<span class="keyword">double</span>)<span class="number">1</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，依次输入所有参数即可调用刚刚编译的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Object result = evaluator.evaluate(parameterValues);</span><br></pre></td></tr></table></figure><h2 id="references">References</h2><ul><li><a href="https://github.com/apache/spark" target="_blank" rel="noopener">Apache Spark - GitHub</a></li><li><a href="https://janino-compiler.github.io/janino" target="_blank" rel="noopener">Janino by janino-compiler</a></li><li><a href="https://github.com/fuyufjh/calculator" target="_blank" rel="noopener">fuyufjh/calculator: A simple calculator to demonstrate code gen technology</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/12/code-generation-banner.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码生成（Code Generation）&lt;/strong&gt;技术广泛应用于现代的数据系统中。代码生成是将用户输入的表达式、查询、存储过程等现场编译成二进制代码再执行，相比解释执行的方式，运行效率要高得多。尤其是对于计算密集型查询、或频繁重复使用的计算过程，运用代码生成技术能达到数十倍的性能提升。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Calcite 对 Volcano 优化器优先队列的实现</title>
    <link href="https://ericfu.me/understand-importance-in-calcite/"/>
    <id>https://ericfu.me/understand-importance-in-calcite/</id>
    <published>2018-11-05T07:00:44.000Z</published>
    <updated>2019-06-04T02:19:09.413Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Calcite 中的 VolcanoPlanner 是对 Volcano/Cascades 优化器的实现。我们知道，Volcano 优化器是在搜索空间中用动态规划（DP）的方式寻找最优解，即使在用了 DP 的情况下，我们也不大可能把搜索空间遍历完。Volcano 的解决方案是定义一个优先队列，优先采用看起来更有希望的 Rule。</p><p>于是问题来了，怎样定义一个 Rule 的优先级？论文中并没有给出答案。Calcite 代码中为此定义了 Importance 的概念。然而相关的资料非常少，本文总结一下我自己的猜测和理解，如果你有不同的观点，欢迎留言讨论。</p><a id="more"></a><h2 id="术语">术语</h2><p>本文假设读者已经充分理解 Volcano 优化器。对以下概念有疑问的，请参考 Valcano/Cascades 原论文。</p><ul><li><strong>RelSet</strong> 描述一组逻辑上相等的 Relation Expression</li><li><strong>RelSubset</strong> 描述一组物理上相等的 Relation Expression，即具有相同的 Physical Properties</li><li><strong>RuleMatch</strong> 描述一次成功的匹配，包含 Rule 和被匹配的节点</li><li><strong>Importance</strong> 描述 RuleMatch 的重要程度，越大越应该优先处理</li></ul><h2 id="基本原则">基本原则</h2><p>为了能在短时间内得到一个较优解，我们的基本原则是：<strong>尽量对代价大的做优化</strong>，从而尽可能在有限的优化次数内获得更大的收益。这又可以分成三个方面来说：</p><ol type="1"><li>优先应用 Transformation Rules 生成各式各样的关系表达式（即优先进行 explore 过程）；</li><li>一般来说，父节点比子节点数据量更大，所以优先处理父节点；</li><li>同级的节点中，代价大的一边应该得到更多的优化。</li></ol><p>为了达成 1，我们只要把逻辑算子的代价设为无穷大即可。为了达成 2、3，我们将 importance 和 cost 关联起来——简单来说就是 cost 越大、importance 也越大。</p><h2 id="实现分析">实现分析</h2><p>原理上说，RuleQueue 是一个优先队列，包含当前所有可行的 RuleMatch，<code>findBestExpr()</code> 时每次循环中我们从中取出优先级最高的并 apply，再根据 apply 的结果更新队列……如此往复，直到满足终止条件。</p><p>但因为性能原因，实际上 RuleQueue 没有使用最大堆之类的数据结构，而是每次选出 importance 最大的那个。这是因为经常需要对 RelSubset 的 importance 做大量调整，用最大堆处理得不偿失。</p><p>RuleMatch 的 importance 定义为以下两个中比较大的一个：</p><ol type="1"><li>输入的 RelSubset 的 importance</li><li>输出的 RelSubset 的 importance</li></ol><blockquote><p>以上参考 <code>VolcanoRuleMatch:computeImportance</code></p></blockquote><p>那 RelSubset 的 importance 如何决定？这边的实现比较 tricky：RuleQueue 的成员变量 <code>subsetImportances</code> 中保存了各个 RelSubset 的 importance，但这并不是 <code>getImportance()</code> 返回的结果。为了区分清楚，我们把 <code>getImportance()</code> 返回的结果称为调整后的 importance，把 <code>subsetImportances</code> 里存的值称为真实 importance。</p><p><strong>调整后的 importance</strong> 定义为以下两个中比较大的一个：</p><ul><li>该 RelSubset 本身的真实 importance</li><li>逻辑上相等的（即位于同一个 RelSet 中）任意一个 RelSubset 的真实 importance 除以 2</li></ul><p>之所以要这么做，注释中的解释是让 Conversion 尽快发生。</p><blockquote><p>以上参考 <code>RuleQueue:getImportance(RelSubset)</code></p></blockquote><p>下一个问题，<strong>真实 importance</strong> 怎么计算呢？</p><ul><li>根节点的 importance 始终是 1.0</li><li>否则，假设它父节点的代价是 <span class="math inline">\(c_{parent}\)</span>，这个节点本身的代价是 <span class="math inline">\(c_{child}\)</span>，则定义节点本身的 <span class="math inline">\(I_{child} =\frac{c_{child}}{c_{parent}} I_{parent}\)</span></li></ul><p><img src="/images/2018/11/calcite-importance-parent-child.png"></p><p>这里说的 cost 是 RelSubset 的 cost，也就是当前这个 RelSubset 的中最佳 Physical Plan 的 cost。DP 算法会保留每个 RelSubset 的最佳 plan 以及对应 cost。</p><blockquote><p>以上参考 <code>RuleQueue:computeImportance(RelSubset)</code></p></blockquote><p>这个定义又引出了下面两个问题：</p><p><strong>1. 如果一个 RelSubset 里还没有 Physical Plan，那它的 cost 是无穷大，怎么处理？</strong></p><ul><li><p>初始设置为 <span class="math inline">\(0.9^n\)</span>，其中 <span class="math inline">\(n\)</span> 是 RelSet 所在的层数（参考 <code>VolcanoPlanner:setInitialImportance</code>）</p></li><li><p>其他时候，比例 <span class="math inline">\(\frac{c_{child}}{c_{parent}}\)</span> 限制最大不超过 <span class="math inline">\(0.99\)</span>（参考 <code>RuleQueue:computeImportanceOfChild</code>）</p></li></ul><p>PS. 理论上只要是一个小于 1 的系数都可以，不知道为什么这里两个系数不一样。</p><p><strong>2. 如果某个 RelSubset 的 cost 降低了（例如找到了一种 Physical Plan），那么 importance 也应该相应的被更新。</strong></p><ul><li>要更新的不仅是该 Plan 本身所在的一个或多个 RelSubset，还有可能是这些 RelSubset 的父节点、父节点的父节点……所以这是一个向上递归的过程。（参考 <code>RelSubset:propagateCostImprovements</code>）</li></ul><h2 id="references">References</h2><ol type="1"><li><a href="https://pdfs.semanticscholar.org/a817/a3e74d1663d9eb35b4baf3161ab16f57df85.pdf" target="_blank" rel="noopener">The Volcano Optimizer Generator: Extensibility and Efficient Search - Goetz Graefe</a></li><li>The Cascades Framework for Query Optimization - Goetz Graefe</li><li><a href="https://github.com/apache/calcite" target="_blank" rel="noopener">Apache Calcite Source Code</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Calcite 中的 VolcanoPlanner 是对 Volcano/Cascades 优化器的实现。我们知道，Volcano 优化器是在搜索空间中用动态规划（DP）的方式寻找最优解，即使在用了 DP 的情况下，我们也不大可能把搜索空间遍历完。Volcano 的解决方案是定义一个优先队列，优先采用看起来更有希望的 Rule。&lt;/p&gt;
&lt;p&gt;于是问题来了，怎样定义一个 Rule 的优先级？论文中并没有给出答案。Calcite 代码中为此定义了 Importance 的概念。然而相关的资料非常少，本文总结一下我自己的猜测和理解，如果你有不同的观点，欢迎留言讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
      <category term="optimizer" scheme="https://ericfu.me/tags/optimizer/"/>
    
  </entry>
  
  <entry>
    <title>从 F1 Query 论文看 SQL 查询的执行模式</title>
    <link href="https://ericfu.me/f1-query-and-approaches-of-sql-execution/"/>
    <id>https://ericfu.me/f1-query-and-approaches-of-sql-execution/</id>
    <published>2018-10-14T06:09:12.000Z</published>
    <updated>2019-06-04T02:27:28.699Z</updated>
    
    <content type="html"><![CDATA[<style type="text/css">.image-captain {    margin-top: -20px;}</style><p><img src="/images/2018/10/banner-lakeview.jpg"></p><p>F1 是起源于 Google AdWords 的分布式 SQL 查询引擎，跟底下的 Spanner 分布式存储搭配，开启了分布式关系数据库——所谓 NewSQL 的时代。我们今天说的是 F1 团队在 VLDB2018 上发的文章 <a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf" target="_blank" rel="noopener">F1 Query: Declarative Querying at Scale</a>，它和之前我们说的 F1 几乎是两个东西。</p><p>F1 Query 是一个分布式的 SQL 执行引擎，现在大数据领域流行的 Presto、Spark SQL、Hive 等等，都可以算在这个范畴里。类似地，F1 Query 也支持对各种不同数据源的查询，既可以是传统的关系表、也可以是 Parquet 这样的半结构化数据。</p><a id="more"></a><p>这样一来，不同数据格式的壁垒也被打破了：你可以在一个系统里完成对不同数据源的 Join，无论数据以什么形式存放在哪里。商业上管这个叫 Federated Query 或者 DataLake，几家云计算巨头都有类似的产品。</p><p><strong>那 F1 Query 的贡献在哪里呢？</strong></p><p>F1 Query 定义了三种不同类型的查询执行模式，根据查询的数据量大小或执行时间，将用户查询划分成：</p><ol type="1"><li>单机执行（Centralized Execution）</li><li>分布式执行（Distributed Execution）</li><li>批处理执行（Batch Execution）</li></ol><p>前两个是交互式的，即客户端会等待结果返回。最后一个批处理更像是 ETL：客户端输入任务之后就不再管了，查询结果会被写到指定的地方。</p><h2 id="单机执行">单机执行</h2><p>单机执行对应我们熟悉的 OLTP 查询，例如单表点查、带索引的 Join 等。这类查询本身已经足够简单，只需几毫秒就能做完，处理它们的最好方式就是在收到请求的机器上立即执行。</p><p>在 F1 Query 系统中有 F1 Server 和 F1 Worker 等角色。F1 Server 负责接收客户端请求，如果它判断这个查询应当使用单机而不是分布式方式执行，它就亲力亲为、直接执行并返回结果。</p><p>这样的行为和绝大多数单机 OLTP 数据库是一致的，例如 MySQL 采用的是 Thread Pool + Dispatcher 的处理模型，Thread Pool 的规模是一定的，Dispatcher 根据高低优先级分派执行任务。最终一个请求只会被一个线程处理，换句话说，对某个查询来说其执行过程是单线程的。</p><img src="/images/2018/10/mysql-thread-group.jpg"><p class="image-captain">▲ <em>MySQL 的线程池处理模型，一般存在多个 Thread Group，图中描绘了一个 Thread Group</em></p><p>F1 Query 单机查询的执行器同样也是教科书式的 Valcano 模型，但也无可厚非——对 OLTP 来说这已经足够好。如下图所示，从顶层算子开始递归地调用 <code>GetNext()</code>，每次取出一行数据，直到没有更多数据为止。各个算子只需要实现 <code>GetNext()</code> 接口即可，简单清晰。</p><p><img src="/images/2018/10/f1-query-valcano.jpg"></p><h2 id="分布式执行">分布式执行</h2><p>F1 Query 对更复杂的查询，例如没有索引的 Join 或聚合等，则采取分布式查询的方式。大部分 OLAP 查询、尤其是 Ad-hoc 的查询都落在这一分类中。这种情况下，分布式导致的网络、调度等 Overhead 已经远小于查询本身的成本；而且随着数据量的增加，单节点内存显然不够用了。</p><p><img src="/images/2018/10/f1-query-system-arch.jpg"></p><p class="image-captain">▲ <em>F1 Query 的系统架构，主要包含 F1 Master、F1 Server、F1 Worker 三个角色，其他 Catalog、UDF Server、Batch Metadata 用于存储查询相关的 Metadata 等</em></p><p>这时，上图中的 F1 Worker 就派上用场了。<strong>F1 Server 此时仅仅作为协调者存在，将任务分配给多个 Worker</strong>，直到 Worker 的任务全都完成，再把结果汇总发给客户端。</p><p>这个模式眼熟吗？你可能会想到 Greenplum 这类的数据仓库，已经很接近了。最相似的我认为是 Presto。Presto 是 Facebook 开发的一套分布式 SQL 引擎，如果单单只看 F1 Query 的分布式查询，和 Presto 大同小异。</p><p>与单机执行不同的是，<strong>分布式查询中的算子可以有多个实例（Instance）并行执行，每个实例负责其中一部分数据</strong>。在 F1 Query 里这样一个数据分片被称为 Fragment，在 Spark SQL 里叫 Partition，在 Presto 里叫 Split。</p><p><img src="/images/2018/10/f1-query-fragments.jpg"></p><p>下面的例子是一个 Join-Aggregation-Sort 的查询，它分成了 4 个阶段：</p><ol type="1"><li><code>Scan(Clicks)</code> 被分配给 1000 个 F1 Worker 上并行拉取数据，并根据每一行数据的 <code>Hash(AdID)</code> 发送给对应的 <code>HashJoin</code> 分片，即一般说的 shuffle 过程;</li><li><code>Scan(Ads)</code> 被分配给 200 个 F1 Worker 上并行拉取数据，并且也以同样的方式做 shuffle；</li><li><code>HashJoin</code> 及 <code>PartialAggregation</code>：根据 Join Key 分成了 1000 个并行任务，各自做 Join 计算，并做一次聚合；</li><li>最后，F1 Server 把各个分片的聚合结果再汇总起来，返回给客户端。</li></ol><p><img src="/images/2018/10/f1-query-distributed-plan.jpg"></p><p>Presto 具有的缺陷，F1 Query 分布式查询同样也有，比如：</p><ul><li>纯内存的计算方式，无法利用磁盘的存储空间，某些查询可能面临内存不足；</li><li>没有 Fault-tolerance，对于一个涉及上千台 Worker 的查询，任何一台的重启都会导致查询失败。</li></ul><h2 id="批处理执行">批处理执行</h2><p>F1 Query 还有个独特的批处理执行，这个模式定位于更大的数据量、更久的查询时间；另一方面，它的结果不再是返回给客户端，而是将查询结果写到指定的地方，例如 Colossus（第二代 GFS）上。</p><p>上一节我们提道，Presto 的模式没有 Fault-tolerance，这对于长时间运行的批处理任务是致命的，查询失败的概率会大大增加。批处理查询首先要解决的就是 Fault-tolerance 问题：<strong>必须能以某种方式从 Worker 节点的失败中恢复</strong>。</p><p>解决这个问题有两条路可走：一是 MapReduce 的模式，将计算分成若干个阶段（Stage），而中间结果持久化到 HDFS 这样的分布式文件系统上；二是 DAG 模式（例如 Spark RDD），通过记录祖先（Lineage）信息，万一发生节点失败，就通过简单的重算来恢复丢失的数据分片，这样数据就可以放在内存里不用落盘。</p><p>Spark 的做法显然是更先进的，原因有很多，这里只说最重要的 2 条。欲知详情可以看我之前的博客文章<a href="https://ericfu.me/apache-spark-in-nutshell/">《一文读懂 Apache Spark》</a>。</p><ol type="1"><li>Spark 的计算基本在内存中，只有当内存不够时才会溢出到磁盘，而 MR 的中间结果必须写入外部文件系统；</li><li>Spark 可以把执行计划 DAG 中相互不依赖的 Stage 并行执行，而 MR 只能线性地一个接一个 Stage 执行。</li></ol><p>但是出乎意料的是，F1 Query 采用的是前者，也就是 MR 模式。这其中的原因我们不得而知，我猜想和 Google 自家的 FlumeJava 不够给力有关系。</p><p>如下图。左边的 Physical Plan 和上一节的分布式查询是一样的，不同之处是<strong>在批处理模式下，它被转换成一系列的 MR 任务</strong>，之后交给调度器（Scheduler）去处理即可。</p><p><img src="/images/2018/10/f1-query-plan-to-mr-stages.jpg"></p><p>相比分布式执行，批处理模式下各个步骤都会持久化到外部文件系统（因为 MapReduce 的特性所致）。不仅如此，<strong>Pipeline 式的执行也没法进行</strong>。以上一节提到的 HashJoin 为例，左边 <code>Clicks</code> 的 Scan 和 HashJoin 原本是可以 Pipeline 执行的，但是在批处理模式下，必须等到 <code>Scan(Clicks)</code> 这个阶段完成才能进行下一步的 HashJoin 阶段。</p><h2 id="单机并行执行">单机并行执行</h2><p>除了上面聊的 F1 Query 所支持的 3 种查询模式之外，事实上还有一种处理模型位于单线程执行和分布式执行之间：单机的并行执行。初看这似乎与分布式执行很相似，但又有些不同：</p><ul><li>不用考虑单个 Worker 的失败恢复，因为它们都在同一个进程里；</li><li>各个 Worker 线程的内存是共享的，它们之间交换数据无需考虑网络通讯代价。</li></ul><p>这种模式在传统的关系型数据库上很常见，尤其是 Postgres、SQL Server 这类以 OLAP 查询见长的选手。以 Postgres 为例，在开启并行查询的情况下，查询优化器会根据代价选择是否生成并行执行计划；如果生成了并行执行计划，执行器会调度多个 Worker 一起完成工作。</p><p>下图是一个 Postgres 上并行 Hash Join 的例子，从执行计划上看和上一节几乎一样，但是这里不再需要对数据做 Shuffle：Hash Join 所用的 Hash Table 本身是全局共享的。</p><p><img src="/images/2018/10/postgres-parallel-query-example.png"></p><blockquote><p>Parallel Hash Join 并非只有这一种做法。SQL Server 就更接近分布式执行的方案：把 Hash Key 相同的数据 shuffle 到同一个分片上——但这个 shuffle 只是逻辑上的，不需要真的做 IO。</p></blockquote><p>相比分布式查询，<strong>单机并行的最大优势在于响应速度更快</strong>，因为省去了大量的网络 IO 时间，而且调度一个 Worker 线程要比调度一个 Worker 机器快得多。</p><p>但别忘了，单机运算能力的 scale up 成本非常高，并且是存在上限的。对于 Google 之类的互联网公司，绝大部分查询都超出了单机的存储或计算能力，我猜测这也是 F1 Query 并未考虑单机并行的理由。</p><h2 id="对-f1-query-的评价">对 F1 Query 的评价</h2><p>从论文描述的情况来看，F1 Query 还不算个完善、成熟的系统，其定位更像是一个解决业务需求的胶水系统，而非 Spanner 这样的“硬核”技术。它追求的是够用就好。很多地方其实还有不小的改进空间，举几个例子：</p><ul><li>对交互式查询，选择分布式还是单机计算目前还是基于启发式规则。</li><li>三种模式的执行计划是用一样的优化器生成的。但是客观的说，这其中的差别可是不小的。</li><li>优化器是基于规则的。之所以不做 CBO，论文给出的解释是数据源众多，不容易估算代价。</li><li>批处理模式下用 DAG 取代 MR 的模式是更好的选择。</li></ul><p>F1 Query 希望<strong>用一套系统解决所有 OLTP、OLAP、ETL 需求、用一套系统访问数据中心里各种格式的数据</strong>，这两点才是 F1 Query 的核心竞争力。</p><h2 id="sql-执行模式总结">SQL 执行模式总结</h2><p>从数据库的视角看，理想的数据库应当隐藏掉查询执行的种种细节，只要用户输入一个声明（例如 SQL），就能以最优的方式执行查询给出答案。F1 Query 做了个勇敢的尝试，<strong>它将多种执行模型揉合在一个系统中，共享同一套优化器和算子</strong>，以较低的开发成本获得其中最优的执行性能（在理想情况下）。</p><p>下面的表格总结了 4 种执行模式的优势和不足。</p><table style="width:100%;"><colgroup><col style="width: 17%"><col style="width: 16%"><col style="width: 23%"><col style="width: 20%"><col style="width: 21%"></colgroup><thead><tr class="header"><th></th><th>单线程</th><th>单机并行</th><th>分布式并行</th><th>批处理</th></tr></thead><tbody><tr class="odd"><td>代表系统</td><td>MySQL / Oracle</td><td>Postgres / MSSQL</td><td>Presto / Greenplum</td><td>Spark (DAG) / Hive (MR)</td></tr><tr class="even"><td>硬件架构</td><td>单核</td><td>SMP / NUMA</td><td>MPP</td><td>MPP</td></tr><tr class="odd"><td>伸缩性</td><td>无</td><td>Scale Up</td><td>弹性 Scale Out</td><td>弹性 Scale Out</td></tr><tr class="even"><td>Fault-Toralence</td><td>无</td><td>无</td><td>重试整个查询</td><td>Worker 级 fail-over</td></tr><tr class="odd"><td>典型数据量</td><td>若干个 Tuple</td><td>单机内存可容纳</td><td>大数据</td><td>大数据</td></tr><tr class="even"><td>典型响应时间</td><td>毫秒</td><td>数百毫秒</td><td>秒级</td><td>秒级到数小时</td></tr></tbody></table><p>总而言之，所谓 <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" target="_blank" rel="noopener"><em>No Free Lunch</em></a> —— 没有最优的方案，<strong>数据量是决定能选用哪个执行模式的前提</strong>。实践中，先确保数据量能够承载的下，再谈优化也就明白多了。</p><h2 id="references">References</h2><ol type="1"><li><a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf" target="_blank" rel="noopener">F1 Query: Declarative Querying at Scale</a></li><li><a href="https://www.programering.com/a/MTO5YDMwATI.html" target="_blank" rel="noopener">MySQL Thread Pool Implementation</a></li><li><a href="https://tech.meituan.com/presto.html" target="_blank" rel="noopener">Presto 实现原理和美团的使用实践 - 美团技术团队</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;style type=&quot;text/css&quot;&gt;
.image-captain {
    margin-top: -20px;
}
&lt;/style&gt;
&lt;p&gt;&lt;img src=&quot;/images/2018/10/banner-lakeview.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;F1 是起源于 Google AdWords 的分布式 SQL 查询引擎，跟底下的 Spanner 分布式存储搭配，开启了分布式关系数据库——所谓 NewSQL 的时代。我们今天说的是 F1 团队在 VLDB2018 上发的文章 &lt;a href=&quot;http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;F1 Query: Declarative Querying at Scale&lt;/a&gt;，它和之前我们说的 F1 几乎是两个东西。&lt;/p&gt;
&lt;p&gt;F1 Query 是一个分布式的 SQL 执行引擎，现在大数据领域流行的 Presto、Spark SQL、Hive 等等，都可以算在这个范畴里。类似地，F1 Query 也支持对各种不同数据源的查询，既可以是传统的关系表、也可以是 Parquet 这样的半结构化数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>一文读懂 Apache Spark</title>
    <link href="https://ericfu.me/apache-spark-in-nutshell/"/>
    <id>https://ericfu.me/apache-spark-in-nutshell/</id>
    <published>2018-06-12T13:12:00.000Z</published>
    <updated>2019-06-04T02:19:09.413Z</updated>
    
    <content type="html"><![CDATA[<style type="text/css">.image-captain {    margin-top: -20px;}</style><p><img src="/images/2018/06/spark-banner.png"></p><p>Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><p>Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。</p><a id="more"></a><p>Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。</p><h2 id="mapreduce-的问题所在">MapReduce 的问题所在</h2><p>Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。</p><p>MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，<strong>复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来</strong>：</p><p><img src="/images/2018/06/multi-stage-mapreduce.png"></p><p class="image-captain">▲ <em>上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。</em></p><p>这种方式下，<strong>每次中间结果都要写入 HDFS 落盘保存，代价很大</strong>（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。</p><h2 id="spark-与-rdd-模型">Spark 与 RDD 模型</h2><p>针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。</p><p>一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。<strong>Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。</strong>Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。</p><p><img src="/images/2018/06/rdd-example.png"></p><p><strong>RDD 是一个只读的、分区的（partitioned）数据集合</strong>。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。<strong>RDD 通过算子连接构成有向无环图（DAG）</strong>，上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。</p><p>回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以<strong>一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区</strong>。这个过程可以递归的进行下去。</p><p><img src="/images/2018/06/rdd-example-crash.png"></p><p class="image-captain">▲ <em>上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。</em></p><p>Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 <code>collect()</code> 这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类：</p><ul><li><code>map()</code>、<code>filter()</code>、<code>join()</code> 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。</li><li><code>collect()</code>、<code>count()</code>、<code>save()</code> 等算子称为 Action，它们通常是将数据收集起来返回；</li></ul><p><img src="/images/2018/06/spark-rdd-api-example.png"></p><p class="image-captain">▲ <em>上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。</em></p><p>像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。</p><p>此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：<strong>窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</strong>，举例来说：</p><ul><li><code>map()</code>、<code>filter()</code> 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。</li><li><code>groupByKey()</code> 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。</li></ul><p><img src="/images/2018/06/2-kinds-of-dependencies.png"></p><p class="image-captain">▲ <em>左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。</em></p><p>在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。<strong>然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。</strong>整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。</p><blockquote><p>了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。</p></blockquote><h2 id="声明式接口spark-sql">声明式接口：Spark SQL</h2><p>Spark 诞生后，大幅简化了 MapReduce 编程模型，但人们并不满足于此。我们知道，<strong>与命令式（imperative）编程相对的是声明式（declarative）编程，前者需要告诉程序怎样得到我需要的结果，后者则是告诉程序我需要的结果是什么</strong>。举例而言：你想知道，各个部门 <code>&lt;dept_id, dept_name&gt;</code> 中性别为女 <code>'female'</code> 的员工分别有多少？</p><p>命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">employees = db.getAllEmployees()</span><br><span class="line">countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count)</span><br><span class="line">for employee in employees:</span><br><span class="line">    if (employee.gender == &apos;female&apos;)</span><br><span class="line">        countByDept[employee.dept_id] += 1</span><br><span class="line">results = list() // 加上 dept.name 列</span><br><span class="line">depts = db.getAllDepartments()</span><br><span class="line">for dept in depts:</span><br><span class="line">    if (countByDept containsKey dept.id)</span><br><span class="line">        results.add(row(dept.id, dept.name, countByDept[dept.id]))</span><br><span class="line">return results;</span><br></pre></td></tr></table></figure><p>声明式编程中，你只要用关系代数的运算表达出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">employees.join(dept, employees.deptId == dept.id)</span><br><span class="line">         .where(employees.gender == &apos;female&apos;)</span><br><span class="line">         .groupBy(dept.id, dept.name)</span><br><span class="line">         .agg()</span><br></pre></td></tr></table></figure><blockquote><p>等价地，如果你更熟悉 SQL，也可以写成这样：</p></blockquote><blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dept.id, dept.name, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> employees.dept_id == dept.id</span><br><span class="line"><span class="keyword">WHERE</span> employees.gender = <span class="string">'female'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dept.id, dept.name</span><br></pre></td></tr></table></figure></blockquote><p>显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。</p><p>Spark SQL 就是这样一个基于 SQL 的声明式编程接口。<strong>你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。</strong></p><p><img src="/images/2018/06/spark-sql-arch.png"></p><p class="image-captain">▲ <em>上图黄色部分是 Spark SQL 中新增的部分。</em></p><p><strong>DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。</strong>计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。</p><p><strong>Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。</strong>Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。</p><p><img src="/images/2018/06/spark-sql-catalyst.png"></p><p class="image-captain">▲ <em>上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。</em></p><h2 id="流计算框架spark-streaming">流计算框架：Spark Streaming</h2><p>以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。</p><p>Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 <strong>D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。</strong></p><p><img src="/images/2018/06/spark-streaming.png"></p><p>Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势：</p><ul><li>更高的吞吐量（大约是 Storm 的 2-5 倍）</li><li>更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可</li><li>开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算</li></ul><p><img src="/images/2018/06/continuous-vs-spark-d-stream.png"></p><p class="image-captain">▲ <em>上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。</em></p><p>你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？</p><p>答案是通过 RDD：<strong>将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果</strong>。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。</p><p><img src="/images/2018/06/d-stream-state-rdd.png"></p><p class="image-captain">▲ <em>上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。</em></p><p>那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD <code>counts</code> 会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！<strong>因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut</strong>，在它之前更早的 RDD 就可以没有顾虑地清理掉了。</p><blockquote><p>关于流行的几个开源流计算框架的对比，可以参考文章 <a href="https://www.cakesolutions.net/teamblogs/comparison-of-apache-stream-processing-frameworks-part-1" target="_blank" rel="noopener">Comparison of Apache Stream Processing Frameworks</a>。</p></blockquote><h2 id="流计算与-sqlspark-structured-streaming">流计算与 SQL：Spark Structured Streaming</h2><p>Spark 通过 Spark Streaming 拥有了流计算能力，那 Spark SQL 是否也能具有类似的流处理能力呢？答案是肯定的，只要将<strong>数据流建模成一张不断增长、没有边界的表</strong>，在这样的语义之下，很多 SQL 操作等就能直接应用在流数据上。</p><blockquote><p>出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。</p></blockquote><p>很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为<strong>增量化</strong>（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。</p><p><img src="/images/2018/06/spark-structured-streaming-model.png"></p><p class="image-captain">▲ <em>左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。</em></p><p>与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。</p><p><strong>窗口（window）是对过去某段时间的定义。</strong>批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。</p><p><strong>水位（watermark）用来丢弃过早的数据。</strong>在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。</p><p><img src="/images/2018/06/spark-structured-streaming-watermark.png"></p><p class="image-captain">▲ <em>上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。</em></p><p>水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。</p><blockquote><p>关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 <a href="https://flink.apache.org/" target="_blank" rel="noopener">Apache Flink</a>。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。</p></blockquote><h2 id="系统架构">系统架构</h2><p>Spark 中有三个角色：Driver, Worker 和 Cluster Manager。</p><p><img src="/images/2018/06/cluster-overview.png"></p><p><strong>驱动程序（Driver）</strong>即用户编写的程序，对应一个 <code>SparkContext</code>，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。</p><p><strong>Cluster Manager</strong> 顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。</p><p><strong>Worker</strong> 节点负责执行计算任务，上面保存了 RDD 等数据。</p><h2 id="总结">总结</h2><p>Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。</p><p>Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 为 Spark SQL 提供了流计算支持，它将输入的数据流看作不断追加的数据行。</p><h2 id="references">References</h2><ol type="1"><li><a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" target="_blank" rel="noopener">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing (NSDI '12)</a></li><li><a href="https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf" target="_blank" rel="noopener">Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP '13)</a></li><li><a href="https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf" target="_blank" rel="noopener">Spark SQL: Relational Data Processing in Spark (SIGMOD '15)</a></li><li><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark (SIGMOD '18)</a></li><li><a href="https://www.cakesolutions.net/teamblogs/comparison-of-apache-stream-processing-frameworks-part-1" target="_blank" rel="noopener">Comparison of Apache Stream Processing Frameworks</a></li><li><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">Structured Streaming in Apache Spark</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;style type=&quot;text/css&quot;&gt;
.image-captain {
    margin-top: -20px;
}
&lt;/style&gt;
&lt;p&gt;&lt;img src=&quot;/images/2018/06/spark-banner.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。&lt;/p&gt;
&lt;p&gt;Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>处理海量数据：列式存储综述（系统篇）</title>
    <link href="https://ericfu.me/columnar-storage-overview-system/"/>
    <id>https://ericfu.me/columnar-storage-overview-system/</id>
    <published>2018-04-15T15:02:42.000Z</published>
    <updated>2019-06-04T02:19:09.413Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/05/database-pic-banner.jpg"></p><p>在上一篇文章<a href="https://ericfu.me/columnar-storage-overview-storage/">《处理海量数据：列式存储综述（存储篇）》</a> 中，我们介绍了几种 Apache ORC、Dremel 等几种典型列式存储的数据组织格式。实践中，很多数据系统构建在 HDFS 等分布式文件系统之上，使用这些规范的格式保存数据，方便各方的业务进行分析查询。</p><p>本文将介绍若干个典型的列式存储数据库系统。作为完整的 OLAP 或 HTAP 数据库系统，他们大多使用了自主设计的存储方式，运行在多台机器节点上，使用网络进行通讯协作。</p><a id="more"></a><h2 id="c-store-2005-vertica">C-Store (2005) / Vertica</h2><p>大多数 DBMS 都是为写优化，而 C-Store 是第一个为读优化的 OLTP 数据库系统，虽然从今天的视角看它应当算作 HTAP 。在 ad-hoc 的分析型查询、ORM 的在线查询等场景中，大多数操作都是查询而非写入，在这些场景中列式存储能取得更好的性能。像主流的 DBMS 一样，C-Store 支持标准的关系型模型。</p><p>在<a href="https://ericfu.me/columnar-storage-overview-storage/">上一篇文章</a>中，我们已经介绍了 C-Store 特有的 projection 数据模型。这里只做一个简单的回顾：<strong>每个 projection 可以包含一个或多个列，完整的表视图需要通过若干 projection JOIN 得到</strong>。Projection 水平拆分成若干 segments。</p><p>C-Store 的设计考虑到企业级应用的使用模式，在优化 AP 查询的同时兼顾了大多数 DBMS 具有的 TP 查询功能。在 ACID 事务方面同样提供了完整的支持，支持快照（snapshot）读事务和一般的 2PC 读写事务。</p><blockquote><p>通常而言，互联网应用对 DBMS 有较高的并发写入需求，对一致性读、分析型查询的需求不那么强烈。而企业级应用（例如 CRM 系统）的并发写入需求不大，但需要对一致性读、分析型查询等。</p></blockquote><h3 id="系统设计">系统设计</h3><p>C-Store 将其物理存储也就是每个 projection 分成两层，<strong>分别是为写优化的 Writeable Store (WS) 和为读优化的 Read-optimized Store (RS)</strong>。RS 即是基线数据，WS 上暂存了对 RS 数据的变更，二者在读取时需要 merge 得到最新的数据。在上一篇文章的 Apache ORC 格式种，我们也看到了类似的做法（基线数据叠加增量数据）。</p><p><img src="/images/2018/05/c-store-ws-rs.png"></p><p>RS 是一个为读优化的列式存储。RS 中采用之前提到的 projection 数据模型，对不同的列采用了不同的编码方式，根据它是否是 projection 的排序列、以及该列的取值个数，来决定采取何种编码方式。</p><p>WS 用于暂存高性能的写入操作，例如 INSERT、UPDATE 等。为了简化系统的设计，WS 逻辑上仍然按照 projection 的列式模型存储，但是物理上<strong>使用 B 树以满足快速的写入要求</strong>。WS 基于 BerkeleyDB 构建。</p><p>对于某一列中的某个值 v，会有两个映射关系存在：一是 <code>(storage_key -&gt; v)</code>，在 RS 中 storage_key 就是 segment 中的行号，但在 WS 中显式的记录下来；二是 <code>(sort_key -&gt; storage_key)</code>，用于满足主键查询的需求。</p><p>值得一提的是，WS 是一个 MVCC 的存储——它的每个数据都保存了对应的写入事务编号，同一行可能有多个版本同时存在。而 RS 是没有 MVCC 的，你可以将它看作过去某个时间点的快照。</p><p>Tuple Mover 周期性地将 WS 中的数据移动到 RS 中。与大多数 MVCC 系统一样，C-Store 中的更新是通过一个删除加一个插入实现的，Tuple Mover 的主要工作是根据 WS 的数据更新 RS：删掉被删除的行、添加新的行。</p><h3 id="事务支持">事务支持</h3><p>C-Store 认为大多数事务是只读事务，因此采用了 Snapshot Isolation。C-Store 维护两个全局的时间戳：低水位（Low Water Mark, LWM）和高水位（High Water Mark, HWM），允许用户查询介于二者之间的任意时间戳的 Snapshot。时间戳来自中心化的 Time Authority (TA)。</p><p>LWM 对应 RS 即基线数据的版本。Tuple Mover 会保证任何高于 LWM 的修改都不会被移动到 RS 中，因为一旦移动到 RS 也就失去了多版本。</p><p>HWM 由中心的 TA 维护，时间被分成固定长度的 epoch。当各个节点确认 epoch e 中<strong>开始</strong>的写入事务完成时，就会发送一个 <code>Complete(e)</code> 的消息给 TA，当 TA 收集到所有节点的 <code>Complete(e)</code> 将 HWM 置为 e。换句话说，HWM 以前的事务一定是已经完成提交的。</p><p>对于读写事务，C-Store 采用了传统的 2PC。</p><h2 id="monetdb-2012-vectorwise">MonetDB (2012) / VectorWise</h2><p>MonetDB 是一个面向 OLAP 的内存数据。区别于大多数 DBMS 使用的 Valcano 执行模式，MonetDB 使用一种独特的 full materialization 的列式（向量）执行模型，也因此设计了对应的一系列算子以及查询优化器。</p><h3 id="bat-algebra">BAT Algebra</h3><p>MonetDB 独有的列式计算是通过 BAT（Binary Association Table）的运算组成的，<strong>BAT 之间通过算子产生新的 BAT，最终生成查询结果</strong>。每个 BAT 可以简单地理解为一列带有编号的数据 &lt;oid, value&gt;，有些 BAT 来自用户的逻辑表，其他则是运算的结果。每个算子被设计地很紧凑、高效，能充分利用 CPU 流水线的计算能力，这和 CPU 设计的 RISC 思想颇为相似，所以被称为“<strong>数据库查询的 RISC 方案</strong>”。</p><p><img src="/images/2018/05/monetdb-bat-example.png"></p><p>如上图，对于用户一条 SELECT 查询，MonetDB 先将其分解为多次 BAT 的运算，执行计划中的每一步的输入和输出都是 BAT。图中蓝框中为输入的 BAT，其他则是执行产生的运算结果。</p><p>MonetDB 的设计决定了它的计算过程十分耗费内存。MonetDB 利用操作系统的 Memory Mapped File 进行内存管理，不使用的页面可以被换出内存，为执行查询腾出空间。但显然这并不是一个彻底的解决方案。</p><p>VectorWise 使用类似的向量化执行模型，但它尝试在 full materialization 和 Valcano 模型中间寻求一个平衡——它将整个列划分成较小的 block，对 block 进行上述的 column algebra 计算。</p><h2 id="apache-kudu-2015">Apache Kudu (2015)</h2><p>Kudu 是 Cloudera 研发的处理实时数据的 OLAP 数据库。上文提到的 Parquet / ORC 是开源界常用的处理静态数据的方式，为什么说是静态数据呢？因为这些紧凑的格式对数据修改很不友好，且随机读写性能极差，通常只能用于后台 OLAP。</p><p>所以我们看到，很多数据系统都采用动态、静态两套数据，例如：把在线业务数据放在 HBase 中，定期通过 ETL 程序产生Parquet 格式文件放到 HDFS 上，再对其进行统计、归档等。这种定期导入的方式不可避免地会带来小时级的延迟，而且，众所周知维护 ETL 代码是一件费时费力的事情。</p><p>Kudu 试图在 OLAP 与 OLTP 之间寻求一个平衡点——<strong>在保持同一份数据的情况下，既能提供在线业务实时写入的能力，又能支持高效的 OLAP 查询。</strong></p><p>Kudu 采用我们熟悉的半关系型模型，允许用户定义 schema，但是目前并不支持二级索引。</p><p>事务方面，Kudu 默认使用 Snapshot Isolation 一致性模型。此外，如果用户需要一个更强的一致性保证（例如 read own's writes），Kudu 也允许用户指定特定的时间戳，读取这个时间戳的 snapshot。这项功能被集成在 Kudu 的 API 层面，用户可以方便地获得因果（causality）一致性保证。</p><h3 id="系统设计-1">系统设计</h3><p>Kudu 采用了类似 HBase 的 master-slave 架构：中心节点被称作 Kudu Master，数据节点被称作 Tablet Server。一个表的数据被分割成多个 tablets，由它们对应的 Tablet Server 来提供数据读写服务。</p><p>与 HBase 相比，中心节点 Kudu Master 除了存放了 Tablet 的分布信息，还身兼了如下角色：</p><ul><li>Catalog 管理：同步各个库、表的 schema 等元信息、负责协调完成建表等 DDL 操作</li><li>集群协调者：各个 Tablet Server 向其汇报自己的状态、replica 变更等</li></ul><p>Kudu 底层数据文件并没有存储在 HDFS 这样的分布式文件系统上，而是<strong>基于 Raft 算法实现了一套副本同步机制，保障数据不丢失及高可用性</strong>。其中 Raft 算法用于同步数据修改操作的 log，这点和大多数 shared-nothing 架构分布式数据库并无二致。对 Raft 算法有兴趣的同学可以参考<a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">原论文</a>。</p><p>作为列式 OLAP 数据库，Kudu 的磁盘存储是常见的列式方案，很多地方直接复用了 Parquet 的代码。我们知道，紧凑的列式存储难以实现高效的更新操作。Kudu 为了提供实时写入功能，采用了类似 C-Store 中的方案——在不可变的基线数据上，叠加后续的更新数据。</p><p><img src="/images/2018/05/kudu-write-path.png"></p><p>具体来说，Tablet 由 RowSet 组成，而 RowSet 既可以是内存中的 MemRowSet，也可以是存储在磁盘上的 DiskRowSet。<strong>一个 RowSet 包含两部分数据：基础数据通常以 DiskRowSet 形式保存在磁盘上；而变更数据先以 MemRowSet 的形式暂存在内存中，后续再异步地刷写到磁盘上。</strong>和 C-Store 类似，内存中的数据使用 B 树存储。</p><p>与 C-Store 不同的是，Delta 数据并不会立即和磁盘上的基线数据进行合并，而是由后台的 compaction 线程异步完成。值得注意的是，为了保证 compaction 操作不影响过去的 snapshot read，被覆盖的旧数据也会以 UNDO 记录的形式保存在另外的文件中。</p><h2 id="powerdrill-2012">PowerDrill (2012)</h2><p>PowerDrill 是 Google 研发用于快速处理 ad-hoc 查询的 OLAP 数据库，为前端的 Web 交互式分析软件提供支持。PowerDrill 的数据放在内存中，为了尽可能节约空间，PowerDrill 引入一种全新的分区的存储格式，在节省内存占用的同时提供了类似索引的功能，能过滤掉无关的分区、避免全表扫描。</p><p>同是 Google 家的产品，和 Dremel 相比，PowerDrill 有以下几点差异：</p><ul><li>定位不同：Dremel 用于查询“大量的大数据集”（数据集的规模都大，数据集很多），PowerDrill 用于查询“少量的大数据集”（数据集的规模大，但数据集不多）</li><li>Dremel 用全表扫描（full scan）处理查询，而 PowerDrill 对数据做了分区，并能根据查询只扫描用到的分区。</li><li>Dremel 使用类似 Protobuf 的嵌套数据模型；PowerDrill 使用关系模型</li><li>Dremel 的数据直接放在分布式文件系统上，而 PowerDrill 需要一个 load 过程将数据载入内存</li></ul><h3 id="数据分区">数据分区</h3><p>Ad-hoc 查询常常包含 <code>GROUP BY</code> 子句，在这些 group key 上进行分区，能很好的过滤掉不需要的数据。<strong>PowerDrill 需要 DBA 根据自己对数据的理解，选出用于用于分区的一组属性</strong> <code>Key1 Key2 Key3 ...</code>（优先级依次递减）。分区是一个递归的过程：一开始把整个数据集视为一个分区（Chunk），如果 <code>Key1</code> 能将数据分开就用 <code>Key1</code>，否则用 <code>Key2</code>、<code>Key3</code>—……直到分区大小小于一个阈值。</p><p>以下是一个分区的例子，第一次使用 Age 分区、第二次使用 Salary 分区。</p><p><img src="/images/2018/05/powerdrill-partition-example.png"></p><h3 id="数据结构">数据结构</h3><p>PowerDrill 的数据组织以列为单位。对于每个列有一个全局字典表，列的每个分区有一个分区字典表：</p><ul><li><strong>全局字典表</strong>（global dictionary）存储列中所有 distinct 的字符串，按字典顺序排序。字典结构是双向的，既能将 string 映射到 global-id，也能从 global-id 查 string。</li><li><strong>分区字典表</strong>（chunk dictionary）存储一个分区中 chunk-id 到 global-id 的双向映射。相应地，数据列（elements）存储 chunk-id 而不是 global-id。</li></ul><p><img src="/images/2018/05/powerdrill-global-chunk-dict-example.png"></p><p>如果要将 chunk 中的一个 element 也就是 chunk-id 还原成数据，<strong>第一步需要查分区字典表，得到 global-id；第二步查全局字典表，得到原本的字符串数据</strong>。以上图举例而言：</p><ol type="1"><li>Chunk 0 存储的 chunk-id 数据 <code>[3, 2, 0, ...]</code></li><li>根据分区字典表，查出 global-id：<code>[5, 4, 1, ...]</code></li><li>根据全局字典表，查出 search string: <code>['ebay', 'cheap flights', 'amazon', ...]</code></li></ol><p>这样的两层映射保证 chunk-id 尽可能的小，所以可以用更紧凑的编码，比如用 8bit、16bit 整数存储。这不仅能节省空间，也能加快扫描速度。</p><p>此外，相同的数据只会在全局字典表中存一份。而且全局字典表中的字符串数据已经被排序，相比不排序，排序后用 Snappy 等算法的压缩比更高。</p><h3 id="分区索引">分区索引</h3><p>上述的数据结构还有一个额外的好处：<strong>它能快速算出某个分区是否包含有用的数据，帮助执行器跳过无关的分区</strong>。以下面的 SQL 为例（数据参考上一张图）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> search_string, <span class="keyword">COUNT</span>(*) <span class="keyword">as</span> c <span class="keyword">FROM</span> <span class="keyword">data</span></span><br><span class="line"><span class="keyword">WHERE</span> search_string <span class="keyword">IN</span> (<span class="string">"la redoute"</span>, <span class="string">"voyages sncf"</span>)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> search_string</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> c <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>步骤如下：</p><ol type="1"><li>在 <code>search_string</code> 列的全局字典表中查找 <code>"[la redoute", "voyages sncf"]</code>，得到 global-id <code>[9, 11]</code></li><li>在各个分区中查找 global-id <code>[9, 11]</code>： Chunk 0，Chunk 1 中都没有找到，所以可以直接跳过；而 Chunk 2 中出现了 <code>[11]</code>，对应 chunk-id 为 <code>[4]</code></li><li>在 Chunk 2 中的 elements 扫描查出 chunk-id = 4 的元素数量一共有 3 次，作为 <code>COUNT(*)</code> 的结果返回。</li></ol><h2 id="总结">总结</h2><p>本文介绍了几个知名的列式存储系统。与<a href="https://ericfu.me/columnar-storage-overview-storage/">上一篇文章</a>不同，本文的系统大多重新设计了存储层。与此同时，系统的复杂性也大大提升。</p><p>在构建自己的数据系统时，除了存储方式本身，以下几个地方是着重需要考虑清楚的地方，上述的几个系统也给我们提供了很好的参考：</p><ul><li><strong>系统需要处理的查询是怎样的模式？</strong>C-Store 主要服务于企业级 HTAP 场景，Kudu 在提供 OLAP 查询能力的同时保持了一定的实时写入能力，PowerDrill 着重处理 ad-hoc 的分析型查询。</li><li><strong>系统如何保证数据的持久性和高可用性？</strong>C-Store 在 projection 上保留了一定的冗余，Kudu 用 Raft 协议保持各个副本的数据一致性及可用性，PowerDrill 则直接把数据放在分布式文件系统上，因为不需要对数据作修改。</li><li><strong>系统提供怎样的数据一致性保证？</strong>对于只读的系统来说，这不是个问题。但是一旦支持写入，数据的一致性、事务隔离性都需要精心的考虑和权衡。Kudu 和 C-Store 的 Snapshot Read 实现可作为参考。</li></ul><h2 id="references">References</h2><ol type="1"><li><a href="http://people.csail.mit.edu/tdanford/6830papers/stonebraker-cstore.pdf" target="_blank" rel="noopener">C-store: a column-oriented DBMS - M Stonebraker, DJ Abadi, A Batkin, X Chen…</a></li><li><a href="http://db.csail.mit.edu/pubs/abadi-column-stores.pdf" target="_blank" rel="noopener">The Design and Implementation of Modern Column-Oriented Database Systems - D Abadi, P Boncz, S Harizopoulos…</a></li><li><a href="https://kudu.apache.org/kudu.pdf" target="_blank" rel="noopener">Kudu: Storage for Fast Analytics on Fast Data - T Lipcon, D Alves…</a></li><li><a href="http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf" target="_blank" rel="noopener">Processing a Trillion Cells per Mouse Click - A Hall, O Bachmann…</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/05/database-pic-banner.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在上一篇文章&lt;a href=&quot;https://ericfu.me/columnar-storage-overview-storage/&quot;&gt;《处理海量数据：列式存储综述（存储篇）》&lt;/a&gt; 中，我们介绍了几种 Apache ORC、Dremel 等几种典型列式存储的数据组织格式。实践中，很多数据系统构建在 HDFS 等分布式文件系统之上，使用这些规范的格式保存数据，方便各方的业务进行分析查询。&lt;/p&gt;
&lt;p&gt;本文将介绍若干个典型的列式存储数据库系统。作为完整的 OLAP 或 HTAP 数据库系统，他们大多使用了自主设计的存储方式，运行在多台机器节点上，使用网络进行通讯协作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>处理海量数据：列式存储综述（存储篇）</title>
    <link href="https://ericfu.me/columnar-storage-overview-storage/"/>
    <id>https://ericfu.me/columnar-storage-overview-storage/</id>
    <published>2018-04-12T05:17:02.000Z</published>
    <updated>2019-06-04T02:19:09.414Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/04/banner-warehouse.jpg"></p><p><strong>列式存储（Column-oriented Storage）</strong>并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。</p><p>总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。</p><a id="more"></a><h2 id="什么是列式存储">什么是列式存储</h2><p>传统 OLTP 数据库通常采用行式存储。以下图为例，所有的列依次排列构成一行，以行为单位存储，再配合以 B+ 树或 SS-Table 作为索引，就能快速通过主键找到相应的行数据。</p><figure><img src="/images/2018/04/row-oriented-example.png" alt="Figure 1.1"><figcaption>Figure 1.1</figcaption></figure><p>行式存储对于 OLTP 场景是很自然的：大多数操作都以实体（entity）为单位，即大多为<strong>增删改查一整行记录</strong>，显然把一行数据存在物理上相邻的位置是个很好的选择。</p><p>然而，对于 OLAP 场景，一个典型的查询需要遍历整个表，进行分组、排序、聚合等操作，这样一来按行存储的优势就不复存在了。更糟糕的是，分析型 SQL 常常不会用到所有的列，而仅仅对其中某些感兴趣的列做运算，那一行中那些无关的列也不得不参与扫描。</p><p>列式存储就是为这样的需求设计的。如下图所示，同一列的数据被一个接一个紧挨着存放在一起，表的每列构成一个长数组。</p><figure><img src="/images/2018/04/column-oriented-example.png" alt="Figure 1.2"><figcaption>Figure 1.2</figcaption></figure><p>显然，列式存储对于 OLTP 不友好，一行数据的写入需要同时修改多个列。但对 OLAP 场景有着很大的优势：</p><ul><li>当查询语句只涉及部分列时，只需要扫描相关的列</li><li>每一列的数据都是相同类型的，彼此间相关性更大，对列数据压缩的效率较高</li></ul><blockquote><p><strong>BigTable（HBase）是列式存储吗？</strong></p><p>很多文章将 BigTable 归为列式存储。但严格地说，BigTable 并非列式存储，虽然论文中提到借鉴了 C-Store 等列式存储的某些设计，但 BigTable 本身按 Key-Value Pair 存储数据，和列式存储并无关系。</p><figure><img src="/images/2018/04/bigtable-column-families-example.png" alt="Figure 1.3"><figcaption>Figure 1.3</figcaption></figure><p>有一点迷惑的是 BigTable 的列簇（column family）概念，列簇可以被指定给某个 locality group，决定了该列簇数据的物理位置，从而可以让同一主键的各个列簇分别存放在最优的物理节点上。由于 column family 内的数据通常具有相似性，对它做压缩要比对整个表压缩效果更好。</p><p>另外，值得强调的一点是：列式数据库可以是关系型、也可以是 NoSQL，这和是否是列式并无关系。本文中讨论的 C-Store 就采用了关系模型。</p></blockquote><h2 id="起源dsm-分页模式">起源：DSM 分页模式</h2><p>我们知道，由于机械磁盘受限于磁头寻址过程，读写通常都以一块（block）为单位，<strong>故在操作系统中被抽象为块设备</strong>，与流设备相对。这能帮助上层应用更好地管理储存空间、增加读写效率等。这一特性直接影响了数据库储存格式的设计：数据库的 Page 对应一个或几个物理扇区，让数据库的 Page 和扇区对齐，提升读写效率。</p><p>那如何将数据存放到页上呢？</p><p>大多数服务于在线查询的 DBMS 采用 NSM (N-ary Storage Model) 即按行存储的方式，将完整的行（即关系 relation）从 Header 开始依次存放。页的最后有一个索引，存放了页内各行的起始偏移量。由于每行长度不一定是固定的，索引可以帮助我们快速找到需要的行，而无需逐个扫描。</p><p>NSM 的缺点在于，如果每次查询只涉及很小的一部分列，那多余的列依然要占用掉宝贵的内存以及 CPU Cache，从而导致更多的 IO；为了避免这一问题，很多分析型数据库采用 DSM (Decomposition Storage Model) 即按列分页：将 relation 按列拆分成多个 sub-relation。类似的，页的尾部存放了一个索引。</p><p><img src="/images/2018/04/nsm-dsm-pax-comparation.png"></p><p>顺便一提，2001 年 Ailamaki 等人提出 PAX (Partition Attributes Cross) 格式，尝试将 DSM 的一些优点引入 NSM，将两者的优点相结合。具体来说，NSM 能更快速的取出一行记录，这是因为一行的数据相邻保存在同一页；DSM 能更好的利用 CPU Cache 以及使用更紧凑的压缩。PAX 的做法是将一个页划分成多个 minipage，minipage 内按列存储，而一页中的各个 minipage 能组合成完整的若干 relation。</p><p>如今，随着分布式文件系统的普及和磁盘性能的提高，<strong>很多先进的 DBMS 已经抛弃了按页存储的模式</strong>，但是其中的某些思想，例如<strong>数据分区、分区内索引、行列混合</strong>等，仍然处处可见于这些现代的系统中。</p><blockquote><p>分布式储存系统虽然不再有页的概念，但是仍然会将文件切割成分块进行储存，但分块的粒度要远远大于一般扇区的大小（如 HDFS 的 Block Size 一般是 128MB）。更大的读写粒度是为了适应网络 IO 更低的带宽以获得更大的吞吐量，但另一方面也牺牲了细粒度随机读写。</p></blockquote><h2 id="列数据的编码与压缩">列数据的编码与压缩</h2><p>无论对于磁盘还是内存数据库，IO 相对于 CPU 通常都是系统的性能瓶颈，<strong>合理的压缩手段不仅能节省空间，也能减少 IO 提高读取性能</strong>。列式存储在数据编码和压缩上具有天然的优势。</p><p>以下介绍的是 C-Store 中的数据编码方式，具有一定的代表性。根据 1) 数据本身是否按顺序排列（self-order） 2) 数据有多少不同的取值（distinct values），分成以下 4 种情况讨论：</p><ul><li><p><strong>有序且 distict 值不多</strong>。使用一系列的三元组 <span class="math inline">\((v, f, n)\)</span> 对列数据编码，表示数值 <span class="math inline">\(v\)</span> 从第 <span class="math inline">\(f\)</span> 行出现，一共有 <span class="math inline">\(n\)</span> 个（即 <span class="math inline">\(f\)</span> 到 <span class="math inline">\(f+n-1\)</span> 行）。例如：数值 4 出现在 12-18 行，则编码为 <span class="math inline">\((4, 12, 7)\)</span>。</p></li><li><p><strong>无序且 distict 值不多</strong>。对于每个取值 <span class="math inline">\(v\)</span> 构造一个二进制串 <span class="math inline">\(b\)</span>，表示 <span class="math inline">\(v\)</span> 所在位置的 bitmap。例如：如果一列的数据是 <span class="math inline">\(0,0,1,1,2,1,0,2,1\)</span>，则编码为 <code>(0, 110000100)</code>、<code>(1, 001101001)</code> 和 <code>(2,000010010)</code>。由于 bitmap 是稀疏的，可以对其再进行行程编码。</p></li><li><p><strong>有序且 distict 值多</strong>。对于这种情况，把每个数值表示为前一个数值加上一个变化量（delta），当然第一个数值除外。例如，对于一列数据 <span class="math inline">\(1,4,7,7,8,12\)</span>，可以表示为序列 <span class="math inline">\(1,3,3,0,1,4\)</span>。显然编码后的数据更容易被 densepack，且压缩比更高。</p></li><li><p><strong>无序且 distict 值多</strong>。对于这种情况没有很好的编码方式。</p></li></ul><p>编码之后，还可以对数据进行压缩。由于一列的数据本身具有相似性，即使不做特殊编码，也能取得相对较好的压缩效果。通常采用 Snappy 等支持流式处理、吞吐量高的压缩算法。</p><p>最后，编码和压缩不仅是节约空间的手段，更多时候也是组织数据的手段。在 PowerDrill、Dremel 等系统中，我们会看到<strong>很多编码本身也兼具了索引的功能</strong>，例如在扫描中跳过不需要的分区，甚至完全改表查询执行的方式。</p><h2 id="列式存储与分布式文件系统">列式存储与分布式文件系统</h2><p>在现代的大数据架构中，GFS、HDFS 等分布式文件系统已经成为存放大规模数据集的主流方式。分布式文件系统相比单机上的磁盘，具备多副本高可用、容量大、成本低等诸多优势，但也带来了一些单机架构所没有的问题：</p><ol type="1"><li>读写均要经过网络，吞吐量可以追平甚至超过硬盘，但是<strong>延迟要比硬盘大得多</strong>，且受网络环境影响很大。</li><li>可以进行大吞吐量的顺序读写，但随机访问性能很差，大多<strong>不支持随机写入</strong>。为了抵消网络的 overhead，通常写入都以几十 MB 为单位。</li></ol><p>上述缺点对于重度依赖随机读写的 OLTP 场景来说是致命的。所以我们看到，很多定位于 OLAP 的列式存储选择放弃 OLTP 能力，从而能构建在分布式文件系统之上。</p><p>要想将分布式文件系统的性能发挥到极致，无非有几种方法：<strong>按块（分片）读取数据、流式读取、追加写入等</strong>。我们在后面会看到一些开源界流行的列式存储模型，将这些优化方法体现在存储格式的设计中。</p><h2 id="列式存储系统案例">列式存储系统案例</h2><h3 id="c-store-2005-vertica">C-Store (2005) / Vertica</h3><p>大多数 DBMS 都是为写优化，而 C-Store 是第一个为读优化的 OLTP 数据库系统，虽然从今天的视角看它应当算作 HTAP 。在 ad-hoc 的分析型查询、ORM 的在线查询等场景中，大多数操作都是查询而非写入，在这些场景中列式存储能取得更好的性能。像主流的 DBMS 一样，C-Store 支持标准的关系型模型。</p><p>就像本文开头即提到——列式存储不是新鲜事。C-Store 的主要贡献有以下几点：<strong>通过精心设计的 projection 同时实现列数据的多副本和多种索引方式；用读写分层的方式兼顾了（少量）写入的性能</strong>。此外，C-Store 可能是第一个现代的列式存储数据库实现，其的设计启发了无数后来的商业或开源数据库，就比如 <a href="https://www.vertica.com/" target="_blank" rel="noopener">Vertica</a>。</p><h4 id="数据模型">数据模型</h4><p>C-Store 是关系型数据库，它的逻辑表和其他数据库中的并没有什么不同。但是在 C-Store 内部，逻辑表被纵向拆分成 projections，每个 projection 可以包含一个或多个列，甚至可以包含来自其他逻辑表的列（构成索引）。当然，每个列至少会存在于一个 projections 上。</p><p>下图的例子中，EMP 表被存储为 3 个 projections，DEPT 被存储为 1 个 projection。每个 projection 按照各自的 sort key 排序，在图中用下划线表示 sort key。</p><p><img src="/images/2018/04/c-store-projections-example.png"></p><p>Projection 内是以列式存储的：里面的每个列分别用一个数据结构存放。为了避免列太长引起问题，也支持每个 projection 以 sort key 的值做横向切分。</p><p>查询时 C-Store 会先选择一组能覆盖结果中所有列的 projections 集合作为 covering set，然后进行 join 计算重构出原来的行。为了能高效地进行 projections 的 join（即按照另一个 key 重新排序），引入 join index 作为辅助，其中存储了 proj1 到 proj2 的下标映射关系。</p><p><strong>Projection 是有冗余性的</strong>，常常 1 个列会出现在多个 projection 中，但是它们的顺序也就是 sort key 并不相同，因此 <strong>C-Store 在查询时可以选用最优的一组 projections</strong>，使得查询执行的代价最小。</p><p>巧妙的是，<strong>C-Store 的 projection 冗余性还用来实现 K-safe 高可用</strong>（容忍最多 K 台机器故障），当部分节点宕机时，只要 C-Store 还能找到某个 covering set 就能执行查询，虽然不一定是最优的 covering set 组合。</p><blockquote><p>从另一个角度看，C-Store 的 Projection 可以看作是一种物化（materialized）的查询结果，即查询结果在查询执行前已经被预先计算好；并且由于每个列至少出现在一个 Projection 当中，没有必要再保存原来的逻辑表。</p><p>为任意查询预先计算好结果显然不现实，但是如果物化某些经常用到的中间视图，就能在预计算代价和查询代价之间获得一个平衡。C-Store 物化的正是以某个 sort key 排好序（甚至 JOIN 了其他表）的一组列数据，同时预计算的还有 join index。</p></blockquote><p>C-Store 对写入的处理将在下一篇文章中呈现。</p><h3 id="apache-orc">Apache ORC</h3><p>Apache ORC 最初是为支持 Hive 上的 OLAP 查询开发的一种文件格式，如今在 Hadoop 生态系统中有广泛的应用。ORC 支持各种格式的字段，包括常见的 int、string 等，也包括 struct、list、map 等组合字段；字段的 meta 信息就放在 ORC 文件的尾部（这被称为自描述的）。</p><h4 id="数据结构及索引">数据结构及索引</h4><p><strong>为分区构造索引是一种常见的优化方案</strong>，ORC 的数据结构分成以下 3 个层级，在每个层级上都有索引信息来加速查询。</p><p><img src="/images/2018/04/orc-file-structure.png"></p><ul><li><strong>File Level</strong>：即一个 ORC 文件，Footer 中保存了数据的 meta 信息，还有文件数据的索引信息，例如各列数据的最大最小值（范围）、NULL 值分布、布隆过滤器等，这些信息可用来<strong>快速确定该文件是否包含要查询的数据</strong>。每个 ORC 文件中包含多个 Stripe。</li><li><strong>Stripe Level</strong> 对应原表的一个范围分区，里面包含该分区内各列的值。每个 Stripe 也有自己的一个索引放在 footer 里，和 file-level 索引类似。</li><li><strong>Row-Group Level</strong> ：一列中的每 10000 行数据构成一个 row-group，每个 row-group 拥有自己的 row-level 索引，信息同上。</li></ul><p>ORC 里的 Stripe 就像传统数据库的页，它是 ORC 文件批量读写的基本单位。这是由于分布式储存系统的读写延迟较大，一次 IO 操作只有批量读取一定量的数据才划算。这和按页读写磁盘的思路也有共通之处。</p><blockquote><p>像其他很多储存格式一样，ORC 选择将统计数据和 Metadata 放在 File 和 Stripe 的尾部而不是头部。</p><p>但 ORC 在 Stripe 的读写上还有一点优化，那就是把分区粒度小于 Stripe 的结构（如 Column 和 Row-Group）的索引统一抽取出来放到 Stripe 的头部。这是因为在批处理计算中一般是把整个 Stripe 读入批量处理的，将这些索引抽取出来可以减少在批处理场景下需要的 IO（批处理读取可以跳过这一部分）。</p></blockquote><h4 id="acid-支持">ACID 支持</h4><p>Apache ORC 提供有限的 ACID 事务支持。受限于分布式文件系统的特点，文件不能随机写，那如何把修改保存下来呢？</p><p>类似于 LSM-Tree 中的 MVCC 那样，writer 并不是直接修改数据，而是为每个事务生成一个 delta 文件，文件中的修改被叠加在原始数据之上。当 delta 文件越来越多时，通过 minor compaction 把连续多个 delta 文件合成一个；当 delta 变得很大时，再执行 major compaction 将 delta 和原始数据合并。</p><p><strong>这种保持基线数据不变、分层叠加 delta 数据的优化方式在列式存储系统中十分常见，是一种通用的解决思路</strong>。</p><blockquote><p>别忘了 ORC 的 delta 文件也是写入到分布式储存中的，因此每个 Delta 文件的内容不宜过短。这也解释了 ORC 文件虽然支持事务，但是主要是对批量写入的事务比较友好，不适合频繁且细小的写入事务的原因。</p></blockquote><h3 id="dremel-2010-apache-parquet">Dremel (2010) / Apache Parquet</h3><p>Dremel 是 Google 研发的用于大规模只读数据的查询系统，用于进行快速的 ad-hoc 查询，弥补 MapReduce 交互式查询能力的不足。为了避免对数据的二次拷贝，Dremel 的数据就放在原处，通常是 GFS 这样的分布式文件系统，为此需要设计一种通用的文件格式。</p><p>Dremel 的系统设计和大多 OLAP 的列式数据库并无太多创新点，但是其精巧的存储格式却变得流行起来，Apache Parquet 就是它的开源复刻版。注意 Parquet 和 ORC 一样都是一种存储格式，而非完整的系统。</p><h4 id="嵌套数据模型">嵌套数据模型</h4><p>Google 内部大量使用 Protobuf 作为跨平台、跨语言的数据序列化格式，相比 JSON 要更紧凑并具有更强的表达能力。Protobuf 不仅允许用户定义必须（required）和可选（optinal）字段，<strong>还允许用户定义 repeated 字段，意味着该字段可以出现 0～N 次，类似变长数组</strong>。</p><p>Dremel 格式的设计目的就是按列来存储 Protobuf 的数据。由于 repeated 字段的存在，这要比按列存储关系型的数据困难一些。一般的思路可能是用终止符表示每个 repeat 结束，<strong>但是考虑到数据可能很稀疏</strong>，Dremel 引入了一种更为紧凑的格式。</p><p>作为例子，下图左半边展示了数据的 schema 和 2 个 Document 的实例，右半边是序列化之后的各个列。序列化之后的列多出了 R、D 两列，分别代表 Repetition Level 和 Definition Level，<strong>通过这两个值就能确保唯一地反序列化出原本的数据</strong>。</p><p><img src="/images/2018/04/google-dremel-example.png"></p><p><strong>Repetition Level</strong> 表示当前值在哪一个级别上重复。对于非 repeated 字段只要填上 trivial 值 0 即可；否则，只要这个字段可能出现重复（无论本身是 repeated 还是外层结构是 repeated），应当为 R 填上当前值在哪一层上 repeat。</p><p>举个例子说明：对于 Name.Language.Code 我们一共有三条非 NULL 的记录。</p><ol type="1"><li>第一个是 <code>en-us</code>，出现在第一个 Name 的第一个 Lanuage 的第一个 Code 里面。在此之前，这三个元素是没有重复过的，都是第一次出现。所以其 R=0</li><li>第二个是 <code>en</code>，出现在下一个 Language 里面。也就是说 Language 是重复的元素。Name.Language.Code 中Language 排第二个，所以其 R=2</li><li>第三个是 <code>en-gb</code>，出现在下一个 Name 中，Name 是重复元素，排第一个，所以其 R=1</li></ol><p>注意到 <code>en-gb</code> 是属于第3个 Name 的而非第2个Name，为了表达这个事实，我们在 <code>en</code> 和 <code>en-gb</code>中间放了一个 R=1 的 NULL。</p><p><strong>Definition Level</strong> 是为了说明 NULL 被定义在哪一层，也就宣告那一层的 repeat 到此为止。对于非 NULL 字段只要填上 trivial 值，即数据本身所在的 level 即可。</p><p>同样举个例子，对于 Name.Language.Country 列</p><ol type="1"><li><code>us</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R1 内部，表示当前 Name 之内、后续所有 Language 都不含有 Country 字段。所以D为2。</li><li><code>NULL</code> 在 R1 内部，表示当前 Document 之内、后续所有 Name 都不含有 Country 字段。所以D为1。</li><li><code>gb</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R2 内部，表示后续所有 Document 都不含有 Country 字段。所以D为0。</li></ol><p>可以证明，结合 R、D 两个数值一定能唯一构建出原始数据。<strong>为了高效编解码，Dremel 在执行时首先构建出状态机，之后利用状态机处理列数据</strong>。不仅如此，状态机还会结合查询需求和数据的 structure 直接跳过无关的数据。</p><blockquote><p>状态机实现可以说是 Dremel 论文的最大贡献。但是受限于篇幅，有兴趣的同学请参考原论文。</p></blockquote><h2 id="总结">总结</h2><p>本文介绍了列式存储的存储结构设计。抛开种种繁复的细节，我们看到，以下这些思想或设计是具有共性的。</p><ol type="1"><li><strong>跳过无关的数据</strong>。从行存到列存，就是消除了无关列的扫描；ORC 中通过三层索引信息，能快速跳过无关的数据分片。</li><li><strong>编码既是压缩，也是索引</strong>。Dremel 中用精巧的嵌套编码避免了大量 NULL 的出现；C-Store 对 distinct 值的编码同时也是对 distinct 值的索引；PowerDrill 则将字典编码用到了极致（见下一篇文章）。</li><li><strong>假设数据不可变</strong>。无论 C-Store、Dremel 还是 ORC，它们的编码和压缩方式都完全不考虑数据更新。如果一定要有更新，暂时写到别处、读时合并即可。</li><li><strong>数据分片</strong>。处理大规模数据，既要纵向切分也要横向切分，不必多说。</li></ol><p>下一篇文章中，将会结合 C-Store、MonetDB、Apache Kudu、PowerDrill 等现代列式数据库系统，侧重描述列式 DBMS 的整体架构设计以及独特的查询执行过程。<strong>敬请期待！</strong></p><h2 id="references">References</h2><ol type="1"><li><a href="http://dbmsmusings.blogspot.jp/2010/03/distinguishing-two-major-types-of_29.html" target="_blank" rel="noopener">Distinguishing Two Major Types of Column-Stores - Daniel Abadi</a></li><li><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html" target="_blank" rel="noopener">Columnar Storage - Amazon Redshift</a></li><li><a href="http://www.vldb.org/conf/2001/P169.pdf" target="_blank" rel="noopener">Weaving Relations for Cache Performance - A Ailamaki, DJ DeWitt, MD Hill, M Skounakis</a></li><li><a href="http://glinden.blogspot.jp/2006/05/c-store-and-google-bigtable.html" target="_blank" rel="noopener">C-Store and Google BigTable - Greg Linden</a></li><li><a href="http://db.csail.mit.edu/pubs/abadi-column-stores.pdf" target="_blank" rel="noopener">The Design and Implementation of Modern Column-Oriented Database Systems - D Abadi, P Boncz, S Harizopoulos…</a></li><li><a href="http://people.csail.mit.edu/tdanford/6830papers/stonebraker-cstore.pdf" target="_blank" rel="noopener">C-store: a column-oriented DBMS - M Stonebraker, DJ Abadi, A Batkin, X Chen…</a></li><li><a href="https://orc.apache.org/docs/" target="_blank" rel="noopener">Apache ORC Docs</a></li><li><a href="https://research.google.com/pubs/archive/36632.pdf" target="_blank" rel="noopener">Dremel: Interactive Analysis of Web-Scale Datasets - S Melnik, A Gubarev, JJ Long, G Romer…</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/04/banner-warehouse.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列式存储（Column-oriented Storage）&lt;/strong&gt;并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。&lt;/p&gt;
&lt;p&gt;总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Let&#39;s Encrypt 免费的通配符域名证书</title>
    <link href="https://ericfu.me/lets-encrypt-wildcard-cert/"/>
    <id>https://ericfu.me/lets-encrypt-wildcard-cert/</id>
    <published>2018-03-15T06:15:14.000Z</published>
    <updated>2019-06-04T02:19:09.413Z</updated>
    
    <content type="html"><![CDATA[<p>免费 SSL 证书提供商 Let's Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日<a href="https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579" target="_blank" rel="noopener">正式发布</a>了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。</p><p>ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。</p><a id="more"></a><p>截至目前为止，官方推荐的 ACME 客户端 <a href="https://github.com/certbot/certbot" target="_blank" rel="noopener">Certbot</a> 还没有支持 ACME v2，这里推荐另一个小巧的客户端 <a href="https://github.com/Neilpang/acme.sh" target="_blank" rel="noopener">acme.sh</a>，它完全用 shell 脚本构成，快速安装方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://get.acme.sh | sh</span><br></pre></td></tr></table></figure><p>为了使用 DNS-01 challenge，先在 DNS 服务商那里启用 Token 访问。这里以 DNSPod 为例，其它服务商参见<a href="https://github.com/Neilpang/acme.sh/blob/master/dnsapi/README.md" target="_blank" rel="noopener">完整支持列表</a>。</p><p><img src="/images/2018/03/dnspod-api-token.png"></p><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> DP_Id=<span class="string">"49644"</span></span><br><span class="line"><span class="built_in">export</span> DP_Key=<span class="string">"******************************"</span></span><br><span class="line">acme.sh --issue --dns dns_dp -d example.com -d *.example.com</span><br></pre></td></tr></table></figure><p>如果一切顺利，可以看到证书已经生成了。如果要安装到 nginx 中，还需要转换成 PEM 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">acme.sh --install-cert -d example.com -d *.example.com \</span><br><span class="line">        --key-file       /etc/nginx/certificates/example.com/key.pem  \</span><br><span class="line">        --fullchain-file /etc/nginx/certificates/example.com/cert.pem</span><br></pre></td></tr></table></figure><p>最后把 nginx 配置从 HTTP 修改成 HTTPS：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name www.example.com example.com;</span><br><span class="line"></span><br><span class="line">    ssl_certificate /etc/nginx/certificates/example.com/cert.pem;</span><br><span class="line">    ssl_certificate_key /etc/nginx/certificates/example.com/key.pem;</span><br><span class="line"></span><br><span class="line">    # other configurations ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 nginx，大功告成。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;免费 SSL 证书提供商 Let&#39;s Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日&lt;a href=&quot;https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;正式发布&lt;/a&gt;了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。&lt;/p&gt;
&lt;p&gt;ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>谈谈如何实现具备 ACID 事务的分布式 KV 存储</title>
    <link href="https://ericfu.me/google-spanner-storage-impl/"/>
    <id>https://ericfu.me/google-spanner-storage-impl/</id>
    <published>2018-03-07T18:05:56.000Z</published>
    <updated>2019-06-04T02:19:09.415Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/03/banner-google-spanner.jpg"></p><p>F1/Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。</p><p>不过，<strong>论文对存储层实现只作了模糊的阐述</strong>：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：</p><ul><li><strong>read-only 事务</strong>: 读取最新或给定时间戳 <span class="math inline">\(t_{read}\)</span> 的快照，也就是 snapshot read</li><li><strong>read-write 事务</strong>：读取事务开始时间戳 <span class="math inline">\(t_{start}\)</span> 的快照，而写入操作在提交时间戳 <span class="math inline">\(t_{commit}\)</span> 生效</li></ul><p>本文从 F1/Spanner 论文出发，结合开源实现 TiDB 和 CockroachDB，谈谈如何设计一个具备 ACID 事务存储层。本文假设读者阅读过原论文 <a href="https://research.google.com/archive/spanner.html" target="_blank" rel="noopener">Spanner: Google's Globally-Distributed Database</a>。</p><a id="more"></a><h2 id="数据的-kv-表示">数据的 KV 表示</h2><p>F1/Spanner 对外提供（半）关系型数据模型：每张表定义了一个或多个主键列，以及其他的非主键列。这和我们熟知的 SQL 关系型模型几乎一摸一样，唯一的不同是 schema 定义中必须含有主键。</p><p>F1/Spanner 早期的设计中大量复用了 BigTable（开源实现即 HBase）的代码。回忆一下 BigTable 的数据模型：每一条数据包含 <code>(Key, Column, Timestamp)</code> 三个维度，满足我们需要的 MVCC 特性。从 BigTable 开始的确是个不错的选择。</p><p>不过，从性能上考虑 Bigtable 毕竟是分布式的 KV 存储系统，在存储这一层我们大可不用搞的那么复杂，分布式的问题例如 scale-out 和 replication 应当留给上层的 sharding 机制和 Paxos 解决。事实上，一个单机的存储引擎足矣。</p><p>Google 自家的 LSM-Tree + SSTable 的实现 LevelDB 是个可选项。它接口非常简单，是一个标准的 KV 存储，可以方便的在它基础上实现我们想要的数据模型。主要接口其实就是两个：</p><ul><li><code>Write(WriteBatch *)</code> 原子地写入一个 WriteBatch，包含一组 <code>Put(K, V)</code> 和 <code>Delete(K)</code> 操作</li><li><code>Iterator()</code> 及 <code>Seek()</code> 从指定位置开始顺序扫描读取 (K, V) 数据</li></ul><p>如何实现列和时间戳呢？举个例子，有如下数据表 <code>Accounts</code>。在数据库中，主键索引通常也是唯一的聚簇索引，它存放了真实的数据，而我们暂时不考虑其他索引。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| UserID (PK) | Balance | LastModified |</span><br><span class="line">|-------------|---------|--------------|</span><br><span class="line">| Alice       | 20      | 2018-02-20   |</span><br><span class="line">| Bob         | 10      | 2018-02-01   |</span><br></pre></td></tr></table></figure><p>Spanner 内部使用 MVCC 机制，所以还有一个隐藏的时间戳维度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| UserID | Timestamp | Balance | LastModified |</span><br><span class="line">|--------|-----------|---------|--------------|</span><br><span class="line">| Alice  | 103       | 20      | 2018-02-20   |</span><br><span class="line">| Alice  | 101       | 15      | 2018-01-20   |</span><br><span class="line">| Bob    | 102       | 10      | 2018-02-01   |</span><br></pre></td></tr></table></figure><p>上述数据表用 KV 模型存储，可以表示为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">| Key                             | Value      |</span><br><span class="line">|---------------------------------|------------|</span><br><span class="line">| Accounts/Alice/Balance/103      | 20         |</span><br><span class="line">| Accounts/Alice/Balance/101      | 15         |</span><br><span class="line">| Accounts/Alice/LastModified/103 | 2018-02-20 |</span><br><span class="line">| Accounts/Alice/LastModified/101 | 2018-01-20 |</span><br><span class="line">| Accounts/Bob/Balance/102        | 10         |</span><br><span class="line">| Accounts/Bob/LastModified/102   | 2018-02-01 |</span><br></pre></td></tr></table></figure><p>上表中 <code>/</code> 表示一个分隔符，真实情况要更复杂。Key 这样编码：从左到右依次是表名（因为可以有不止一张表）、主键字段、列的标识符、时间戳（通常倒序排列，Tips. 取反即可）。Value 则对应原表中的数据。</p><p>显然，对于半关系型数据一定能由表名、主键字段、列名唯一地确定一个值，所以这个编码方式能满足我们的要求。</p><blockquote><p>如果一张表只有主键怎么办呢？这种情况下可以为每个主键填充一个 placeholder 的 value 即可。</p></blockquote><h2 id="事务的原子性">事务的原子性</h2><p>众所周知，事务具有四个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。其中一致性和持久性其实是数据库系统的特性，对于事务，我们更多讨论的是<strong>原子性</strong>和<strong>隔离性</strong>。</p><p>对于存储层而言，为上层提供原子性 commit 的接口是必须的功能。如何在 KV 存储的基础上实现原子性呢？以下思路是一种常见的方案：</p><ol type="1"><li>首先，准备一个<strong>开关</strong>，初始状态为 off，当我们把开关打开的那一刻，意味着 commit 生效可见；</li><li>将所有变更以一种可回滚的方式（e.g. 不能覆盖现有的值）写入存储中。开关同时决定了其它 reader 的视图，由于开关还是 off 状态，现在写入的变更不会被其它事务看到。</li><li>之后，写入开关状态为 on，标志着 commit 的成功，新数据生效，即所谓 commit point。这个写入操作本身的原子性由 LevelDB 保证。</li><li>最后，清除掉中间状态（比如第 2 步中的临时数据）并写入最终的数据。这一步可以异步的完成，因为在第 3 步中事实上 commit 已经成功了，无需等待。</li></ol><p><img src="/images/2018/03/commit-4-steps.svg"></p><p>保证原子性的关键在于 commit point。例如，在单机数据库中，commit point 是 commit 的 redo-log 写入磁盘的一瞬间；在 XA 两阶段提交中，commit point 是协调器将事务状态置为 Committed 的一瞬间。</p><p>在我们的存储中，commit point 也就是第 3 步的写入操作。如果提交过程意外终止在 commit point 之前，我们会在读取时发现第 2 步中的临时写入，然后轻松地清除它；如果意外终止在 commit point 之后，部分临时状态没有被清除，只需继续执行 4 即可。</p><p>上述只是一个解决问题的思路。具体的解决方案可以参考 Percolator 的事务实现。这同时也是 TiDB 的做法，CockroachDB 做法略有不同，但同样遵从这个模式。</p><h3 id="percolator-事务方案">Percolator 事务方案</h3><p>Percolator 是 Google 早期的分布式事务解决方案，用于进行大规模增量数据处理。Percolator 在 BigTable 基础上基于 2PL 思想实现了分布式事务。这个算法很简单，你可以把它看作是是封装了一系列 BigTable 的 API 访问（本身无状态），所以可以容易地移植到 KV 存储模型上。</p><p>Percolator 事务模型基于单调递增的时间戳，来源于集群中唯一的 timestamp oracle。每个事务拥有提交时间戳 <span class="math inline">\(t_{commit}\)</span> 和开始时间戳 <span class="math inline">\(t_{start}\)</span>。Percolator 事务模型和之前说到的 write-read 事务一致：事务中总是读取 <span class="math inline">\(t_{start}\)</span> 时的 snapshot，而写入则全部在 <span class="math inline">\(t_{commit}\)</span> 生效。这也意味着事务中所有写入都被 buffer 到最后进行，不支持类似于 read-write-read 这样的模式。</p><p><img src="/images/2018/03/percolator-transaction-model-read-write.jpg"></p><p><em>如图，事务 2 看到的是事务 1 提交前的状态，而事务 3 看到的是事务 1、2 提交后的状态。</em></p><p>Percolator 基于 BigTable 的事务实现如下：</p><p>除了数据本身（bal:data 列）以外，我们给数据再加上两列：lock 和 write。</p><ul><li>write 列存放了一个<em>指针</em>，指向写入的 data 的时间戳</li><li>lock 列用于 2PL，加锁时也保存了 primary lock 的位置。</li></ul><p>primary lock 不仅代表当前行的锁状态，还兼任上文中“开关”的作用。通常选取第一个写入的数据作为 primary lock。</p><p>以下表为例。表中 <code>6: data @ 5</code> 表示：<span class="math inline">\(ts=6\)</span> 时事务提交，确定了 <code>Bob</code> 对应的值是 <code>5: $10</code>（所以推测出该事务 <span class="math inline">\(t_{start}=5\)</span>）。其他事务读取时，为了避免读到 uncommitted 的数据，都会先从 write 列开始找，然后再读出其指向的 data。</p><p><img src="/images/2018/03/percolator-commit-1.jpg"></p><p>现在，用户要从 Bob 账户里转 $7 给 Joe，为此必须开启一个事务。<span class="math inline">\(ts = 7\)</span> 时，转账事务开始，向 Bob 和 Joe 的 data 写入新的余额。</p><p><span class="math inline">\(ts = 8\)</span> 时，用户 commit 事务。事务的第一阶段（Prewrite）亦即是 2PL 的加锁阶段，先为 Bob 和 Joe 都加上锁。如下图所示，lock 不为空即代表加上了锁，其内容指向 primary lock 的位置。简单起见，不妨设第一条被锁的数据为 primary row。</p><p><img src="/images/2018/03/percolator-commit-2.jpg"></p><p>下一步很关键：<strong>清除 primary row 的 lock 并向 write 列写入新 data 的位置</strong>。这也就是所谓 commit point，这个写入的成功或失败决定了事务提交成功与否：</p><ul><li>若写入成功，则代表整个事务成功。之后会遍历所有加锁的行，解除 lock 并向 write 列写入新的 data 位置。这样一来，其他事务就能读到当前事务写入的数据。</li><li>否则，整个事务失败。之后会遍历所有加锁的行，解除 lock 并清除之前写入的 data，恢复原状。</li></ul><p>回到例子中，当 commit point 完成后，表的状态如下：</p><p><img src="/images/2018/03/percolator-commit-3.jpg"></p><p>解除 Joe 的 lock 并向 write 列写入新 data 的位置，至此事务 commit 完成：</p><p><img src="/images/2018/03/percolator-commit-4.jpg"></p><p>Commit point 这一步本身的原子性由 BigTable 行事务保证。对于 commit point 前后的其他操作，如果系统当机重启，恢复线程可以通过检查 commit point 操作的结果，来确定该 roll forward 还是 roll back。具体而言：</p><ul><li>通过 lock 找到 primary lock，如果已经解除，说明 commit point 已经完成，需要 roll forward 事务。</li><li>否则，如果 primary lock 还在，说明 commit point 还没到，只能 roll back 事务。</li></ul><p>于是，通过 2PL，我们成功地在 BigTable 的行级事务基础上实现了表级事务。</p><p>上述过程很容易的能映射到 KV 存储模型上。按照前一节描述的方法，将 lock 和 write 列都视作普通的列即可。这里不再赘述。</p><h2 id="事务的隔离性">事务的隔离性</h2><p>上述的讨论只考虑了单个事务的原子性保证——如何<strong>确保</strong>能从从中间状态恢复到未提交或已提交的状态，而没有考虑多线程并发的情况。如果同时有多个 client 在运行多个事务，如何保证严格互相隔离？（Serializable级别）</p><p>Percolator 是一个典型的 Snapshot Isolation 实现。Percolator 包含一个被称为 Strict-SI 的改进：在事务 commit 中，如果发现有一个高于 <span class="math inline">\(t_{start}\)</span> 的版本出现，则放弃 commit。这能避免 lost update 问题。但是 write-skew 问题依然存在。</p><p>F1/Spanner 提供 Serializable 隔离性保证。相应的算法被称为 Serializable Snapshot Isolation (SSI)。</p><h3 id="冲突图理论">冲突图理论</h3><p>首先对以上问题建模。考虑两个事务对同一条数据先后发生两次读或写操作，于是有 4 种情况：</p><ul><li>Read-Read：这是OK的，它不会引起冲突；</li><li>Read-Write：后发生的操作覆盖了前一个读的数据，这是一种冲突；</li><li>Write-Read：读到另一个事务的写入，这是一种冲突。</li><li>Write-Write：即覆盖写，这是一种冲突。</li></ul><p>上述三种冲突的情况，并不是一定会导致问题。举个例子：事务<span class="math inline">\(T_2\)</span>仅仅是覆盖了事务<span class="math inline">\(T_1\)</span>写入的数据，那么<span class="math inline">\(T_1\)</span>和<span class="math inline">\(T_2\)</span>仍然是符合 serializable 的，只要逻辑上认为<span class="math inline">\(T_2\)</span>发生在<span class="math inline">\(T_1\)</span>之后。</p><p>哪些情况会违反 serializable 呢？简单来说，如果冲突A迫使我们规定 <span class="math inline">\(T_1\)</span> 先于 <span class="math inline">\(T_2\)</span>，冲突B迫使我们规定 <span class="math inline">\(T_2\)</span> 先于 <span class="math inline">\(T_1\)</span>，这个因果关系就没法成立了，<span class="math inline">\(T_1\)</span>、<span class="math inline">\(T_2\)</span>无法以任何方式串行化。形式化的说：<strong>以所有事务 <span class="math inline">\(T\)</span> 作为节点、以所有冲突 <span class="math inline">\(C\)</span> 作为有向边构成一张有向图（这被称为冲突图或依赖图），如果这张图是有向无环图（DAG）则满足 serializable；否则（有环）不满足</strong>。</p><p>举个例子：</p><p>这是一个有向无环图，<span class="math inline">\(T_1\)</span>、<span class="math inline">\(T_2\)</span>、<span class="math inline">\(T_3\)</span> 满足 serializable。</p><p><img src="/images/2018/03/serializable-1.jpg"></p><p>这是一个有环的图，<span class="math inline">\(T_1\)</span>、<span class="math inline">\(T_2\)</span>、<span class="math inline">\(T_3\)</span> 无法被串行化。</p><p><img src="/images/2018/03/serializable-2.jpg"></p><p>图论告诉我们，如果一张图是 DAG，<strong>等价于</strong>我们能为它进行拓扑排序，即给每个节点 assign 一个编号，使得所有边都是从编号小的节点指向编号大的。换而言之，<strong>如果我们能给每个节点 assign 一个这样的编号，则可以反推出原图是 DAG，进而证明 T 集合满足 serializable</strong>。</p><blockquote><p>你可能已经隐约感觉到，这个编号和事务发生的顺序有关！事实上，编号代表 serializable 后的逻辑顺序，大多数时候，这个顺序和真实的时间顺序都是一致的。</p><p>Spanner 中强调自己满足的是比 serializable 更强的一致性：linearizable，说的就是不仅能序列化，而且序列化的“逻辑顺序”和时间上的“物理顺序”也一致。</p></blockquote><h3 id="serializable-snapshot-isolation-ssi">Serializable Snapshot Isolation (SSI)</h3><p>不妨把事务开始的时间戳 <span class="math inline">\(t_{start}\)</span> 作为这个编号。将上述约束条件略微加强一些，就得到了简单有效的判断法则：<strong>对于冲突 <span class="math inline">\(T_1 \rightarrow T_2\)</span>，如果时间戳满足 <span class="math inline">\(t_1 &lt; t_2\)</span> 则允许发生；如果 <span class="math inline">\(t_1 &gt; t_2\)</span> 则终止事务。</strong></p><p>具体的来说，对于三种冲突，分别用以下方式处理：</p><ul><li><p><strong>Write-Read 冲突</strong>：感谢 MVCC，这是不会发生的，在 Percolator 的事务模型中，读操作一定是从一个过去时间点的 snapshot 上读取，而不会读到一个正在进行中事务的脏数据。（但是 MVCC 会引发另一个问题——staled read。见下文）</p></li><li><p><strong>Write-Write 冲突</strong>：如果 Write 发生的时候，出现了一条 <span class="math inline">\(t_{start}\)</span> 比较大的记录，则终止写事务。</p></li></ul><blockquote><p>Percolator 的 SI 实现使用了更强的约束：如果出现另一条比开始时间大的记录，无论其时间戳如何都会终止当前提交，这与 SSI 的机制有所区别。</p><p>由于 SI 无法完全避免 Read-Write 冲突（例如 write-skew 问题），所以在 Write-Write 冲突的处理上更为激进；但 SSI 已经解决了 Read-Write 冲突检测，不必用更强的约束。</p></blockquote><ul><li><strong>Read-Write 冲突</strong>：为了知道 Write 和另一个事务的 Read 冲突，必须要以某种方式记录下所有被读过的数据、以及读取事务的 <span class="math inline">\(t_{start}\)</span>。这通常用范围锁（range lock）来实现——将所有查询的 TableScan 范围记录在内存中，如果某一条写入的数据满足某个 where 条件，则有必要检查一下二者的时间戳先后顺序。如果不满足上述判断法则，需要终止写事务。</li></ul><p><img src="/images/2018/03/read-write-conflict-1.jpg"></p><ul><li>由于 MVCC 的存在，<strong>Read-Write 冲突</strong>还有另一种形式：<span class="math inline">\(T_2\)</span> 的 Read 发生地更迟，但是由于 MVCC 它读到的是 <span class="math inline">\(T_1\)</span> 写之前的值（staled read），而且这里 $T_1 $ 先于 $ T_2$ 从而构成 Read-Write 冲突。</li></ul><p><img src="/images/2018/03/read-write-conflict-2.jpg"></p><p>对此，一个简单的解决方案是：如果 <span class="math inline">\(T_2\)</span> 发现 <span class="math inline">\(T_1\)</span> 写入的中间数据（lock），则立即终止自己。经典 SSI 的做法是，在 <span class="math inline">\(T_2\)</span> commit 时如果发现 <span class="math inline">\(T_1\)</span> 已经 commit 则放弃本次提交。</p><p>综上，通过给每个事务赋予一个时间戳，并保证每个冲突都符合时间戳顺序，达到 serializable 隔离级别。</p><h2 id="总结">总结</h2><ol type="1"><li>将 <code>(Table, Key, Column, Timestamp)</code> 作为 Key 的编码，从而把（半）关系型数据存储在 KV 引擎中；</li><li>用两阶段锁（2PL）的方式在 KV 引擎上实现事务的原子性提交。</li><li>禁止冲突违反时间戳先后顺序，从而保证 serializable 的隔离性。</li></ol><h2 id="references">References</h2><ol start="0" type="1"><li><a href="https://research.google.com/archive/spanner.html" target="_blank" rel="noopener">Spanner: Google's Globally-Distributed Database (OSDI'12)</a></li><li><a href="https://research.google.com/pubs/pub36726.html" target="_blank" rel="noopener">Large-scale Incremental Processing Using Distributed Transactions and Notifications - USENIX 2010 - Daniel Peng, Frank Dabek</a></li><li><a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/" target="_blank" rel="noopener">How CockroachDB Does Distributed, Atomic Transactions - Cockroach Labs</a></li><li><a href="https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/" target="_blank" rel="noopener">Serializable, Lockless, Distributed: Isolation in CockroachDB - Cockroach Labs</a></li><li>Designing Data‑Intensive Applications - Martin Kleppmann</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/03/banner-google-spanner.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;F1/Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。&lt;/p&gt;
&lt;p&gt;不过，&lt;strong&gt;论文对存储层实现只作了模糊的阐述&lt;/strong&gt;：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;read-only 事务&lt;/strong&gt;: 读取最新或给定时间戳 &lt;span class=&quot;math inline&quot;&gt;\(t_{read}\)&lt;/span&gt; 的快照，也就是 snapshot read&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read-write 事务&lt;/strong&gt;：读取事务开始时间戳 &lt;span class=&quot;math inline&quot;&gt;\(t_{start}\)&lt;/span&gt; 的快照，而写入操作在提交时间戳 &lt;span class=&quot;math inline&quot;&gt;\(t_{commit}\)&lt;/span&gt; 生效&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文从 F1/Spanner 论文出发，结合开源实现 TiDB 和 CockroachDB，谈谈如何设计一个具备 ACID 事务存储层。本文假设读者阅读过原论文 &lt;a href=&quot;https://research.google.com/archive/spanner.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spanner: Google&#39;s Globally-Distributed Database&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>《指数基金投资指南》读书笔记</title>
    <link href="https://ericfu.me/index-fund-guide-notes/"/>
    <id>https://ericfu.me/index-fund-guide-notes/</id>
    <published>2018-02-04T16:08:14.000Z</published>
    <updated>2018-03-16T02:50:24.546Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/02/index-fund-guide-book.jpg"></p><p>最近程序员圈子悄悄流行起斯坦福的 <a href="https://cs007.blog/" target="_blank" rel="noopener">CS 007: Personal Finance For Engineers</a> 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。</p><p>在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。</p><a id="more"></a><h2 id="指数基金的分类">指数基金的分类</h2><p>从投资范围来看，可以分为宽基指数和行业指数。</p><h3 id="宽基指数">宽基指数</h3><h4 id="国内市场指数">国内市场指数</h4><table><thead><tr class="header"><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr class="odd"><td>上证 50</td><td>沪市规模最大的 50 支股票</td><td>大盘蓝筹</td></tr><tr class="even"><td>沪深 300</td><td>沪市和深市规模最大的 300 支股票</td><td>大盘蓝筹，最有代表性</td></tr><tr class="odd"><td>中证 500</td><td>除了沪深 300 之外最大的 500 支股票</td><td>中小型企业</td></tr><tr class="even"><td>创业板指数</td><td>创业板规模最大的 100 支股票</td><td>小型企业</td></tr><tr class="odd"><td>上证红利</td><td>沪市股息率最高的 50 支股票</td><td>大盘股为主</td></tr><tr class="even"><td>中证红利</td><td>沪市和深市股息率最高的 100 支股票</td><td>大盘股为主</td></tr><tr class="odd"><td>红利机会</td><td>过去 3 年盈利增长、1 年净利润为正等限制下股息率前 100 支</td><td>优化筛选</td></tr></tbody></table><h4 id="国外市场指数">国外市场指数</h4><table><thead><tr class="header"><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr class="odd"><td>恒生指数</td><td>香港股市规模最大的 50 支股票</td><td>投资港股</td></tr><tr class="even"><td>上证 50AH 优选</td><td>27 支 A 股 + 23 支 A/H 股中便宜的一方</td><td>优化上证 50</td></tr><tr class="odd"><td>纳斯达克 100</td><td>NASDAQ 规模最大的的 100 支股票</td><td>美股，互联网科技股</td></tr><tr class="even"><td>标普 500</td><td>美股中各行业领导者</td><td>美股大盘蓝筹</td></tr></tbody></table><h3 id="行业指数">行业指数</h3><table><thead><tr class="header"><th>行业</th><th>推荐理由</th></tr></thead><tbody><tr class="odd"><td>消费</td><td>需求稳定</td></tr><tr class="even"><td>医疗</td><td>需求稳定，我国老龄化利好</td></tr><tr class="odd"><td>可选消费</td><td>消费升级利好</td></tr><tr class="even"><td>养老</td><td>老龄化利好</td></tr><tr class="odd"><td>金融</td><td>牛市上涨多</td></tr></tbody></table><h2 id="如何挑选指数基金">如何挑选指数基金</h2><h3 id="格雷厄姆的理论">格雷厄姆的理论</h3><p>格雷厄姆对价值投资总结出三个重要的理论：</p><ul><li><strong>价格与价值的关系。</strong>股票价格围绕其内在价值上下波动，但长期来看是趋于一致的。</li><li><strong>能力圈。</strong>只投资自己了解的品种，只有具备能力才能判断出内在价值。</li><li><strong>安全边际。</strong>只有当股价大幅低于内在价值的时候，我们才会买入。</li></ul><h3 id="常用估值指标">常用估值指标</h3><h4 id="市盈率">市盈率</h4><p><span class="math display">\[市盈率 = 市值 / 盈利\]</span></p><p><span class="math display">\[PE = P / E\]</span></p><p>市盈率反应了我们愿意每 1 元的利润付出多少代价。</p><p>市盈率适用于流通性好、盈利稳定的品种。</p><h4 id="盈利收益率">盈利收益率</h4><p><span class="math display">\[盈利收益率 = 盈利 / 市值\]</span></p><p>盈利收益率是市盈率的倒数。适用条件同上。</p><h4 id="市净率">市净率</h4><p><span class="math display">\[市净率 = 市值 / 净资产\]</span></p><p><span class="math display">\[PB = P / B\]</span></p><p>净资产等于资产减负债，净资产相对盈利比较稳定。</p><p>当企业的资产大多是比较容易衡量价值的有形资产，并且是长期保值的资产时，比较适合用市净率估值。</p><h4 id="股息率">股息率</h4><p>股息率是过去一年的现金派息额除以公司总市值。</p><h3 id="盈利收益率法">盈利收益率法</h3><ol type="1"><li>当盈利收益率大于 10% 时，开始定投；</li><li>当盈利收益率低于 10% 大于 6.4% 时，停止定投并持有已有份额。</li><li>当盈利收益率低于 6.4% 时，分批卖出。</li></ol><p>适用条件比较苛刻，只适用于流通性比较好、盈利比较稳定的品种，不适用于增长速度快、或盈利波动比较大的品种。例如：上证 50 指数、上证红利指数、恒生指数、H 股指数等。</p><h3 id="博格公式法">博格公式法</h3><p>指数基金未来的年复合收益率由三个部分构成：</p><ul><li>初始股息率</li><li>未来每年的市盈率变化率</li><li>未来每年的盈利变化率</li></ul><p>分析一下：</p><ul><li>初始股息率在买入时就确定了</li><li>市盈率在某个范围内呈现周期性变化</li><li>盈利长期来看是会上涨的</li></ul><p>所以，我们的策略是：</p><ol type="1"><li>在市盈率处于历史较低位置时定投买入；</li><li>等市盈率回归正常估值，暂停定投，继续持有；</li><li>当市盈率进入历史较高区域时卖出。</li></ol><p>适用于盈利增长较快的品种、波动较大的品种，例如：沪深 300 指数、中证 500 指数、创业板指数、红利机会指数、消费行业指数、医药行业指数、养老行业指数等。</p><h3 id="博格公式变种">博格公式变种</h3><p>对于盈利波动大的行业，用市净率代替市盈率。</p><p>在市净率率处于历史较低位置时买入。买入后，等待市净率回归正常即可。</p><p>适用于周期性较强、盈利不稳定的行业，例如：证券行业指数、非银金融行业指数、地产行业指数等。</p><h2 id="定投收益">定投收益</h2><p>提高定投收益的技巧：</p><ol type="1"><li><p>降低交易基金的费用。</p></li><li><p>正确处理分红：相当于一笔现金收入，投入到相对低估的指数基金中。</p></li><li><p>频率选择：按周定投收益更稳定，但长期来看差距很小，按个人习惯选择即可。</p></li><li><p>定期不定额： <span class="math display">\[每个月定投金额 = \left( \frac{当月的盈利收益率}{首次的盈利收益率} \right) ^ n\]</span> <span class="math inline">\(n\)</span> 是放大倍数，建议 <span class="math inline">\(n=1\)</span>。</p></li><li><p>定期不定额（博格公式法） <span class="math display">\[每个月定投金额 = \left( \frac{首次的市盈率}{当月的市盈率} \right) ^ n\]</span></p></li></ol><h2 id="制定定投计划">制定定投计划</h2><ol type="1"><li>梳理现金流，确定每月用于定投的数额；</li><li>选择适合定投的指数基金；</li><li>选择每月／每周的某一天为定投日，按之前的方法制定策略；</li><li>做好定投记录。</li></ol><p>定投记录表示例：</p><table><thead><tr class="header"><th>日期</th><th>操作</th><th>交易品种</th><th>金额</th><th>成交单价</th><th>成交份额</th><th>当前估值</th></tr></thead><tbody><tr class="odd"><td>2017-05-05</td><td>买入</td><td>501029 红利指数</td><td>750</td><td>0.994</td><td>753</td><td>12.64</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/02/index-fund-guide-book.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近程序员圈子悄悄流行起斯坦福的 &lt;a href=&quot;https://cs007.blog/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CS 007: Personal Finance For Engineers&lt;/a&gt; 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。&lt;/p&gt;
&lt;p&gt;在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Learned Index Structures 论文解读</title>
    <link href="https://ericfu.me/learned-index-paper-notes/"/>
    <id>https://ericfu.me/learned-index-paper-notes/</id>
    <published>2017-12-23T09:12:44.000Z</published>
    <updated>2018-03-16T02:49:05.735Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/12/learned-index-cover.jpg"></p><p>数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。</p><p>这些事，似乎拿预测模型都是可以做的。Yes, but...B 树那是精确的映射关系，和预测模型能一样吗？</p><p>所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。</p><a id="more"></a><h2 id="范围索引">范围索引</h2><p>B 树和一个预测 model 是很相似的：</p><ul><li>B 树能定位某行数据所在的 page，可以看作是确定了一个区间：[pos, pos + pagesize]</li><li>预测模型也能做相似的事，假设我们把预测错误率成功 bound 在 min/max_err 以内，那么也就可以确定，数据位于区间 [pos - min_err, pos + max_err]</li></ul><p>一图以概之：</p><p><img src="/images/2017/12/figure-1.jpg"></p><p>于是问题变成，<strong>如何把预测错误率 bound 在 max_err 以内</strong>？</p><p>答案非常简单！通过训练。你可以把训练集的上的 error 降到多小都行，只要你模型的表现力足够强。这个问题和绝大多数机器学习的问题都不同，<strong>我们只要照顾好训练集（也就是被索引的数据）就可以了，没有测试集！</strong>当然也就不会有过拟合，模型的泛化能力是不用考虑的，比想象的简单吧！</p><p>说到模型表现力强，很容易想到神经网络。除此以外，NN 还带来了另一个好处，在 GPU（或其他专用芯片，比如 Google 的 TPU）上，NN 能获得惊人的计算速度。NN 的结构决定了它并行起来非常快，时间复杂度上把 <span class="math inline">\(O(\log{n})\)</span> 的 B 树等甩在身后。</p><p>说到这里，我们来做个思维拓展：如果把 key 作为横轴，key 在有序数组中的位置 pos 作为纵轴，画出目标函数的曲线，那应该长成这个样子：</p><p><img src="/images/2017/12/figure-2.jpg"></p><p>这是一个 CDF (累积分布函数，累积的是各个 key 对应数据的长度）。从这个角度看，无论是 B 树还是预测模型都是在拟合这个函数，只是 approach 完全不同。</p><p>这时候再回头看我们熟悉的 B 树，有没有一点顿悟—— Aha! 这不是就是决策树吗？</p><h2 id="递归模型索引-rm-index">递归模型索引 RM-Index</h2><p>以上已经说完了核心思想，接下来就是要找到一个合适的预测模型来代替 B 树。实验发现，直接上 DNN 效果并不好：单次计算代价太大，只能用 GPU（而调用 GPU 会产生不小的 overhead）；而且网络很庞大，retrain（增删改）代价很大。</p><p>Naive 的预测模型之所以做的不好，一个重要原因是：把如此大量的数据每条误差 bound 在 min/max_err 之内，实在太难了（所谓 <em>last mile</em> 问题）。为解决这个问题，决策树给我们做了个很好的提示，<strong>如果一个模型解决不了问题，就再加几层</strong>。</p><p>举个例子：为 100M 记录训练一个足够精确的预测器太难，那就分成 3 层树状结构。根节点分类器把记录分出 100 份，每份大约有 1M 记录；第二层再分出 100 份，每份大约只剩 10K 记录；第三层再分出 100 份，每份大约有 100 条记录——假设 100 条纪录足够把误差在 min/max_err 之内。</p><p><img src="/images/2017/12/figure-3.jpg"></p><p><em>注意，上图并不是一棵树，例如 Model 2.1 和 2.2 都可以选择 Model 3.2 作为下一层的分类模型。</em></p><p>这样做的好处是，每层要做的事情简单多了（每层 precision gain = 100），模型可以变得简单得多。每个 NN 模型就像一个精通自己领域的专家，他只要学习某个很小子集的 keys 就可以了。这也同时解决了 <em>last mile</em> 难题，大不了为这一百左右个 keys 过拟合一下也无妨。</p><h2 id="混合索引">混合索引</h2><p>上图中的三层网络结构还带来一个额外的优势：每个 Model 其实是独立的，我们可以用除了 NN 以外的预测方法代替之，包括线性回归等简单算法，甚至是 B 树。</p><p>事实上，最后选用了两种 Model：</p><ul><li>简单的神经网络（0～2 层全连接的 hidden layer，ReLU 激发函数，每层最多 32 个神经元）</li><li>当叶节点的 NN 模型 error rate 超过阈值时，替换成 B 树</li></ul><p>训练算法如下，</p><p><img src="/images/2017/12/algorithm-1.jpg"></p><p>简单说就是：</p><ol type="1"><li>固定整个 RM-Index 的结构，比如层数、每层 Model 数量等（可以用网格法调参）；</li><li>用全部数据训练根节点，然后用根节点分类后的数据训练第二层模型，再用第二层分类后的数据训练第三层；</li><li>对于第三层（叶节点），如果 max_error 大于预设的阈值，就换成 B 树。</li></ol><p>有了 Step 3，这个 RM-Index 的分类能力也就有了下界，即使面对纯噪声数据（毫无规律可循），至少能和 B 树保持差不多的性能。</p><blockquote><p>索引的 keys 常常是字符串，而我们前文说的 NN 模型的输入是向量。Luckily，字符串向量化是 ML 里研究很多的一个课题了，这里不再讨论，有兴趣的可以看下原论文（抛砖引玉为主）。</p></blockquote><h2 id="测试结果">测试结果</h2><p>为了对比 RM-Index 和 B 树的性能，论文作者找了 4 个数据集，分别用 RM-Index 和 B 树作二级索引。</p><ul><li>Weblogs 数据集：访问时间 timestamp -&gt; log entry （约 200M）</li><li>Maps 数据集：纬度 longitude -&gt; locations （约 200M）</li><li>Web-documents 数据集：documents（字符串）-&gt; document-id（约 10M）</li><li>Lognormal 数据集：按对数正态分布随机生成的数据</li></ul><p>测试中用了不同参数的 Learned Index 和 B 树，B 树也用了一个高度优化的实现。</p><p>总体来说，Size savings 都相当可观（下降 1～2 个数量级），而速度也有所优化，最多能快 1 倍。</p><p><img src="/images/2017/12/figure-5.jpg"></p><blockquote><p>对于每个数据集，论文都给出了详细的测试结果，有兴趣的同学请看原文。</p></blockquote><p>可以说是符合预期的，个人看法是：</p><ul><li>因为算法 Step 3 的帮助，即使不经过调优，性能至少不输 B 树；</li><li>肯定能省下许多空间，因为 B 树是基于比较的查找，叶结点要保存 key 的内容，key 越多索引越大；而 NN 完全不受这个制约。</li></ul><p>但暂且不要太激动，这只是查找性能。索引的维护（增／删／改）代价如何也是要考虑的。用作者原话说，这是 learned index 的阿喀琉斯之踵，因为 NN 模型的 retrain 代价是不可预测的，这是多数 ML 算法和传统算法一个很大的不同点。对此，作者意见如下：</p><ul><li>如果恰好新的数据已经能被成功预测了，那就不用 retrain 了；但这太理想化，我们为达到 <em>last mile</em> 做的那些 overfitting 也导致了这个模型泛化性堪忧。</li><li>如果一定要 retrain，一个简单有效的优化是：把变更数据累积起来先放着，批量训练；</li><li>此外，retrain 可以借鉴一些 warm-start 的方法加快训练过程。</li></ul><h2 id="其他索引结构">其他索引结构</h2><p>论文中这部分没有详细展开，因为原理和前文几乎没有区别，仅仅换一种用法。</p><h3 id="point-index">Point Index</h3><p>拳打完 B 树再来脚踢 hash-map。大家都知道 hash-map 兼具 <span class="math inline">\(O(1)\)</span> 的高效率和低效的空间使用率，想快就要减少碰撞，于是要牺牲更多的空间。所谓空间换时间。即使是 Google 的 Dense-hashmap 也会有 78% 的 overhead。</p><p>解决方案如图所示，用 RM-Index 模型替换掉 hash function。其思想是，利用数据的某些内在特征，帮助它找到一个最均匀（uniform）的映射方式，而不是用哈希彻底随机化。</p><p><img src="/images/2017/12/figure-9.jpg"></p><p>在三个数据集上的测试表明，这一方法的确提升了 slot 的空间利用率，减少了很多空 slot，减少的比例约 -6% ~ -78%。</p><h3 id="existence-index">Existence Index</h3><p>这回轮到的是 bloom filter，有两种思路：</p><ol type="1"><li>直接用一个二分类模型判断是否存在；</li><li>和上一小节的原理类似，把 hash 函数替换成 RMI 模型。</li></ol><p><img src="/images/2017/12/figure-12.jpg"></p><h2 id="后记">后记</h2><p>在 Jeff Dean 大神的光环之下，这篇文章很快引起热烈的讨论。</p><p>不得不说，这个脑洞开的很大，令人为之一振。一直在人们心中只能做“模糊”预测的 ML 算法竟然能代替经典的 B 树，放在 10 年前估计会被喷到体无完肤。</p><p>论文的亮点在于，大家一直在“训练——预测”这样一个思维下作 ML 而忽视了一点：至少对于已知的数据，ML 算法也是能输出一个确定的结果的！换句话说，<strong>在全集上训练，把错误率强行 bound 住其实很容易</strong>。</p><p>下面说说优缺点。</p><p>Learned index 对于规律性强的数据是大杀器，作这种数据的二级索引再合适不过了。<strong>内在规律越强，就意味着 B 树、哈希这些通用算法浪费的越多，这也是 ML 算法能捡到便宜的地方</strong>。</p><p>就像很多 DBMS 引入全文索引一样，未来的 DBMS 也也可以尝试给用户更多其他选项，为某些特别有规律的 column 建立非 B 树／Hash 的二级索引。甚至可以让 DMBS 智能化，自己尝试寻找一些规律，将 B 树索引透明的替换成其他索引。</p><p>缺点也是明显的：<strong>增删改代价难以控制</strong>，可想而知，对于规律性不那么明显的数据，这足以抹平它查找的速度优势。但我相信之后会有更多改进的 index 模型出现，把这个代价尽可能减少。</p><p>一句话总结个人看法：</p><p><strong>B 树作为通用索引的地位仍然难以撼动，但特定数据场景下，learned index 将成为一个有力的补充。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/12/learned-index-cover.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。&lt;/p&gt;
&lt;p&gt;这些事，似乎拿预测模型都是可以做的。Yes, but...B 树那是精确的映射关系，和预测模型能一样吗？&lt;/p&gt;
&lt;p&gt;所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>Sharding-JDBC 的事务处理小结</title>
    <link href="https://ericfu.me/sharding-jdbc-transaction-overview/"/>
    <id>https://ericfu.me/sharding-jdbc-transaction-overview/</id>
    <published>2017-12-13T02:17:13.000Z</published>
    <updated>2017-12-13T02:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。</p><p>本文主要介绍 Sharding-JDBC 的事务处理。</p><h2 id="overview">Overview</h2><blockquote><p>Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。</p></blockquote><ul><li>默认使用 “弱XA” 事务</li><li>可选使用柔性事务：<ul><li>BED（最大努力送达型）事务</li><li>TCC（补偿型）事务</li></ul></li></ul><a id="more"></a><h2 id="弱xa事务">“弱XA”事务</h2><p>之所以加引号，是因为这个和 MySQL 的 XA 其实没有关系。</p><p>它的实现是很自然的：</p><ul><li><strong>Prepare 阶段</strong>（执行 SQL）过程遇到异常，则中止当前事务，对所有分库上的事务连接执行 <code>conn.rollback()</code></li><li><strong>Commit 阶段</strong>（提交事务）时对所有分库依次（其实可以并行）做 <code>conn.commit()</code>；如果某个事务连接 commit 时抛出异常，由于无法回滚其他连接，所以仅仅是收集起来报给调用者，交由用户处理。不影响其他分库 commit。</li></ul><p>文档中对使用注意事项也写的很明确：</p><blockquote><p>不支持因网络、硬件异常导致的跨库事务。例如：同一事务中，跨两个库更新，更新完毕后、未提交之前，第一个库死机，则只有第二个库数据提交。</p></blockquote><h2 id="柔性事务">柔性事务</h2><p>分布式场景下传统数据库 ACID 无法满足业务的性能要求，所以诞生了 BASE 理论。BASE是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写，其中<em>软状态</em>是指允许系统中的数据存在中间状态。</p><p>Sharding-JDBC 的柔性事务是需要调用者显式开启的。以 BED 事务为例，客户需要先通过柔性事务管理器创建出 <em>BEDSoftTransaction</em> 对象，然后使用 <code>begin()</code> 开始柔性事务。</p><h2 id="bed最大努力送达型事务">BED（最大努力送达型）事务</h2><p>用户保证该数据库的操作最终一定可以成功，所以通过最大努力<strong>反复尝试</strong>。</p><p><img src="media/15130580762465/15130630328613.png"></p><p>结合上图，执行过程可以分为 4 种情况：</p><ol type="1"><li>同步执行成功</li><li>同步执行失败，同步重试成功</li><li>同步执行失败，同步重试失败，异步重试成功</li><li>同步执行失败，同步重试失败，异步重试失败，事务日志保留（人工介入处理）</li></ol><p>BED 不保证 exactly once，所以使用 BED 的 SQL 需要满足幂等性，例如不能用 <code>UPDATE SET x = x + 1</code> 这样的 SQL。</p><h2 id="tcc补偿型事务">TCC（补偿型）事务</h2><p>目前还在规划中，没有实现。</p><h2 id="参考资料">参考资料</h2><ul><li><a href="http://shardingjdbc.io/docs/02-guide/transaction/" target="_blank" rel="noopener">Sharding-JDBC 文档 - 事务支持</a></li><li><a href="http://blog.csdn.net/yanyan19880509/article/details/78335935" target="_blank" rel="noopener">sharding-jdbc 事务解读</a></li><li><a href="http://www.iocoder.cn/Sharding-JDBC/transaction-bed/" target="_blank" rel="noopener">Sharding-JDBC 源码分析 —— 分布式事务（一）之最大努力型</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。&lt;/p&gt;
&lt;p&gt;本文主要介绍 Sharding-JDBC 的事务处理。&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;默认使用 “弱XA” 事务&lt;/li&gt;
&lt;li&gt;可选使用柔性事务：
&lt;ul&gt;
&lt;li&gt;BED（最大努力送达型）事务&lt;/li&gt;
&lt;li&gt;TCC（补偿型）事务&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>博客从 Ghost 迁移到 Hexo</title>
    <link href="https://ericfu.me/move-blog-from-ghost-to-hexo/"/>
    <id>https://ericfu.me/move-blog-from-ghost-to-hexo/</id>
    <published>2017-10-24T11:10:17.000Z</published>
    <updated>2017-10-29T11:38:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>不知道是不是我的错觉，<a href="https://ghost.org/" target="_blank" rel="noopener">Ghost</a> 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。</p><p>当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 <a href="http://zh.mweb.im/" target="_blank" rel="noopener">MWeb 编辑器</a> 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）</p><p>下面说正经的。</p><a id="more"></a><h2 id="pros-hexo-有哪里好">Pros: Hexo 有哪里好？</h2><ul><li>Markdown 格式，方便本地保存以及迁移</li><li>社区活跃，使用问题基本都能 google 到答案</li><li>Hexo + NexT 主题基本不用折腾就能满足我的所有要求，比如自带 MathJax</li><li>最近墙变高了，很容易迁移到国内的静态页面托管平台（比如 coding.net）</li></ul><h2 id="cons-迁移代价">Cons: 迁移代价</h2><ul><li>因为之前的用了 <a href="https://posativ.org/isso/" target="_blank" rel="noopener">isso 开源评论系统</a>懒得迁过来了，丢失一堆评论</li><li>你可能因此浪费一个周末。</li></ul><h2 id="从-ghost-导出内容">从 Ghost 导出内容</h2><p>Ghost 一直很良心地在实验室页面保留了一个 <code>export</code> 按钮，导出后是一个巨大的 JSON，包含所有文章以及一些元数据：修改日期、Tags 等等。</p><p>但是 Ghost 1.X 开始启动了一个<a href="https://github.com/bustle/mobiledoc-kit/blob/master/MOBILEDOC.md" target="_blank" rel="noopener">叫 <code>mobiledoc</code> 的文档格式</a>。突然得知这一消息的我是一脸懵逼的，说好的 Markdown 呢？根据<a href="https://github.com/TryGhost/Ghost/issues/7429" target="_blank" rel="noopener">这个 Issue</a>所述：</p><blockquote><p>你知道的，我们现在的编辑器非常烂，不好用，还有一堆 Bug!</p><p>所以我们决定 Mobiledoc 重新做一个！Mobiledoc 很棒，mobiledoc 就是未来！</p></blockquote><p>Emmmm…… 好吧，你开心就好！</p><p>后果就是无论是 Ghost 导出的 JSON 还是数据库，都只有 mobiledoc 文档而没有 Markdown。所幸的是，Ghost 产生的 mobiledoc 也很奇葩，是把 Markdown 强行塞进去了。</p><p>后面会在导入时处理这个问题，先点实验室里的 <code>export</code> 拿到 JSON。</p><p>对于图片等资源，我们到 <code>assets</code> 文件夹下，打包下载下来即可。</p><h2 id="将内容导入-hexo">将内容导入 Hexo</h2><p>根据 <a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">Hexo 官方的安装指引</a>在本地装好 Hexo。</p><p>时间充裕的话，可以顺手把 <a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">NexT 主题</a>也给装了。</p><h3 id="导入文章-markdown">导入文章 Markdown</h3><p>之后我们开始导入文章。Hexo 是留了 migrator 插件接口的，GitHub 上能找到一个名为 hexo-migrator-ghost 的插件，但是已经年久失修。于是我帮他修了一下 bug，其中也包括从 mobiledoc 中解出 Markdown 代码。</p><p>你可以直接把<a href="https://github.com/fuyufjh/hexo-migrator-ghost" target="_blank" rel="noopener">我的 Repo</a> <code>git clone</code> 到 node_modules 里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/fuyufjh/hexo-migrator-ghost.git ./node_modules/hexo-migrator-ghost</span><br></pre></td></tr></table></figure><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo migrate ghost ghost-export.json</span><br></pre></td></tr></table></figure><h3 id="导入图片等资源">导入图片等资源</h3><p>原来的资源都是放在 contents 目录下的，格式大概是 <code>contents/2017/10/29/imagename.jpg</code>。</p><p>只要把 contents 改成 images 放到 _post 目录下，然后，用你喜爱的编辑器（或者 <code>sed</code>）对所有文章做一次全局替换：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/images/ -&gt; /contents/</span><br></pre></td></tr></table></figure><p><code>hexo serve</code> 试一下，应该可以看到图片了！</p><h3 id="最后做一些必要的配置">最后，做一些必要的配置</h3><ul><li><a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">Hexo 配置</a></li><li><a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT 配置</a></li><li><a href="http://theme-next.iissnan.com/theme-settings.html" target="_blank" rel="noopener">NexT 高级配置</a></li></ul><p>自己琢磨去吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道是不是我的错觉，&lt;a href=&quot;https://ghost.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ghost&lt;/a&gt; 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。&lt;/p&gt;
&lt;p&gt;当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 &lt;a href=&quot;http://zh.mweb.im/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MWeb 编辑器&lt;/a&gt; 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）&lt;/p&gt;
&lt;p&gt;下面说正经的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="misc" scheme="https://ericfu.me/tags/misc/"/>
    
  </entry>
  
  <entry>
    <title>Python 2 字符串编码踩坑小结</title>
    <link href="https://ericfu.me/python-2-str-and-unicode/"/>
    <id>https://ericfu.me/python-2-str-and-unicode/</id>
    <published>2017-08-31T21:55:08.000Z</published>
    <updated>2017-09-01T07:59:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Tips:</strong> 如果您已经充分理解问题是什么，请直接跳到 <a href="#问题出在哪里">#问题出在哪里</a> 一节。</p><h2 id="字符串和编码">字符串和编码</h2><p>先从概念说起，字符串和它的编码是两个不同的概念：</p><ul><li><strong>字符串</strong>是一段文字本身，可以是中文可以是英文，以及各种语言</li><li>字符串的<strong>编码</strong>是计算机存储、处理字符串的方式；作为一种<strong>数据</strong>，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。</li></ul><p>字符串经过<strong>编码（encode）</strong> 就成为了一堆数据，反过来，数据经过<strong>解码（decode）</strong> 就变回我们认识的字符串。</p><p><img src="/images/2017/09/encode_decode.png"></p><a id="more"></a><h2 id="从-python-3-说起">从 Python 3 说起</h2><p>这个编码问题（坑）可以说是 Python 2 被吐槽最多的黑点，没有之一。为了防止上来就掉进 Python 2 的坑里，我们先来看看 Python 3 里“改进”后是什么样子的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">len</span><span class="params">(s)</span></span></span><br><span class="line"><span class="class">9</span></span><br></pre></td></tr></table></figure><p>哈，没有任何问题！（数长度的时候别漏了空格）</p><p>查阅文档，我们发现 <code>str</code> 有个函数叫 <code>encode()</code>，它看起来很眼熟，让我们来试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">b'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>这个 <code>b''</code> 的前缀表示返回值是一个 <code>bytes</code> 变量，也就是一堆数据了。</p><blockquote><p><strong>为什么这里面"Hello"还是原来的样子，但是“世界”变成一坨 <code>\x??</code> 了？</strong></p><p>这是因为 ASCII 实在太有名了，程序员们都看得懂：这个 <code>H</code> 其实表示的是一字节 <code>0x48</code>。而后面“世界”的编码不在 ASCII 的编码范围内，所以只能用 <code>\x??</code> 表示了。</p><p>这样看起来也许更清晰一些：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt;&gt;&gt; b.hex()</span><br><span class="line">&gt; <span class="string">'48656c6c6f2c20e4b896e7958c'</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>有 <code>encode()</code> 当然也有 <code>decode()</code>。我们对刚刚拿到的 <code>bytes</code> <code>b</code> 解码，果然会变成原来的字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'Hello, 世界'</span></span><br></pre></td></tr></table></figure><p>OK，现在你已经明白了奥义所在，是时候去踩坑了。</p><h2 id="python-2-的世界">Python 2 的世界</h2><h3 id="初见茅庐">初见茅庐</h3><p>先来一道开胃菜：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;type <span class="string">'str'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</span><br><span class="line"><span class="number">13</span></span><br></pre></td></tr></table></figure><p>▲ 为什么这个长度是 13 ？明明是 9 个字符啊！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>▲ <code>s</code> 你怎么坏掉了？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>▲ 我可能用了假的 <code>encode()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br></pre></td></tr></table></figure><p>▲ 喵喵喵？</p><p><strong>以上，Python 2 中字符串并不像我们想的那样工作。</strong></p><h3 id="问题出在哪里">问题出在哪里</h3><p>其实说起来也简单，Python 是一门诞生于 1989 年的古老语言，比 Unicode 还要早两年，当时的程序员并不在乎编码问题，因为 ASCII 已经足够了。</p><p>如果你熟悉 C/C++ 会发现同样的问题：<code>char*</code> 被同时用于表示字符串和字节数组。Python 2 里也是同样，<code>str</code> 其实是个字节数组，却被挂上了字符串的名字。二十年后用着中文字符的我们被坑惨了。</p><p>后来 Python 2 为了支持 Unicode，增加了 <code>unicode</code> 类型，然而并没有卵用——程序员们不记得在每个字符串前面加上 <code>u</code>，这也不够优雅。</p><p>Python 3 设计之初就立志解决这个问题，不惜<strong>彻底修改了<code>str</code>的定义，把 <code>str</code> 这个名字让给了原来的 <code>unicode</code>！</strong>，而新增的 <code>bytes</code> 类型才是字节数组。如下表所示：</p><table><thead><tr class="header"><th></th><th>Python 2</th><th>Python 3</th></tr></thead><tbody><tr class="odd"><td>字符串（Unicode）</td><td>unicode</td><td>str</td></tr><tr class="even"><td>字节数组</td><td>str (bytes)</td><td>bytes</td></tr></tbody></table><p>所以，在 Python 2 里，如果遇到非英语字符，一定要记得用 unicode。效果是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">u'Hello, 世界'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;type <span class="string">'unicode'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br></pre></td></tr></table></figure><p>至于为什么 <code>str</code> 也有 <code>encode()</code>，主要是为了尽可能保持和 Python 3 的兼容性，以让部分程序能在 2、3 同时运行。<del>于是事情变得更糟糕了。</del></p><h3 id="原来如此">原来如此</h3><p>现在我们可以解释刚刚遇到的奇怪情况了：</p><ul><li>“为什么这个长度是 13 ？明明是 9 个字符啊！”——因为 Python 自动帮你编码了，编码后是 13 个字节，常见的汉字在 UTF-8 编码下为 3 个字节</li><li>“<code>s</code> 你怎么坏掉了？” ——<code>str</code> 本来就是字节数组</li><li>“我可能用了假的 <code>encode()</code>”——你不应该对 <code>str</code> 变量做 <code>encode</code>，它本来就是编码后的</li><li>“喵喵喵？”——这是正常的，只是因为 Python 2 没有把 Unicode 字符显示成中文字符，用 print 就没问题了：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> s</span><br><span class="line">Hello, 世界</span><br></pre></td></tr></table></figure><h2 id="解决方案">解决方案</h2><ol type="1"><li>永远记住 str 其实是 bytes，字符串应该用 unicode，尤其是包含中文时</li><li>如果能说服你的老板和同事，尽快把 Python 2 升级到 3</li></ol><p>最后，如果你需要写出兼容 Python 2\3 的程序，<a href="http://python-future.org/compatible_idioms.html#strings-and-bytes" target="_blank" rel="noopener">这篇文档</a>可以给你一些帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Tips:&lt;/strong&gt; 如果您已经充分理解问题是什么，请直接跳到 &lt;a href=&quot;#问题出在哪里&quot;&gt;#问题出在哪里&lt;/a&gt; 一节。&lt;/p&gt;
&lt;h2 id=&quot;字符串和编码&quot;&gt;字符串和编码&lt;/h2&gt;
&lt;p&gt;先从概念说起，字符串和它的编码是两个不同的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;字符串&lt;/strong&gt;是一段文字本身，可以是中文可以是英文，以及各种语言&lt;/li&gt;
&lt;li&gt;字符串的&lt;strong&gt;编码&lt;/strong&gt;是计算机存储、处理字符串的方式；作为一种&lt;strong&gt;数据&lt;/strong&gt;，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;字符串经过&lt;strong&gt;编码（encode）&lt;/strong&gt; 就成为了一堆数据，反过来，数据经过&lt;strong&gt;解码（decode）&lt;/strong&gt; 就变回我们认识的字符串。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/09/encode_decode.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>用 Bandit 做 Python 代码静态安全分析</title>
    <link href="https://ericfu.me/bandit-the-python-static-analyzer/"/>
    <id>https://ericfu.me/bandit-the-python-static-analyzer/</id>
    <published>2017-08-16T01:50:47.000Z</published>
    <updated>2017-08-21T00:55:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bandit-是什么">Bandit 是什么？</h2><p>Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。</p><p>安装说明：参见 <a href="https://github.com/openstack/bandit" target="_blank" rel="noopener">GitHub 项目主页</a>。</p><a id="more"></a><h2 id="编写自定义的检查">编写自定义的检查</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@bandit.checks('Call')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prohibit_unsafe_deserialization</span><span class="params">(context)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'unsafe_load'</span> <span class="keyword">in</span> context.call_function_name_qual:</span><br><span class="line">        <span class="keyword">return</span> bandit.Issue(</span><br><span class="line">            severity=bandit.HIGH,</span><br><span class="line">            confidence=bandit.HIGH,</span><br><span class="line">            text=<span class="string">"Unsafe deserialization detected."</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><ul><li><code>@bandit.checks('Call')</code>: 仅仅检查类型为 <code>Call</code> 的 AST Node</li><li><code>return bandit.Issue(...)</code>: 返回一个 Security Issue</li></ul><h2 id="源码分析">源码分析</h2><p>入口在 <code>cli/main.py</code> 的 <code>main()</code></p><p>先初始化了一堆参数，然后在这里创建了一个关键的 BanditManager 对象，之后的事情都是由它来完成的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b_mgr = b_manager.BanditManager(b_conf, args.agg_type, args.debug,</span><br><span class="line">                                profile=profile, verbose=args.verbose,</span><br><span class="line">                                ignore_nosec=args.ignore_nosec)</span><br></pre></td></tr></table></figure><h3 id="扫描文件">扫描文件</h3><p>紧接着就能看到这行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initiate file discovery step within Bandit Manager</span></span><br><span class="line">b_mgr.discover_files(args.targets, args.recursive, args.excluded_paths)</span><br></pre></td></tr></table></figure><p>让我们跟进去，然后看看 <code>discover_files()</code> 都做了些什么：（代码太长就不贴了）</p><ul><li>处理 include/exclude 参数<ul><li>如果有 include 就只看这里面的文件，否则扫描所有文件</li><li>如果有 exclude，之后扫描的时候要去掉这些文件</li></ul></li><li>对所有指定的目标进行扫描<ul><li>如果设置了 recursive 选项，就递归地遍历子目录。</li></ul></li></ul><p>最后把遍历的结果排序并以列表的形式存放在 <code>self.files_list</code> 中。</p><h3 id="运行-tests">运行 Tests</h3><p>回到 <code>main()</code> 函数中，再往下看一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initiate execution of tests within Bandit Manager</span></span><br><span class="line">b_mgr.run_tests()</span><br></pre></td></tr></table></figure><p>看来这里是核心的一步，当然要走进去看看：（代码不贴了）</p><ul><li>枚举刚刚列表中的所有文件，读出来、并调用 <code>_parse_file()</code> 处理之。</li><li>如果处理失败了，也记下来，最后汇总输出会用到。</li></ul><p>继续跟进 <code>_parse_file()</code>，发现只是个包装，进入 <code>_execute_ast_visitor()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_execute_ast_visitor</span><span class="params">(self, fname, data, nosec_lines)</span>:</span></span><br><span class="line">    score = []</span><br><span class="line">    res = b_node_visitor.BanditNodeVisitor(fname, self.b_ma,</span><br><span class="line">                                           self.b_ts, self.debug,</span><br><span class="line">                                           nosec_lines, self.metrics)</span><br><span class="line"></span><br><span class="line">    score = res.process(data)</span><br><span class="line">    self.results.extend(res.tester.results)</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>这个 BanditNodeVisitor 虽然没有继承标准库里的 ast.NodeVisitor 但实际上做的工作就是那样的——遍历所有 AST Node，同时对各个类型的 Node 执行对应的函数。</p><p>在 BanditNodeVisitor 中定义了很多类似 <code>visit_Call</code>, <code>visit_FunctionDef</code>, <code>visit_Str</code> 这样奇怪名字的函数，顾名思义就是对各个类型的 Node 所运行的函数。遍历的逻辑看 <code>visit</code> 函数。</p><p>以 <code>visit_Call</code> 为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visit_Call</span><span class="params">(self, node)</span>:</span></span><br><span class="line">    self.context[<span class="string">'call'</span>] = node</span><br><span class="line">    qualname = b_utils.get_call_name(node, self.import_aliases)</span><br><span class="line">    name = qualname.split(<span class="string">'.'</span>)[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    self.context[<span class="string">'qualname'</span>] = qualname</span><br><span class="line">    self.context[<span class="string">'name'</span>] = name</span><br><span class="line"></span><br><span class="line">    self.update_scores(self.tester.run_tests(self.context, <span class="string">'Call'</span>))</span><br></pre></td></tr></table></figure><p>其实很简单，把对应的一些上下文信息 extract 出来并存到 <code>self.context</code>，然后用 <code>tester.run_tests</code> 执行所有对应 Call Node 的检查。</p><p>所以 <code>run_tests()</code> 的逻辑你应该能猜到个大概了：</p><ol type="1"><li>拿到所有类型为 <code>checktype</code> 的检查</li><li>对每个检查，以当前的 <code>context</code> 作为参数做检查，如果检查出问题就存起来</li></ol><h3 id="输出结果">输出结果</h3><p>再回到 <code>main()</code> 函数中，再往下就是输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trigger output of results by Bandit Manager</span></span><br><span class="line">sev_level = constants.RANKING[args.severity - <span class="number">1</span>]</span><br><span class="line">conf_level = constants.RANKING[args.confidence - <span class="number">1</span>]</span><br><span class="line">b_mgr.output_results(args.context_lines,</span><br><span class="line">                     sev_level,</span><br><span class="line">                     conf_level,</span><br><span class="line">                     args.output_file,</span><br><span class="line">                     args.output_format)</span><br></pre></td></tr></table></figure><p>Severity 和 Confidence 都是用来过滤的。最后输出到指定的形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;bandit-是什么&quot;&gt;Bandit 是什么？&lt;/h2&gt;
&lt;p&gt;Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。&lt;/p&gt;
&lt;p&gt;安装说明：参见 &lt;a href=&quot;https://github.com/openstack/bandit&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub 项目主页&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
      <category term="security" scheme="https://ericfu.me/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>获得第三届阿里中间件性能大赛冠军</title>
    <link href="https://ericfu.me/aliware-performance-contest-first-prize/"/>
    <id>https://ericfu.me/aliware-performance-contest-first-prize/</id>
    <published>2017-07-18T16:48:53.000Z</published>
    <updated>2017-08-18T00:48:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。</p><p><img src="/images/2017/07/alibaba-praise.jpg"></p><p>Emmm... 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。</p><a id="more"></a><h2 id="第一赛季消息中间件">第一赛季：消息中间件</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/middlewarerace2017/open-messaging-demo" target="_blank" rel="noopener">这里</a>）</p><p>设计一个消息中间件，它支持 Topic 和 Queue，每个 Topic 可以被一个或多个 Queue 订阅。显然，发往 Topic 的消息会被所有的订阅者接收到，而发往某个特定 Queue 的消息只会被它接收到。</p><p>消息不能丢失，且对于各个 Queue 消息要保持有序。在此前提下，吞吐量最优者获胜。</p><p>要注意的是，生产和消费是分两个阶段进行的：先进行生产，结束后再进行消费。</p></blockquote><p>我的思路很简单，对于每个 Topic 或者 Queue，把所有消息序列化后写入对应的一个文件中。消费时，从磁盘中顺序读取即可。</p><p>本题最后瓶颈落在了磁盘 IO 上，所以很多优化也变得无关紧要了。为了充分利用磁盘 IO，一定要确保两点：</p><ul><li>顺序写入和读取（参见我的<a href="https://ericfu.me/varnish-notes-from-archetect/">这篇文章</a>）</li><li>使用 Linux 内存映射（mmap）技术，也就是 Java nio 包里的 MappedByteBuffer</li></ul><p>此外，序列化的过程中尽可能减少内存拷贝，以及避免不必要的 String 序列化、反序列化。</p><p>做到以上几点，进入前 20 不是问题。初赛只要前 200 名就可以晋级了（吐槽：那很容易啊！！但是一开始不知道初赛成绩也要计入最后评分的）</p><p><a href="https://code.aliyun.com/fuyufjh/open-messaging-demo-zero/" target="_blank" rel="noopener">代码</a>托管在阿里云 code 上。</p><h2 id="第二赛季数据库同步">第二赛季：数据库同步</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/wanshao/IncrementalSync/" target="_blank" rel="noopener">这里</a>）</p><p>模拟数据库的主备复制。给定一批 binlog 文本数据，你的任务是重放所有 binlog 从而得到数据表的最新状态（假设原本状态是空的）。重放的操作包含 Insert/Update/Delete 三种操作，注意主键也可能被更新！</p><p>验证结果的方式是，客户端给定一个 PK 的区间，输出该区间的所有行。</p><p>注意，程序分 Server 和 Client 端，机器配置都很高（16核CPU），但是 JVM 的堆大小被限制为 1G 新生代 + 2G 老年代。此外，必须顺序读取 binlog。</p></blockquote><p>思路如下：并行化是一定要的，如何并行呢？答案（当然）是按主键哈希分桶。然而这样主键更新怎么处理？这是最大的难点。</p><p>为此我们想出了一种并行化的算法——可以说这就是最终获得冠军的原因。具体思路在决赛答辩 PPT 里写的很清楚啦。</p><p>答辩PPT（包括完整的解题思路）：</p><iframe src="//www.slideshare.net/slideshow/embed_code/key/jURs7pfS57Uz7d" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen></iframe><div style="margin-bottom:5px"><p>这题思考了很久，最后交出的答卷含金量也很高。哈希分桶作为经典算法中常见的一种模式，在很多分布式系统中都有应用。</p><p>此外，比赛初期还没有加上“必须顺序读取”这个条件。如果没有这个条件又会有别的奇思妙想的算法来解决。留给读者自己思考（喵）</p><p><a href="https://code.aliyun.com/fuyufjh/IncrementalSyncSequential" target="_blank" rel="noopener">代码</a> 托管在阿里云上，同时还有 <a href="https://code.aliyun.com/fuyufjh/tianchi-middleware-doc" target="_blank" rel="noopener">设计文档</a> 也很有帮助（如果你觉得 PPT 还不够细致）。</p><h2 id="小时极客-pk-赛">24 小时极客 PK 赛</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/fuyufjh/topkn_final" target="_blank" rel="noopener">这里</a>）</p><p>分页排序。给定一批数据，求解按顺序从小到大，顺序排名从第k下标序号之后连续的n个数据，类似于 <code>order by id limit k, n</code>，n 会很大，k &lt;= 100</p><p>数据是文本文件，每行是一个长度在 256 字符以内的字符串。排序的方式是：先按长度从小到大、再按字典序。</p><p>注意一共有 2 台 Worker 和 1 台 Master，Worker 上分别放了 5G 的数据，最后 Master 要请求到完整的排序结果。</p></blockquote><p>和之前的两场比赛不同，24小时赛的成绩是不计入最后评价的，而且是以个人名义参赛。奖品是去硅谷开（玩）会（耍）！（PS. 然而不能带女朋友，最后去了一帮基佬啊摔！）</p><p>外排序是很容易想到的思路，然而十分复杂，也不能很好的并行。那怎么办？依然是分桶，Bucket Sort。</p><p>假设字符串长度的分布是均匀的，字符出现的概率也是相近的，则我们可以用以下的值作为每个字符串的 key：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;length&gt; &lt;text[0]&gt; &lt;text[1]&gt; &lt;text[2]&gt;</span><br></pre></td></tr></table></figure><p>（如果长度小于 3 就用 '\0' 填充一下）</p><p>这样我们就能对每个 key 做统计了——有多少个字符串是在这个桶里呢？假设这个结果放在数组 <code>count</code> 中，那再对 <code>count</code> 求一个累计和，就能用二分查找找到第 N 个数应该位于哪一个桶里。</p><p>同样的也可以找到第 N+K-1 个数应该位于哪个桶里。然后就简单了，把区间内的桶里的值都取出来，让 Master 排个序就好了。</p><p>把区间内的桶里的值都取出来？——这个过程可以扫描整个文件。也可以做个预处理的优化：把每个字符串的 key 和 offset 分别存到两个数组里，比如 <code>keys[]</code> 和 <code>offsets[]</code>，这样只要扫描数组就可以了！</p><p>然而我并没有想到这个优化，所以是第二名。哈哈哈！</p><p><a href="https://code.aliyun.com/fuyufjh/topkn_final?spm=a2111a.8458726.0.0.59556667DTd4xf" target="_blank" rel="noopener">代码</a>托管在阿里云上。</p><h2 id="后记">后记</h2><ol type="1"><li>算法是好的，要多刷 HackerRank</li><li>阿里搞的这个比赛啊，Excited ！</li></ol></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/07/alibaba-praise.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;Emmm... 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
</feed>
