<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding Husky</title>
  
  <subtitle>Thoughts, stories and ideas.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ericfu.me/"/>
  <updated>2018-10-14T07:03:27.949Z</updated>
  <id>https://ericfu.me/</id>
  
  <author>
    <name>Eric Fu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从 F1 Query 论文看 SQL 查询的执行模式</title>
    <link href="https://ericfu.me/f1-query-and-approaches-of-sql-execution/"/>
    <id>https://ericfu.me/f1-query-and-approaches-of-sql-execution/</id>
    <published>2018-10-14T06:09:12.000Z</published>
    <updated>2018-10-14T07:03:27.949Z</updated>
    
    <content type="html"><![CDATA[<style type="text/css">.image-captain {    margin-top: -20px;}</style><p><img src="/images/2018/10/banner-lakeview.jpg" alt=""></p><p>F1 是起源于 Google AdWords 的分布式 SQL 查询引擎，跟底下的 Spanner 分布式存储搭配，开启了分布式关系数据库——所谓 NewSQL 的时代。我们今天说的是 F1 团队在 VLDB2018 上发的文章 <a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf" target="_blank" rel="noopener">F1 Query: Declarative Querying at Scale</a>，它和之前我们说的 F1 几乎是两个东西。</p><p>F1 Query 是一个分布式的 SQL 执行引擎，现在大数据领域流行的 Presto、Spark SQL、Hive 等等，都可以算在这个范畴里。类似地，F1 Query 也支持对各种不同数据源的查询，既可以是传统的关系表、也可以是 Parquet 这样的半结构化数据。</p><a id="more"></a><p>这样一来，不同数据格式的壁垒也被打破了：你可以在一个系统里完成对不同数据源的 Join，无论数据以什么形式存放在哪里。商业上管这个叫 Federated Query 或者 DataLake，几家云计算巨头都有类似的产品。</p><p><strong>那 F1 Query 的贡献在哪里呢？</strong></p><p>F1 Query 定义了三种不同类型的查询执行模式，根据查询的数据量大小或执行时间，将用户查询划分成：</p><ol><li>单机执行（Centralized Execution）</li><li>分布式执行（Distributed Execution）</li><li>批处理执行（Batch Execution）</li></ol><p>前两个是交互式的，即客户端会等待结果返回。最后一个批处理更像是 ETL：客户端输入任务之后就不再管了，查询结果会被写到指定的地方。</p><h2 id="单机执行"><a href="#单机执行" class="headerlink" title="单机执行"></a>单机执行</h2><p>单机执行对应我们熟悉的 OLTP 查询，例如单表点查、带索引的 Join 等。这类查询本身已经足够简单，只需几毫秒就能做完，处理它们的最好方式就是在收到请求的机器上立即执行。</p><p>在 F1 Query 系统中有 F1 Server 和 F1 Worker 等角色。F1 Server 负责接收客户端请求，如果它判断这个查询应当使用单机而不是分布式方式执行，它就亲力亲为、直接执行并返回结果。</p><p>这样的行为和绝大多数单机 OLTP 数据库是一致的，例如 MySQL 采用的是 Thread Pool + Dispatcher 的处理模型，Thread Pool 的规模是一定的，Dispatcher 根据高低优先级分派执行任务。最终一个请求只会被一个线程处理，换句话说，对某个查询来说其执行过程是单线程的。</p><p><img src="/images/2018/10/mysql-thread-group.jpg" alt=""></p><p class="image-captain">▲ <em>MySQL 的线程池处理模型，一般存在多个 Thread Group，图中描绘了一个 Thread Group</em></p><p>F1 Query 单机查询的执行器同样也是教科书式的 Valcano 模型，但也无可厚非——对 OLTP 来说这已经足够好。如下图所示，从顶层算子开始递归地调用 <code>GetNext()</code>，每次取出一行数据，直到没有更多数据为止。各个算子只需要实现 <code>GetNext()</code> 接口即可，简单清晰。</p><p><img src="/images/2018/10/f1-query-valcano.jpg" alt=""></p><h2 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h2><p>F1 Query 对更复杂的查询，例如没有索引的 Join 或聚合等，则采取分布式查询的方式。大部分 OLAP 查询、尤其是 Ad-hoc 的查询都落在这一分类中。这种情况下，分布式导致的网络、调度等 Overhead 已经远小于查询本身的成本；而且随着数据量的增加，单节点内存显然不够用了。</p><p><img src="/images/2018/10/f1-query-system-arch.jpg" alt=""></p><p class="image-captain"> ▲ <em>F1 Query 的系统架构，主要包含 F1 Master、F1 Server、F1 Worker 三个角色，其他 Catalog、UDF Server、Batch Metadata 用于存储查询相关的 Metadata 等</em> </p><p>这时，上图中的 F1 Worker 就派上用场了。<strong>F1 Server 此时仅仅作为协调者存在，将任务分配给多个 Worker</strong>，直到 Worker 的任务全都完成，再把结果汇总发给客户端。</p><p>这个模式眼熟吗？你可能会想到 Greenplum 这类的数据仓库，已经很接近了。最相似的我认为是 Presto。Presto 是 Facebook 开发的一套分布式 SQL 引擎，如果单单只看 F1 Query 的分布式查询，和 Presto 大同小异。</p><p>与单机执行不同的是，<strong>分布式查询中的算子可以有多个实例（Instance）并行执行，每个实例负责其中一部分数据</strong>。在 F1 Query 里这样一个数据分片被称为 Fragment，在 Spark SQL 里叫 Partition，在 Presto 里叫 Split。</p><p><img src="/images/2018/10/f1-query-fragments.jpg" alt=""></p><p>下面的例子是一个 Join-Aggregation-Sort 的查询，它分成了 4 个阶段：</p><ol><li><code>Scan(Clicks)</code> 被分配给 1000 个 F1 Worker 上并行拉取数据，并根据每一行数据的 <code>Hash(AdID)</code> 发送给对应的 <code>HashJoin</code> 分片，即一般说的 shuffle 过程;</li><li><code>Scan(Ads)</code> 被分配给 200 个 F1 Worker 上并行拉去数据，并且也以同样的方式做 shuffle；</li><li><code>HashJoin</code> 及 <code>PartialAggregation</code>：根据 Join Key 分成了 1000 个并行任务，各自做 Join 计算，并做一次聚合；</li><li>最后，F1 Server 把各个分片的聚合结果再汇总起来，返回给客户端。</li></ol><p><img src="/images/2018/10/f1-query-distributed-plan.jpg" alt=""></p><p>Presto 具有的缺陷，F1 Query 分布式查询同样也有，比如：</p><ul><li>纯内存的计算方式，无法利用磁盘的存储空间，某些查询可能面临内存不足；</li><li>没有 Fault-tolerance，对于一个涉及上千台 Worker 的查询，任何一台的重启都会导致查询失败。</li></ul><h2 id="批处理执行"><a href="#批处理执行" class="headerlink" title="批处理执行"></a>批处理执行</h2><p>F1 Query 还有个独特的批处理执行，这个模式定位于更大的数据量、更久的查询时间；另一方面，它的结果不再是返回给客户端，而是将查询结果写到指定的地方，例如 Colossus（第二代 GFS）上。</p><p>上一节我们提道，Presto 的模式没有 Fault-tolerance，这对于长时间运行的批处理任务是致命的，查询失败的概率会大大增加。批处理查询首先要解决的就是 Fault-tolerance 问题：<strong>必须能以某种方式从 Worker 节点的失败中恢复</strong>。</p><p>解决这个问题有两条路可走：一是 MapReduce 的模式，将计算分成若干个阶段（Stage），而中间结果持久化到 HDFS 这样的分布式文件系统上；二是 Spark RDD 模式，通过记录祖先（Lineage）信息，万一发生节点失败，就通过简单的重算来恢复丢失的数据分片，这样数据就可以放在内存里不用落盘。</p><p>Spark 的做法显然是更先进的，原因有很多，这里只说最重要的 2 条。欲知详情可以看我之前的博客文章<a href="https://ericfu.me/apache-spark-in-nutshell/">《一文读懂 Apache Spark》</a>。</p><ol><li>Spark 的计算基本在内存中，只有当内存不够时才会溢出到磁盘，而 MR 的中间结果必须写入外部文件系统；</li><li>Spark 可以把执行计划 DAG 中相互不依赖的 Stage 并行执行，而 MR 只能线性地一个接一个 Stage 执行。</li></ol><p>但是出乎意料的是，F1 Query 采用的是前者，也就是 MR 模式。这其中的原因我们不得而知，我猜想和 Google 自家的 FlumeJava 不够给力有关系。</p><p>如下图。左边的 Physical Plan 和上一节的分布式查询是一样的，不同之处是<strong>在批处理模式下，它被转换成一系列的 MR 任务</strong>，之后交给调度器（Scheduler）去处理即可。</p><p><img src="/images/2018/10/f1-query-plan-to-mr-stages.jpg" alt=""></p><p>相比分布式查询的执行方式，MapReduce 模式下各个步骤都会持久化到外部文件系统。不仅如此，<strong>Pipeline 的执行也没法进行</strong>。以上一节提到的 HashJoin 为例，左边 <code>Clicks</code> 的 Scan 和 HashJoin 原本是可以 Pipeline 执行的，但是在批处理模式下，必须等到 <code>Scan(Clicks)</code> 这个阶段完成才能进行下一步的 HashJoin 阶段。</p><h2 id="单机并行执行"><a href="#单机并行执行" class="headerlink" title="单机并行执行"></a>单机并行执行</h2><p>除了上面聊的 F1 Query 所支持的 3 种查询模式之外，事实上还有一种处理模型位于单线程执行和分布式执行之间：单机的并行查询。初看这似乎与分布式查询很相似，但又有些不同：</p><ul><li>不用考虑单个 Worker 的失败恢复，因为它们都在同一个进程里；</li><li>各个 Worker 线程的内存是共享的，它们之间交换数据无需考虑网络通讯代价。</li></ul><p>这种模式在传统的关系型数据库上也很常见，尤其是 Postgres、SQL Server 这类以 OLAP 查询见长的选手。以 Postgres 为例，在开启并行查询的情况下，查询优化器会根据代价选择是否生成并行执行计划；如果生成了并行执行计划，执行器会调度多个 Worker 一起完成工作。</p><p>下图是一个 Postgres 上并行 Hash Join 的例子，从执行计划上看和上一节几乎一样，唯一区别是这里的 shuffle 过程变得容易多了，不再是一件代价很高的事情。</p><p><img src="/images/2018/10/postgres-parallel-query-example.png" alt="postgres-parallel-query-example"></p><p>相比分布式查询，<strong>单机并行的最大优势在于响应速度更快</strong>，因为省去了大量的网络 IO 时间，而且调度一个 Worker 线程要比调度一个 Worker 机器快得多。</p><p>但别忘了，单机运算能力的 scale up 成本非常高，并且是存在上限的。对于 Google 之类的互联网公司，绝大部分查询都超出了单机的存储或计算能力，我猜测这也是 F1 Query 并未考虑单机并行的理由。</p><h2 id="对-F1-Query-的评价"><a href="#对-F1-Query-的评价" class="headerlink" title="对 F1 Query 的评价"></a>对 F1 Query 的评价</h2><p>从论文透露的情况来看，F1 Query 还不算个完善、成熟的系统，其定位更像是一个解决业务需求的胶水系统，而非 Spanner 这样的“硬核”技术。它追求的是够用就好，很多地方其实还有很大的改进空间，举几个例子：</p><ul><li>对交互式查询，选择分布式还是单机计算目前还是基于启发式规则。</li><li>三种模式的执行计划是用一样的优化器生成的。但是客观的说，这其中的差别可是不小的。</li><li>优化器是基于规则的。之所以不做 CBO，论文给出的解释是数据源太多，不容易计算。</li><li>批处理模式下用 Spark 取代 MR 的模式是更好的选择。</li></ul><p>F1 Query 希望用一套系统解决所有 OLTP、OLAP、ETL 需求、用一套系统访问数据中心里各种格式的数据，这两点才是 F1 Query 的核心竞争力。</p><h2 id="SQL-执行模式总结"><a href="#SQL-执行模式总结" class="headerlink" title="SQL 执行模式总结"></a>SQL 执行模式总结</h2><p>从数据库的视角看，理想的数据库应当隐藏掉查询执行的种种细节，只要用户输入一个声明（例如 SQL），就能以最优的方式进行执行并给出答案。F1 Query 做了个勇敢的尝试，<strong>它将多种执行模型揉合在一个系统中，共享同一套优化器和算子</strong>，以较低的开发成本获得其中最优的执行性能（在理想情况下）。</p><p>下面的表格总结了 4 种执行模式的优势和不足。</p><table><thead><tr><th></th><th>单线程</th><th>并行执行</th><th>分布式并行执行</th><th>批处理</th></tr></thead><tbody><tr><td>代表系统</td><td>MySQL / Oracle</td><td>Postgres / MSSQL</td><td>Presto / Greenplum</td><td>Spark / MapReduce</td></tr><tr><td>硬件架构</td><td>单核</td><td>SMP / NUMA</td><td>MPP</td><td>MPP</td></tr><tr><td>伸缩性</td><td>无</td><td>Scale Up</td><td>弹性 Scale Out</td><td>弹性 Scale Out</td></tr><tr><td>Fault-Toralence</td><td>无</td><td>无</td><td>重试整个查询</td><td>Worker 级 fail-over</td></tr><tr><td>典型数据量</td><td>若干个 Tuple</td><td>单机内存可容纳</td><td>大数据</td><td>大数据</td></tr><tr><td>典型响应时间</td><td>毫秒</td><td>数百毫秒</td><td>秒级</td><td>秒级到数小时</td></tr></tbody></table><p>总而言之，所谓 <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" target="_blank" rel="noopener"><em>No Free Launch</em></a> —— 没有最优的方案，<strong>数据量是决定能选用哪个执行模式的前提</strong>。实践中，先确保数据量能够承载的下，再谈优化也就明白多了。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf" target="_blank" rel="noopener">F1 Query: Declarative Querying at Scale</a></li><li><a href="https://www.programering.com/a/MTO5YDMwATI.html" target="_blank" rel="noopener">MySQL Thread Pool Implementation</a></li><li><a href="https://tech.meituan.com/presto.html" target="_blank" rel="noopener">Presto 实现原理和美团的使用实践 - 美团技术团队</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;style type=&quot;text/css&quot;&gt;
.image-captain {
    margin-top: -20px;
}
&lt;/style&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018/10/banner-lakeview.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;F1 是起源于 Google AdWords 的分布式 SQL 查询引擎，跟底下的 Spanner 分布式存储搭配，开启了分布式关系数据库——所谓 NewSQL 的时代。我们今天说的是 F1 团队在 VLDB2018 上发的文章 &lt;a href=&quot;http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;F1 Query: Declarative Querying at Scale&lt;/a&gt;，它和之前我们说的 F1 几乎是两个东西。&lt;/p&gt;
&lt;p&gt;F1 Query 是一个分布式的 SQL 执行引擎，现在大数据领域流行的 Presto、Spark SQL、Hive 等等，都可以算在这个范畴里。类似地，F1 Query 也支持对各种不同数据源的查询，既可以是传统的关系表、也可以是 Parquet 这样的半结构化数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>一文读懂 Apache Spark</title>
    <link href="https://ericfu.me/apache-spark-in-nutshell/"/>
    <id>https://ericfu.me/apache-spark-in-nutshell/</id>
    <published>2018-06-12T13:12:00.000Z</published>
    <updated>2018-06-20T11:23:35.256Z</updated>
    
    <content type="html"><![CDATA[<style type="text/css">.image-captain {    margin-top: -20px;}</style><p><img src="/images/2018/06/spark-banner.png" alt="spark"></p><p>Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><p>Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。</p><a id="more"></a><p>Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。</p><h2 id="MapReduce-的问题所在"><a href="#MapReduce-的问题所在" class="headerlink" title="MapReduce 的问题所在"></a>MapReduce 的问题所在</h2><p>Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。</p><p>MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，<strong>复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来</strong>：</p><p><img src="/images/2018/06/multi-stage-mapreduce.png" alt="multi-stage-mapreduce"></p><p class="image-captain">▲ <em>上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。</em></p><p>这种方式下，<strong>每次中间结果都要写入 HDFS 落盘保存，代价很大</strong>（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。</p><h2 id="Spark-与-RDD-模型"><a href="#Spark-与-RDD-模型" class="headerlink" title="Spark 与 RDD 模型"></a>Spark 与 RDD 模型</h2><p>针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。</p><p>一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。<strong>Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。</strong>Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。</p><p><img src="/images/2018/06/rdd-example.png" alt="rdd-example"></p><p><strong>RDD 是一个只读的、分区的（partitioned）数据集合</strong>。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。<strong>RDD 通过算子连接构成有向无环图（DAG）</strong>，上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。</p><p>回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以<strong>一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区</strong>。这个过程可以递归的进行下去。</p><p><img src="/images/2018/06/rdd-example-crash.png" alt="rdd-example-crash"></p><p class="image-captain">▲ <em>上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。</em></p><p>Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 <code>collect()</code> 这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类：</p><ul><li><code>map()</code>、<code>filter()</code>、<code>join()</code> 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。</li><li><code>collect()</code>、<code>count()</code>、<code>save()</code> 等算子称为 Action，它们通常是将数据收集起来返回；</li></ul><p><img src="/images/2018/06/spark-rdd-api-example.png" alt="spark-rdd-api-example"></p><p class="image-captain">▲ <em>上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。</em></p><p>像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。</p><p>此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：<strong>窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</strong>，举例来说：</p><ul><li><code>map()</code>、<code>filter()</code> 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。</li><li><code>groupByKey()</code> 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。</li></ul><p><img src="/images/2018/06/2-kinds-of-dependencies.png" alt="2-kinds-of-dependencies"></p><p class="image-captain">▲ <em>左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。</em></p><p>在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。<strong>然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。</strong>整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。</p><blockquote><p>了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。</p></blockquote><h2 id="声明式接口：Spark-SQL"><a href="#声明式接口：Spark-SQL" class="headerlink" title="声明式接口：Spark SQL"></a>声明式接口：Spark SQL</h2><p>Spark 诞生后，大幅简化了 MapReduce 编程模型，但人们并不满足于此。我们知道，<strong>与命令式（imperative）编程相对的是声明式（declarative）编程，前者需要告诉程序怎样得到我需要的结果，后者则是告诉程序我需要的结果是什么</strong>。举例而言：你想知道，各个部门 <code>&lt;dept_id, dept_name&gt;</code> 中性别为女 <code>&#39;female&#39;</code> 的员工分别有多少？</p><p>命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">employees = db.getAllEmployees()</span><br><span class="line">countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count)</span><br><span class="line">for employee in employees:</span><br><span class="line">    if (employee.gender == &apos;female&apos;)</span><br><span class="line">        countByDept[employee.dept_id] += 1</span><br><span class="line">results = list() // 加上 dept.name 列</span><br><span class="line">depts = db.getAllDepartments()</span><br><span class="line">for dept in depts:</span><br><span class="line">    if (countByDept containsKey dept.id)</span><br><span class="line">        results.add(row(dept.id, dept.name, countByDept[dept.id]))</span><br><span class="line">return results;</span><br></pre></td></tr></table></figure><p>声明式编程中，你只要用关系代数的运算表达出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">employees.join(dept, employees.deptId == dept.id)</span><br><span class="line">         .where(employees.gender == &apos;female&apos;)</span><br><span class="line">         .groupBy(dept.id, dept.name)</span><br><span class="line">         .agg()</span><br></pre></td></tr></table></figure><blockquote><p>等价地，如果你更熟悉 SQL，也可以写成这样：</p></blockquote><blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dept.id, dept.name, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> employees.dept_id == dept.id</span><br><span class="line"><span class="keyword">WHERE</span> employees.gender = <span class="string">'female'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dept.id, dept.name</span><br></pre></td></tr></table></figure></blockquote><p>显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。</p><p>Spark SQL 就是这样一个基于 SQL 的声明式编程接口。<strong>你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。</strong></p><p><img src="/images/2018/06/spark-sql-arch.png" alt="spark-sql-arch"></p><p class="image-captain">▲ <em>上图黄色部分是 Spark SQL 中新增的部分。</em></p><p><strong>DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。</strong>计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。</p><p><strong>Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。</strong>Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。</p><p><img src="/images/2018/06/spark-sql-catalyst.png" alt="spark-sql-catalyst"></p><p class="image-captain">▲ <em>上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。</em></p><h2 id="流计算框架：Spark-Streaming"><a href="#流计算框架：Spark-Streaming" class="headerlink" title="流计算框架：Spark Streaming"></a>流计算框架：Spark Streaming</h2><p>以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。</p><p>Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 <strong>D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。</strong></p><p><img src="/images/2018/06/spark-streaming.png" alt="spark-streaming"></p><p>Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势：</p><ul><li>更高的吞吐量（大约是 Storm 的 2-5 倍）</li><li>更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可</li><li>开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算</li></ul><p><img src="/images/2018/06/continuous-vs-spark-d-stream.png" alt="continuous-vs-spark-d-stream"></p><p class="image-captain">▲ <em>上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。</em></p><p>你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？</p><p>答案是通过 RDD：<strong>将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果</strong>。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。</p><p><img src="/images/2018/06/d-stream-state-rdd.png" alt="d-stream-state-rdd"></p><p class="image-captain">▲ <em>上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。</em></p><p>那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD <code>counts</code> 会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！<strong>因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut</strong>，在它之前更早的 RDD 就可以没有顾虑地清理掉了。</p><blockquote><p>关于流行的几个开源流计算框架的对比，可以参考文章 <a href="https://www.cakesolutions.net/teamblogs/comparison-of-apache-stream-processing-frameworks-part-1" target="_blank" rel="noopener">Comparison of Apache Stream Processing Frameworks</a>。</p></blockquote><h2 id="流计算与-SQL：Spark-Structured-Streaming"><a href="#流计算与-SQL：Spark-Structured-Streaming" class="headerlink" title="流计算与 SQL：Spark Structured Streaming"></a>流计算与 SQL：Spark Structured Streaming</h2><p>Spark 通过 Spark Streaming 拥有了流计算能力，那 Spark SQL 是否也能具有类似的流处理能力呢？答案是肯定的，只要将<strong>数据流建模成一张不断增长、没有边界的表</strong>，在这样的语义之下，很多 SQL 操作等就能直接应用在流数据上。</p><blockquote><p>出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。</p></blockquote><p>很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为<strong>增量化</strong>（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。</p><p><img src="/images/2018/06/spark-structured-streaming-model.png" alt="spark-structured-streaming-mode"></p><p class="image-captain">▲ <em>左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。</em></p><p>与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。</p><p><strong>窗口（window）是对过去某段时间的定义。</strong>批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。</p><p><strong>水位（watermark）用来丢弃过早的数据。</strong>在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。</p><p><img src="/images/2018/06/spark-structured-streaming-watermark.png" alt="spark-structured-streaming-watermark"></p><p class="image-captain">▲ <em>上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。</em></p><p>水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。</p><blockquote><p>关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 <a href="https://flink.apache.org/" target="_blank" rel="noopener">Apache Flink</a>。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。</p></blockquote><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>Spark 中有三个角色：Driver, Worker 和 Cluster Manager。</p><p><img src="/images/2018/06/cluster-overview.png" alt="cluster-overview"></p><p><strong>驱动程序（Driver）</strong>即用户编写的程序，对应一个 <code>SparkContext</code>，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。</p><p><strong>Cluster Manager</strong> 顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。</p><p><strong>Worker</strong> 节点负责执行计算任务，上面保存了 RDD 等数据。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。</p><p>Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 为 Spark SQL 提供了流计算支持，它将输入的数据流看作不断追加的数据行。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" target="_blank" rel="noopener">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing (NSDI ‘12)</a> </li><li><a href="https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf" target="_blank" rel="noopener">Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP ‘13)</a></li><li><a href="https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf" target="_blank" rel="noopener">Spark SQL: Relational Data Processing in Spark (SIGMOD ‘15)</a></li><li><a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark (SIGMOD ‘18)</a></li><li><a href="https://www.cakesolutions.net/teamblogs/comparison-of-apache-stream-processing-frameworks-part-1" target="_blank" rel="noopener">Comparison of Apache Stream Processing Frameworks</a></li><li><a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener">Structured Streaming in Apache Spark</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;style type=&quot;text/css&quot;&gt;
.image-captain {
    margin-top: -20px;
}
&lt;/style&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018/06/spark-banner.png&quot; alt=&quot;spark&quot;&gt;&lt;/p&gt;
&lt;p&gt;Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。&lt;/p&gt;
&lt;p&gt;Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>处理海量数据：列式存储综述（系统篇）</title>
    <link href="https://ericfu.me/columnar-storage-overview-system/"/>
    <id>https://ericfu.me/columnar-storage-overview-system/</id>
    <published>2018-04-15T15:02:42.000Z</published>
    <updated>2018-06-19T03:15:58.105Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/05/database-pic-banner.jpg" alt="banner"></p><p>在上一篇文章<a href="https://ericfu.me/columnar-storage-overview-storage/">《处理海量数据：列式存储综述（存储篇）》</a> 中，我们介绍了几种 Apache ORC、Dremel 等几种典型列式存储的数据组织格式。实践中，很多数据系统构建在 HDFS 等分布式文件系统之上，使用这些规范的格式保存数据，方便各方的业务进行分析查询。</p><p>本文将介绍若干个典型的列式存储数据库系统。作为完整的 OLAP 或 HTAP 数据库系统，他们大多使用了自主设计的存储方式，运行在多台机器节点上，使用网络进行通讯协作。</p><a id="more"></a><h2 id="C-Store-2005-Vertica"><a href="#C-Store-2005-Vertica" class="headerlink" title="C-Store (2005) / Vertica"></a>C-Store (2005) / Vertica</h2><p>大多数 DBMS 都是为写优化，而 C-Store 是第一个为读优化的 OLTP 数据库系统，虽然从今天的视角看它应当算作 HTAP 。在 ad-hoc 的分析型查询、ORM 的在线查询等场景中，大多数操作都是查询而非写入，在这些场景中列式存储能取得更好的性能。像主流的 DBMS 一样，C-Store 支持标准的关系型模型。</p><p>在<a href="https://ericfu.me/columnar-storage-overview-storage/">上一篇文章</a>中，我们已经介绍了 C-Store 特有的 projection 数据模型。这里只做一个简单的回顾：<strong>每个 projection 可以包含一个或多个列，完整的表视图需要通过若干 projection JOIN 得到</strong>。Projection 水平拆分成若干 segments。</p><p>C-Store 的设计考虑到企业级应用的使用模式，在优化 AP 查询的同时兼顾了大多数 DBMS 具有的 TP 查询功能。在 ACID 事务方面同样提供了完整的支持，支持快照（snapshot）读事务和一般的 2PC 读写事务。</p><blockquote><p>通常而言，互联网应用对 DBMS 有较高的并发写入需求，对一致性读、分析型查询的需求不那么强烈。而企业级应用（例如 CRM 系统）的并发写入需求不大，但需要对一致性读、分析型查询等。</p></blockquote><h3 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h3><p>C-Store 将其物理存储也就是每个 projection 分成两层，<strong>分别是为写优化的 Writeable Store (WS) 和为读优化的 Read-optimized Store (RS)</strong>。RS 即是基线数据，WS 上暂存了对 RS 数据的变更，二者在读取时需要 merge 得到最新的数据。在上一篇文章的 Apache ORC 格式种，我们也看到了类似的做法（基线数据叠加增量数据）。</p><p><img src="/images/2018/05/c-store-ws-rs.png" alt="cstore-architecture"></p><p>RS 是一个为读优化的列式存储。RS 中采用之前提到的 projection 数据模型，对不同的列采用了不同的编码方式，根据它是否是 projection 的排序列、以及该列的取值个数，来决定采取何种编码方式。</p><p>WS 用于暂存高性能的写入操作，例如 INSERT、UPDATE 等。为了简化系统的设计，WS 逻辑上仍然按照 projection 的列式模型存储，但是物理上<strong>使用 B 树以满足快速的写入要求</strong>。WS 基于 BerkeleyDB 构建。</p><p>对于某一列中的某个值 v，会有两个映射关系存在：一是 <code>(storage_key -&gt; v)</code>，在 RS 中 storage_key 就是 segment 中的行号，但在 WS 中显式的记录下来；二是 <code>(sort_key -&gt; storage_key)</code>，用于满足主键查询的需求。</p><p>值得一提的是，WS 是一个 MVCC 的存储——它的每个数据都保存了对应的写入事务编号，同一行可能有多个版本同时存在。而 RS 是没有 MVCC 的，你可以将它看作过去某个时间点的快照。</p><p>Tuple Mover 周期性地将 WS 中的数据移动到 RS 中。与大多数 MVCC 系统一样，C-Store 中的更新是通过一个删除加一个插入实现的，Tuple Mover 的主要工作是根据 WS 的数据更新 RS：删掉被删除的行、添加新的行。</p><h3 id="事务支持"><a href="#事务支持" class="headerlink" title="事务支持"></a>事务支持</h3><p>C-Store 认为大多数事务是只读事务，因此采用了 Snapshot Isolation。C-Store 维护两个全局的时间戳：低水位（Low Water Mark, LWM）和高水位（High Water Mark, HWM），允许用户查询介于二者之间的任意时间戳的 Snapshot。时间戳来自中心化的 Time Authority (TA)。</p><p>LWM 对应 RS 即基线数据的版本。Tuple Mover 会保证任何高于 LWM 的修改都不会被移动到 RS 中，因为一旦移动到 RS 也就失去了多版本。</p><p>HWM 由中心的 TA 维护，时间被分成固定长度的 epoch。当各个节点确认 epoch e 中<strong>开始</strong>的写入事务完成时，就会发送一个 <code>Complete(e)</code> 的消息给 TA，当 TA 收集到所有节点的 <code>Complete(e)</code> 将 HWM 置为 e。换句话说，HWM 以前的事务一定是已经完成提交的。  </p><p>对于读写事务，C-Store 采用了传统的 2PC。</p><h2 id="MonetDB-2012-VectorWise"><a href="#MonetDB-2012-VectorWise" class="headerlink" title="MonetDB (2012) / VectorWise"></a>MonetDB (2012) / VectorWise</h2><p>MonetDB 是一个面向 OLAP 的内存数据。区别于大多数 DBMS 使用的 Valcano 执行模式，MonetDB 使用一种独特的 full materialization 的列式（向量）执行模型，也因此设计了对应的一系列算子以及查询优化器。</p><h3 id="BAT-Algebra"><a href="#BAT-Algebra" class="headerlink" title="BAT Algebra"></a>BAT Algebra</h3><p>MonetDB 独有的列式计算是通过 BAT（Binary Association Table）的运算组成的，<strong>BAT 之间通过算子产生新的 BAT，最终生成查询结果</strong>。每个 BAT 可以简单地理解为一列带有编号的数据 &lt;oid, value&gt;，有些 BAT 来自用户的逻辑表，其他则是运算的结果。每个算子被设计地很紧凑、高效，能充分利用 CPU 流水线的计算能力，这和 CPU 设计的 RISC 思想颇为相似，所以被称为“<strong>数据库查询的 RISC 方案</strong>”。</p><p><img src="/images/2018/05/monetdb-bat-example.png" alt="monetdb-bat-example"></p><p>如上图，对于用户一条 SELECT 查询，MonetDB 先将其分解为多次 BAT 的运算，执行计划中的每一步的输入和输出都是 BAT。图中蓝框中为输入的 BAT，其他则是执行产生的运算结果。</p><p>MonetDB 的设计决定了它的计算过程十分耗费内存。MonetDB 利用操作系统的 Memory Mapped File 进行内存管理，不使用的页面可以被换出内存，为执行查询腾出空间。但显然这并不是一个彻底的解决方案。</p><p>VectorWise 使用类似的向量化执行模型，但它尝试在 full materialization 和 Valcano 模型中间寻求一个平衡——它将整个列划分成较小的 block，对 block 进行上述的 column algebra 计算。</p><h2 id="Apache-Kudu-2015"><a href="#Apache-Kudu-2015" class="headerlink" title="Apache Kudu (2015)"></a>Apache Kudu (2015)</h2><p>Kudu 是 Cloudera 研发的处理实时数据的 OLAP 数据库。上文提到的 Parquet / ORC 是开源界常用的处理静态数据的方式，为什么说是静态数据呢？因为这些紧凑的格式对数据修改很不友好，且随机读写性能极差，通常只能用于后台 OLAP。</p><p>所以我们看到，很多数据系统都采用动态、静态两套数据，例如：把在线业务数据放在 HBase 中，定期通过 ETL 程序产生Parquet 格式文件放到 HDFS 上，再对其进行统计、归档等。这种定期导入的方式不可避免地会带来小时级的延迟，而且，众所周知维护 ETL 代码是一件费时费力的事情。</p><p>Kudu 试图在 OLAP 与 OLTP 之间寻求一个平衡点——<strong>在保持同一份数据的情况下，既能提供在线业务实时写入的能力，又能支持高效的 OLAP 查询。</strong></p><p>Kudu 采用我们熟悉的半关系型模型，允许用户定义 schema，但是目前并不支持二级索引。</p><p>事务方面，Kudu 默认使用 Snapshot Isolation 一致性模型。此外，如果用户需要一个更强的一致性保证（例如 read own’s writes），Kudu 也允许用户指定特定的时间戳，读取这个时间戳的 snapshot。这项功能被集成在 Kudu 的 API 层面，用户可以方便地获得因果（causality）一致性保证。</p><h3 id="系统设计-1"><a href="#系统设计-1" class="headerlink" title="系统设计"></a>系统设计</h3><p>Kudu 采用了类似 HBase 的 master-slave 架构：中心节点被称作 Kudu Master，数据节点被称作 Tablet Server。一个表的数据被分割成多个 tablets，由它们对应的 Tablet Server 来提供数据读写服务。 </p><p>与 HBase 相比，中心节点 Kudu Master 除了存放了 Tablet 的分布信息，还身兼了如下角色：</p><ul><li>Catalog 管理：同步各个库、表的 schema 等元信息、负责协调完成建表等 DDL 操作</li><li>集群协调者：各个 Tablet Server 向其汇报自己的状态、replica 变更等</li></ul><p>Kudu 底层数据文件并没有存储在 HDFS 这样的分布式文件系统上，而是<strong>基于 Raft 算法实现了一套副本同步机制，保障数据不丢失及高可用性</strong>。其中 Raft 算法用于同步数据修改操作的 log，这点和大多数 shared-nothing 架构分布式数据库并无二致。对 Raft 算法有兴趣的同学可以参考<a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">原论文</a>。</p><p>作为列式 OLAP 数据库，Kudu 的磁盘存储是常见的列式方案，很多地方直接复用了 Parquet 的代码。我们知道，紧凑的列式存储难以实现高效的更新操作。Kudu 为了提供实时写入功能，采用了类似 C-Store 中的方案——在不可变的基线数据上，叠加后续的更新数据。</p><p><img src="/images/2018/05/kudu-write-path.png" alt="kudu-write-path"></p><p>具体来说，Tablet 由 RowSet 组成，而 RowSet 既可以是内存中的 MemRowSet，也可以是存储在磁盘上的 DiskRowSet。<strong>一个 RowSet 包含两部分数据：基础数据通常以 DiskRowSet 形式保存在磁盘上；而变更数据先以 MemRowSet 的形式暂存在内存中，后续再异步地刷写到磁盘上。</strong>和 C-Store 类似，内存中的数据使用 B 树存储。</p><p>与 C-Store 不同的是，Delta 数据并不会立即和磁盘上的基线数据进行合并，而是由后台的 compaction 线程异步完成。值得注意的是，为了保证 compaction 操作不影响过去的 snapshot read，被覆盖的旧数据也会以 UNDO 记录的形式保存在另外的文件中。</p><h2 id="PowerDrill-2012"><a href="#PowerDrill-2012" class="headerlink" title="PowerDrill (2012)"></a>PowerDrill (2012)</h2><p>PowerDrill 是 Google 研发用于快速处理 ad-hoc 查询的 OLAP 数据库，为前端的 Web 交互式分析软件提供支持。PowerDrill 的数据放在内存中，为了尽可能节约空间，PowerDrill 引入一种全新的分区的存储格式，在节省内存占用的同时提供了类似索引的功能，能过滤掉无关的分区、避免全表扫描。</p><p>同是 Google 家的产品，和 Dremel 相比，PowerDrill 有以下几点差异：</p><ul><li>定位不同：Dremel 用于查询“大量的大数据集”（数据集的规模都大，数据集很多），PowerDrill 用于查询“少量的大数据集”（数据集的规模大，但数据集不多）</li><li>Dremel 用全表扫描（full scan）处理查询，而 PowerDrill 对数据做了分区，并能根据查询只扫描用到的分区。</li><li>Dremel 使用类似 Protobuf 的嵌套数据模型；PowerDrill 使用关系模型</li><li>Dremel 的数据直接放在分布式文件系统上，而 PowerDrill 需要一个 load 过程将数据载入内存</li></ul><h3 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h3><p>Ad-hoc 查询常常包含 <code>GROUP BY</code> 子句，在这些 group key 上进行分区，能很好的过滤掉不需要的数据。<strong>PowerDrill 需要 DBA 根据自己对数据的理解，选出用于用于分区的一组属性</strong> <code>Key1 Key2 Key3 ...</code>（优先级依次递减）。分区是一个递归的过程：一开始把整个数据集视为一个分区（Chunk），如果 <code>Key1</code> 能将数据分开就用 <code>Key1</code>，否则用 <code>Key2</code>、<code>Key3</code>—……直到分区大小小于一个阈值。</p><p>以下是一个分区的例子，第一次使用 Age 分区、第二次使用 Salary 分区。</p><p><img src="/images/2018/05/powerdrill-partition-example.png" alt="powerdrill-partition-example"></p><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p>PowerDrill 的数据组织以列为单位。对于每个列有一个全局字典表，列的每个分区有一个分区字典表：</p><ul><li><strong>全局字典表</strong>（global dictionary）存储列中所有 distinct 的字符串，按字典顺序排序。字典结构是双向的，既能将 string 映射到 global-id，也能从 global-id 查 string。</li><li><strong>分区字典表</strong>（chunk dictionary）存储一个分区中 chunk-id 到 global-id 的双向映射。相应地，数据列（elements）存储 chunk-id 而不是 global-id。</li></ul><p><img src="/images/2018/05/powerdrill-global-chunk-dict-example.png" alt="powerdrill-global-chunk-dict-example"></p><p>如果要将 chunk 中的一个 element 也就是 chunk-id 还原成数据，<strong>第一步需要查分区字典表，得到 global-id；第二步查全局字典表，得到原本的字符串数据</strong>。以上图举例而言：</p><ol><li>Chunk 0 存储的 chunk-id 数据 <code>[3, 2, 0, ...]</code></li><li>根据分区字典表，查出 global-id：<code>[5, 4, 1, ...]</code></li><li>根据全局字典表，查出 search string: <code>[&#39;ebay&#39;, &#39;cheap flights&#39;, &#39;amazon&#39;, ...]</code></li></ol><p>这样的两层映射保证 chunk-id 尽可能的小，所以可以用更紧凑的编码，比如用 8bit、16bit 整数存储。这不仅能节省空间，也能加快扫描速度。</p><p>此外，相同的数据只会在全局字典表中存一份。而且全局字典表中的字符串数据已经被排序，相比不排序，排序后用 Snappy 等算法的压缩比更高。</p><h3 id="分区索引"><a href="#分区索引" class="headerlink" title="分区索引"></a>分区索引</h3><p>上述的数据结构还有一个额外的好处：<strong>它能快速算出某个分区是否包含有用的数据，帮助执行器跳过无关的分区</strong>。以下面的 SQL 为例（数据参考上一张图）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> search_string, <span class="keyword">COUNT</span>(*) <span class="keyword">as</span> c <span class="keyword">FROM</span> <span class="keyword">data</span></span><br><span class="line"><span class="keyword">WHERE</span> search_string <span class="keyword">IN</span> (<span class="string">"la redoute"</span>, <span class="string">"voyages sncf"</span>)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> search_string</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> c <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>步骤如下：</p><ol><li>在 <code>search_string</code> 列的全局字典表中查找 <code>&quot;[la redoute&quot;, &quot;voyages sncf&quot;]</code>，得到 global-id <code>[9, 11]</code></li><li>在各个分区中查找 global-id <code>[9, 11]</code>： Chunk 0，Chunk 1 中都没有找到，所以可以直接跳过；而 Chunk 2 中出现了 <code>[11]</code>，对应 chunk-id 为 <code>[4]</code></li><li>在 Chunk 2 中的 elements 扫描查出 chunk-id = 4 的元素数量一共有 3 次，作为 <code>COUNT(*)</code> 的结果返回。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了几个知名的列式存储系统。与<a href="https://ericfu.me/columnar-storage-overview-storage/">上一篇文章</a>不同，本文的系统大多重新设计了存储层。与此同时，系统的复杂性也大大提升。</p><p>在构建自己的数据系统时，除了存储方式本身，以下几个地方是着重需要考虑清楚的地方，上述的几个系统也给我们提供了很好的参考：</p><ul><li><strong>系统需要处理的查询是怎样的模式？</strong>C-Store 主要服务于企业级 HTAP 场景，Kudu 在提供 OLAP 查询能力的同时保持了一定的实时写入能力，PowerDrill 着重处理 ad-hoc 的分析型查询。</li><li><strong>系统如何保证数据的持久性和高可用性？</strong>C-Store 在 projection 上保留了一定的冗余，Kudu 用 Raft 协议保持各个副本的数据一致性及可用性，PowerDrill 则直接把数据放在分布式文件系统上，因为不需要对数据作修改。</li><li><strong>系统提供怎样的数据一致性保证？</strong>对于只读的系统来说，这不是个问题。但是一旦支持写入，数据的一致性、事务隔离性都需要精心的考虑和权衡。Kudu 和 C-Store 的 Snapshot Read 实现可作为参考。</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://people.csail.mit.edu/tdanford/6830papers/stonebraker-cstore.pdf" target="_blank" rel="noopener">C-store: a column-oriented DBMS - M Stonebraker, DJ Abadi, A Batkin, X Chen…</a></li><li><a href="http://db.csail.mit.edu/pubs/abadi-column-stores.pdf" target="_blank" rel="noopener">The Design and Implementation of Modern Column-Oriented Database Systems - D Abadi, P Boncz, S Harizopoulos…</a></li><li><a href="https://kudu.apache.org/kudu.pdf" target="_blank" rel="noopener">Kudu: Storage for Fast Analytics on Fast Data - T Lipcon, D Alves…</a></li><li><a href="http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf" target="_blank" rel="noopener">Processing a Trillion Cells per Mouse Click - A Hall, O Bachmann…</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/05/database-pic-banner.jpg&quot; alt=&quot;banner&quot;&gt;&lt;/p&gt;
&lt;p&gt;在上一篇文章&lt;a href=&quot;https://ericfu.me/columnar-storage-overview-storage/&quot;&gt;《处理海量数据：列式存储综述（存储篇）》&lt;/a&gt; 中，我们介绍了几种 Apache ORC、Dremel 等几种典型列式存储的数据组织格式。实践中，很多数据系统构建在 HDFS 等分布式文件系统之上，使用这些规范的格式保存数据，方便各方的业务进行分析查询。&lt;/p&gt;
&lt;p&gt;本文将介绍若干个典型的列式存储数据库系统。作为完整的 OLAP 或 HTAP 数据库系统，他们大多使用了自主设计的存储方式，运行在多台机器节点上，使用网络进行通讯协作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>处理海量数据：列式存储综述（存储篇）</title>
    <link href="https://ericfu.me/columnar-storage-overview-storage/"/>
    <id>https://ericfu.me/columnar-storage-overview-storage/</id>
    <published>2018-04-12T05:17:02.000Z</published>
    <updated>2018-04-26T04:23:25.563Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/04/banner-warehouse.jpg" alt="banner-warehouse"></p><p><strong>列式存储（Column-oriented Storage）</strong>并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。</p><p>总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。</p><a id="more"></a><h2 id="什么是列式存储"><a href="#什么是列式存储" class="headerlink" title="什么是列式存储"></a>什么是列式存储</h2><p>传统 OLTP 数据库通常采用行式存储。以下图为例，所有的列依次排列构成一行，以行为单位存储，再配合以 B+ 树或 SS-Table 作为索引，就能快速通过主键找到相应的行数据。</p><p><img src="/images/2018/04/row-oriented-example.png" alt="Figure 1.1"></p><p>行式存储对于 OLTP 场景是很自然的：大多数操作都以实体（entity）为单位，即大多为<strong>增删改查一整行记录</strong>，显然把一行数据存在物理上相邻的位置是个很好的选择。</p><p>然而，对于 OLAP 场景，一个典型的查询需要遍历整个表，进行分组、排序、聚合等操作，这样一来按行存储的优势就不复存在了。更糟糕的是，分析型 SQL 常常不会用到所有的列，而仅仅对其中某些感兴趣的列做运算，那一行中那些无关的列也不得不参与扫描。</p><p>列式存储就是为这样的需求设计的。如下图所示，同一列的数据被一个接一个紧挨着存放在一起，表的每列构成一个长数组。</p><p><img src="/images/2018/04/column-oriented-example.png" alt="Figure 1.2"></p><p>显然，列式存储对于 OLTP 不友好，一行数据的写入需要同时修改多个列。但对 OLAP 场景有着很大的优势：</p><ul><li>当查询语句只涉及部分列时，只需要扫描相关的列</li><li>每一列的数据都是相同类型的，彼此间相关性更大，对列数据压缩的效率较高</li></ul><blockquote><p><strong>BigTable（HBase）是列式存储吗？</strong></p><p>很多文章将 BigTable 归为列式存储。但严格地说，BigTable 并非列式存储，虽然论文中提到借鉴了 C-Store 等列式存储的某些设计，但 BigTable 本身按 Key-Value Pair 存储数据，和列式存储并无关系。</p><p><img src="/images/2018/04/bigtable-column-families-example.png" alt="Figure 1.3"></p><p>有一点迷惑的是 BigTable 的列簇（column family）概念，列簇可以被指定给某个 locality group，决定了该列簇数据的物理位置，从而可以让同一主键的各个列簇分别存放在最优的物理节点上。由于 column family 内的数据通常具有相似性，对它做压缩要比对整个表压缩效果更好。</p><p>另外，值得强调的一点是：列式数据库可以是关系型、也可以是 NoSQL，这和是否是列式并无关系。本文中讨论的 C-Store 就采用了关系模型。</p></blockquote><h2 id="起源：DSM-分页模式"><a href="#起源：DSM-分页模式" class="headerlink" title="起源：DSM 分页模式"></a>起源：DSM 分页模式</h2><p>我们知道，由于机械磁盘受限于磁头寻址过程，读写通常都以一块（block）为单位，<strong>故在操作系统中被抽象为块设备</strong>，与流设备相对。这能帮助上层应用更好地管理储存空间、增加读写效率等。这一特性直接影响了数据库储存格式的设计：数据库的 Page 对应一个或几个物理扇区，让数据库的 Page 和扇区对齐，提升读写效率。</p><p>那如何将数据存放到页上呢？</p><p>大多数服务于在线查询的 DBMS 采用 NSM (N-ary Storage Model) 即按行存储的方式，将完整的行（即关系 relation）从 Header 开始依次存放。页的最后有一个索引，存放了页内各行的起始偏移量。由于每行长度不一定是固定的，索引可以帮助我们快速找到需要的行，而无需逐个扫描。</p><p>NSM 的缺点在于，如果每次查询只涉及很小的一部分列，那多余的列依然要占用掉宝贵的内存以及 CPU Cache，从而导致更多的 IO；为了避免这一问题，很多分析型数据库采用 DSM (Decomposition Storage Model) 即按列分页：将 relation 按列拆分成多个 sub-relation。类似的，页的尾部存放了一个索引。</p><p><img src="/images/2018/04/nsm-dsm-pax-comparation.png" alt="nsm-dsm-pax-comparation"></p><p>顺便一提，2001 年 Ailamaki 等人提出 PAX (Partition Attributes Cross) 格式，尝试将 DSM 的一些优点引入 NSM，将两者的优点相结合。具体来说，NSM 能更快速的取出一行记录，这是因为一行的数据相邻保存在同一页；DSM 能更好的利用 CPU Cache 以及使用更紧凑的压缩。PAX 的做法是将一个页划分成多个 minipage，minipage 内按列存储，而一页中的各个 minipage 能组合成完整的若干 relation。</p><p>如今，随着分布式文件系统的普及和磁盘性能的提高，<strong>很多先进的 DBMS 已经抛弃了按页存储的模式</strong>，但是其中的某些思想，例如<strong>数据分区、分区内索引、行列混合</strong>等，仍然处处可见于这些现代的系统中。</p><blockquote><p>分布式储存系统虽然不再有页的概念，但是仍然会将文件切割成分块进行储存，但分块的粒度要远远大于一般扇区的大小（如 HDFS 的 Block Size 一般是 128MB）。更大的读写粒度是为了适应网络 IO 更低的带宽以获得更大的吞吐量，但另一方面也牺牲了细粒度随机读写。</p></blockquote><h2 id="列数据的编码与压缩"><a href="#列数据的编码与压缩" class="headerlink" title="列数据的编码与压缩"></a>列数据的编码与压缩</h2><p>无论对于磁盘还是内存数据库，IO 相对于 CPU 通常都是系统的性能瓶颈，<strong>合理的压缩手段不仅能节省空间，也能减少 IO 提高读取性能</strong>。列式存储在数据编码和压缩上具有天然的优势。</p><p>以下介绍的是 C-Store 中的数据编码方式，具有一定的代表性。根据 1) 数据本身是否按顺序排列（self-order） 2) 数据有多少不同的取值（distinct values），分成以下 4 种情况讨论：</p><ul><li><p><strong>有序且 distict 值不多</strong>。使用一系列的三元组 $(v, f, n)$ 对列数据编码，表示数值 $v$ 从第 $f$ 行出现，一共有 $n$ 个（即 $f$ 到 $f+n-1$ 行）。例如：数值 4 出现在 12-18 行，则编码为 $(4, 12, 7)$。</p></li><li><p><strong>无序且 distict 值不多</strong>。对于每个取值 $v$ 构造一个二进制串 $b$，表示 $v$ 所在位置的 bitmap。例如：如果一列的数据是 $0,0,1,1,2,1,0,2,1$，则编码为 <code>(0, 110000100)</code>、<code>(1, 001101001)</code> 和 <code>(2,000010010)</code>。由于 bitmap 是稀疏的，可以对其再进行行程编码。</p></li><li><p><strong>有序且 distict 值多</strong>。对于这种情况，把每个数值表示为前一个数值加上一个变化量（delta），当然第一个数值除外。例如，对于一列数据 $1,4,7,7,8,12$，可以表示为序列 $1,3,3,0,1,4$。显然编码后的数据更容易被 densepack，且压缩比更高。</p></li><li><p><strong>无序且 distict 值多</strong>。对于这种情况没有很好的编码方式。</p></li></ul><p>编码之后，还可以对数据进行压缩。由于一列的数据本身具有相似性，即使不做特殊编码，也能取得相对较好的压缩效果。通常采用 Snappy 等支持流式处理、吞吐量高的压缩算法。</p><p>最后，编码和压缩不仅是节约空间的手段，更多时候也是组织数据的手段。在 PowerDrill、Dremel 等系统中，我们会看到<strong>很多编码本身也兼具了索引的功能</strong>，例如在扫描中跳过不需要的分区，甚至完全改表查询执行的方式。</p><h2 id="列式存储与分布式文件系统"><a href="#列式存储与分布式文件系统" class="headerlink" title="列式存储与分布式文件系统"></a>列式存储与分布式文件系统</h2><p>在现代的大数据架构中，GFS、HDFS 等分布式文件系统已经成为存放大规模数据集的主流方式。分布式文件系统相比单机上的磁盘，具备多副本高可用、容量大、成本低等诸多优势，但也带来了一些单机架构所没有的问题：</p><ol><li>读写均要经过网络，吞吐量可以追平甚至超过硬盘，但是<strong>延迟要比硬盘大得多</strong>，且受网络环境影响很大。</li><li>可以进行大吞吐量的顺序读写，但随机访问性能很差，大多<strong>不支持随机写入</strong>。为了抵消网络的 overhead，通常写入都以几十 MB 为单位。</li></ol><p>上述缺点对于重度依赖随机读写的 OLTP 场景来说是致命的。所以我们看到，很多定位于 OLAP 的列式存储选择放弃 OLTP 能力，从而能构建在分布式文件系统之上。</p><p>要想将分布式文件系统的性能发挥到极致，无非有几种方法：<strong>按块（分片）读取数据、流式读取、追加写入等</strong>。我们在后面会看到一些开源界流行的列式存储模型，将这些优化方法体现在存储格式的设计中。</p><h2 id="列式存储系统案例"><a href="#列式存储系统案例" class="headerlink" title="列式存储系统案例"></a>列式存储系统案例</h2><h3 id="C-Store-2005-Vertica"><a href="#C-Store-2005-Vertica" class="headerlink" title="C-Store (2005) / Vertica"></a>C-Store (2005) / Vertica</h3><p>大多数 DBMS 都是为写优化，而 C-Store 是第一个为读优化的 OLTP 数据库系统，虽然从今天的视角看它应当算作 HTAP 。在 ad-hoc 的分析型查询、ORM 的在线查询等场景中，大多数操作都是查询而非写入，在这些场景中列式存储能取得更好的性能。像主流的 DBMS 一样，C-Store 支持标准的关系型模型。</p><p>就像本文开头即提到——列式存储不是新鲜事。C-Store 的主要贡献有以下几点：<strong>通过精心设计的 projection 同时实现列数据的多副本和多种索引方式；用读写分层的方式兼顾了（少量）写入的性能</strong>。此外，C-Store 可能是第一个现代的列式存储数据库实现，其的设计启发了无数后来的商业或开源数据库，就比如 <a href="https://www.vertica.com/" target="_blank" rel="noopener">Vertica</a>。</p><h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>C-Store 是关系型数据库，它的逻辑表和其他数据库中的并没有什么不同。但是在 C-Store 内部，逻辑表被纵向拆分成 projections，每个 projection 可以包含一个或多个列，甚至可以包含来自其他逻辑表的列（构成索引）。当然，每个列至少会存在于一个 projections 上。</p><p>下图的例子中，EMP 表被存储为 3 个 projections，DEPT 被存储为 1 个 projection。每个 projection 按照各自的 sort key 排序，在图中用下划线表示 sort key。</p><p><img src="/images/2018/04/c-store-projections-example.png" alt="cstore-architecture"></p><p>Projection 内是以列式存储的：里面的每个列分别用一个数据结构存放。为了避免列太长引起问题，也支持每个 projection 以 sort key 的值做横向切分。</p><p>查询时 C-Store 会先选择一组能覆盖结果中所有列的 projections 集合作为 covering set，然后进行 join 计算重构出原来的行。为了能高效地进行 projections 的 join（即按照另一个 key 重新排序），引入 join index 作为辅助，其中存储了 proj1 到 proj2 的下标映射关系。</p><p><strong>Projection 是有冗余性的</strong>，常常 1 个列会出现在多个 projection 中，但是它们的顺序也就是 sort key 并不相同，因此 <strong>C-Store 在查询时可以选用最优的一组 projections</strong>，使得查询执行的代价最小。</p><p>巧妙的是，<strong>C-Store 的 projection 冗余性还用来实现 K-safe 高可用</strong>（容忍最多 K 台机器故障），当部分节点宕机时，只要 C-Store 还能找到某个 covering set 就能执行查询，虽然不一定是最优的 covering set 组合。</p><blockquote><p>从另一个角度看，C-Store 的 Projection 可以看作是一种物化（materialized）的查询结果，即查询结果在查询执行前已经被预先计算好；并且由于每个列至少出现在一个 Projection 当中，没有必要再保存原来的逻辑表。</p><p>为任意查询预先计算好结果显然不现实，但是如果物化某些经常用到的中间视图，就能在预计算代价和查询代价之间获得一个平衡。C-Store 物化的正是以某个 sort key 排好序（甚至 JOIN 了其他表）的一组列数据，同时预计算的还有 join index。</p></blockquote><p>C-Store 对写入的处理将在下一篇文章中呈现。</p><h3 id="Apache-ORC"><a href="#Apache-ORC" class="headerlink" title="Apache ORC"></a>Apache ORC</h3><p>Apache ORC 最初是为支持 Hive 上的 OLAP 查询开发的一种文件格式，如今在 Hadoop 生态系统中有广泛的应用。ORC 支持各种格式的字段，包括常见的 int、string 等，也包括 struct、list、map 等组合字段；字段的 meta 信息就放在 ORC 文件的尾部（这被称为自描述的）。</p><h4 id="数据结构及索引"><a href="#数据结构及索引" class="headerlink" title="数据结构及索引"></a>数据结构及索引</h4><p><strong>为分区构造索引是一种常见的优化方案</strong>，ORC 的数据结构分成以下 3 个层级，在每个层级上都有索引信息来加速查询。</p><p><img src="/images/2018/04/orc-file-structure.png" alt="orc-file-structure"></p><ul><li><strong>File Level</strong>：即一个 ORC 文件，Footer 中保存了数据的 meta 信息，还有文件数据的索引信息，例如各列数据的最大最小值（范围）、NULL 值分布、布隆过滤器等，这些信息可用来<strong>快速确定该文件是否包含要查询的数据</strong>。每个 ORC 文件中包含多个 Stripe。</li><li><strong>Stripe Level</strong> 对应原表的一个范围分区，里面包含该分区内各列的值。每个 Stripe 也有自己的一个索引放在 footer 里，和 file-level 索引类似。</li><li><strong>Row-Group Level</strong> ：一列中的每 10000 行数据构成一个 row-group，每个 row-group 拥有自己的 row-level 索引，信息同上。</li></ul><p>ORC 里的 Stripe 就像传统数据库的页，它是 ORC 文件批量读写的基本单位。这是由于分布式储存系统的读写延迟较大，一次 IO 操作只有批量读取一定量的数据才划算。这和按页读写磁盘的思路也有共通之处。</p><blockquote><p>像其他很多储存格式一样，ORC 选择将统计数据和 Metadata 放在 File 和 Stripe 的尾部而不是头部。</p><p>但 ORC 在 Stripe 的读写上还有一点优化，那就是把分区粒度小于 Stripe 的结构（如 Column 和 Row-Group）的索引统一抽取出来放到 Stripe 的头部。这是因为在批处理计算中一般是把整个 Stripe 读入批量处理的，将这些索引抽取出来可以减少在批处理场景下需要的 IO（批处理读取可以跳过这一部分）。</p></blockquote><h4 id="ACID-支持"><a href="#ACID-支持" class="headerlink" title="ACID 支持"></a>ACID 支持</h4><p>Apache ORC 提供有限的 ACID 事务支持。受限于分布式文件系统的特点，文件不能随机写，那如何把修改保存下来呢？</p><p>类似于 LSM-Tree 中的 MVCC 那样，writer 并不是直接修改数据，而是为每个事务生成一个 delta 文件，文件中的修改被叠加在原始数据之上。当 delta 文件越来越多时，通过 minor compaction 把连续多个 delta 文件合成一个；当 delta 变得很大时，再执行 major compaction 将 delta 和原始数据合并。</p><p><strong>这种保持基线数据不变、分层叠加 delta 数据的优化方式在列式存储系统中十分常见，是一种通用的解决思路</strong>。</p><blockquote><p>别忘了 ORC 的 delta 文件也是写入到分布式储存中的，因此每个 Delta 文件的内容不宜过短。这也解释了 ORC 文件虽然支持事务，但是主要是对批量写入的事务比较友好，不适合频繁且细小的写入事务的原因。</p></blockquote><h3 id="Dremel-2010-Apache-Parquet"><a href="#Dremel-2010-Apache-Parquet" class="headerlink" title="Dremel (2010) / Apache Parquet"></a>Dremel (2010) / Apache Parquet</h3><p>Dremel 是 Google 研发的用于大规模只读数据的查询系统，用于进行快速的 ad-hoc 查询，弥补 MapReduce 交互式查询能力的不足。为了避免对数据的二次拷贝，Dremel 的数据就放在原处，通常是 GFS 这样的分布式文件系统，为此需要设计一种通用的文件格式。</p><p>Dremel 的系统设计和大多 OLAP 的列式数据库并无太多创新点，但是其精巧的存储格式却变得流行起来，Apache Parquet 就是它的开源复刻版。注意 Parquet 和 ORC 一样都是一种存储格式，而非完整的系统。</p><h4 id="嵌套数据模型"><a href="#嵌套数据模型" class="headerlink" title="嵌套数据模型"></a>嵌套数据模型</h4><p>Google 内部大量使用 Protobuf 作为跨平台、跨语言的数据序列化格式，相比 JSON 要更紧凑并具有更强的表达能力。Protobuf 不仅允许用户定义必须（required）和可选（optinal）字段，<strong>还允许用户定义 repeated 字段，意味着该字段可以出现 0～N 次，类似变长数组</strong>。</p><p>Dremel 格式的设计目的就是按列来存储 Protobuf 的数据。由于 repeated 字段的存在，这要比按列存储关系型的数据困难一些。一般的思路可能是用终止符表示每个 repeat 结束，<strong>但是考虑到数据可能很稀疏</strong>，Dremel 引入了一种更为紧凑的格式。</p><p>作为例子，下图左半边展示了数据的 schema 和 2 个 Document 的实例，右半边是序列化之后的各个列。序列化之后的列多出了 R、D 两列，分别代表 Repetition Level 和 Definition Level，<strong>通过这两个值就能确保唯一地反序列化出原本的数据</strong>。</p><p><img src="/images/2018/04/google-dremel-example.png" alt="google-dremel-example"></p><p><strong>Repetition Level</strong> 表示当前值在哪一个级别上重复。对于非 repeated 字段只要填上 trivial 值 0 即可；否则，只要这个字段可能出现重复（无论本身是 repeated 还是外层结构是 repeated），应当为 R 填上当前值在哪一层上 repeat。</p><p>举个例子说明：对于 Name.Language.Code 我们一共有三条非 NULL 的记录。</p><ol><li>第一个是 <code>en-us</code>，出现在第一个 Name 的第一个 Lanuage 的第一个 Code 里面。在此之前，这三个元素是没有重复过的，都是第一次出现。所以其 R=0</li><li>第二个是 <code>en</code>，出现在下一个 Language 里面。也就是说 Language 是重复的元素。Name.Language.Code 中Language 排第二个，所以其 R=2</li><li>第三个是 <code>en-gb</code>，出现在下一个 Name 中，Name 是重复元素，排第一个，所以其 R=1</li></ol><p>注意到 <code>en-gb</code> 是属于第3个 Name 的而非第2个Name，为了表达这个事实，我们在 <code>en</code> 和 <code>en-gb</code>中间放了一个 R=1 的 NULL。</p><p><strong>Definition Level</strong> 是为了说明 NULL 被定义在哪一层，也就宣告那一层的 repeat 到此为止。对于非 NULL 字段只要填上 trivial 值，即数据本身所在的 level 即可。</p><p>同样举个例子，对于 Name.Language.Country 列</p><ol><li><code>us</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R1 内部，表示当前 Name 之内、后续所有 Language 都不含有 Country 字段。所以D为2。</li><li><code>NULL</code> 在 R1 内部，表示当前 Document 之内、后续所有 Name 都不含有 Country 字段。所以D为1。</li><li><code>gb</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R2 内部，表示后续所有 Document 都不含有 Country 字段。所以D为0。</li></ol><p>可以证明，结合 R、D 两个数值一定能唯一构建出原始数据。<strong>为了高效编解码，Dremel 在执行时首先构建出状态机，之后利用状态机处理列数据</strong>。不仅如此，状态机还会结合查询需求和数据的 structure 直接跳过无关的数据。</p><blockquote><p>状态机实现可以说是 Dremel 论文的最大贡献。但是受限于篇幅，有兴趣的同学请参考原论文。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了列式存储的存储结构设计。抛开种种繁复的细节，我们看到，以下这些思想或设计是具有共性的。</p><ol><li><strong>跳过无关的数据</strong>。从行存到列存，就是消除了无关列的扫描；ORC 中通过三层索引信息，能快速跳过无关的数据分片。</li><li><strong>编码既是压缩，也是索引</strong>。Dremel 中用精巧的嵌套编码避免了大量 NULL 的出现；C-Store 对 distinct 值的编码同时也是对 distinct 值的索引；PowerDrill 则将字典编码用到了极致（见下一篇文章）。</li><li><strong>假设数据不可变</strong>。无论 C-Store、Dremel 还是 ORC，它们的编码和压缩方式都完全不考虑数据更新。如果一定要有更新，暂时写到别处、读时合并即可。</li><li><strong>数据分片</strong>。处理大规模数据，既要纵向切分也要横向切分，不必多说。</li></ol><p>下一篇文章中，将会结合 C-Store、MonetDB、Apache Kudu、PowerDrill 等现代列式数据库系统，侧重描述列式 DBMS 的整体架构设计以及独特的查询执行过程。<strong>敬请期待！</strong></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://dbmsmusings.blogspot.jp/2010/03/distinguishing-two-major-types-of_29.html" target="_blank" rel="noopener">Distinguishing Two Major Types of Column-Stores - Daniel Abadi</a></li><li><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html" target="_blank" rel="noopener">Columnar Storage - Amazon Redshift</a></li><li><a href="http://www.vldb.org/conf/2001/P169.pdf" target="_blank" rel="noopener">Weaving Relations for Cache Performance - A Ailamaki, DJ DeWitt, MD Hill, M Skounakis</a></li><li><a href="http://glinden.blogspot.jp/2006/05/c-store-and-google-bigtable.html" target="_blank" rel="noopener">C-Store and Google BigTable - Greg Linden</a></li><li><a href="http://db.csail.mit.edu/pubs/abadi-column-stores.pdf" target="_blank" rel="noopener">The Design and Implementation of Modern Column-Oriented Database Systems - D Abadi, P Boncz, S Harizopoulos…</a></li><li><a href="http://people.csail.mit.edu/tdanford/6830papers/stonebraker-cstore.pdf" target="_blank" rel="noopener">C-store: a column-oriented DBMS - M Stonebraker, DJ Abadi, A Batkin, X Chen…</a></li><li><a href="https://orc.apache.org/docs/" target="_blank" rel="noopener">Apache ORC Docs</a></li><li><a href="https://research.google.com/pubs/archive/36632.pdf" target="_blank" rel="noopener">Dremel: Interactive Analysis of Web-Scale Datasets - S Melnik, A Gubarev, JJ Long, G Romer…</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/04/banner-warehouse.jpg&quot; alt=&quot;banner-warehouse&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列式存储（Column-oriented Storage）&lt;/strong&gt;并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。&lt;/p&gt;
&lt;p&gt;总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Let&#39;s Encrypt 免费的通配符域名证书</title>
    <link href="https://ericfu.me/lets-encrypt-wildcard-cert/"/>
    <id>https://ericfu.me/lets-encrypt-wildcard-cert/</id>
    <published>2018-03-15T06:15:14.000Z</published>
    <updated>2018-03-16T16:31:48.673Z</updated>
    
    <content type="html"><![CDATA[<p>免费 SSL 证书提供商 Let’s Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日<a href="https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579" target="_blank" rel="noopener">正式发布</a>了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。</p><p>ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。</p><a id="more"></a><p>截至目前为止，官方推荐的 ACME 客户端 <a href="https://github.com/certbot/certbot" target="_blank" rel="noopener">Certbot</a> 还没有支持 ACME v2，这里推荐另一个小巧的客户端 <a href="https://github.com/Neilpang/acme.sh" target="_blank" rel="noopener">acme.sh</a>，它完全用 shell 脚本构成，快速安装方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://get.acme.sh | sh</span><br></pre></td></tr></table></figure><p>为了使用 DNS-01 challenge，先在 DNS 服务商那里启用 Token 访问。这里以 DNSPod 为例，其它服务商参见<a href="https://github.com/Neilpang/acme.sh/blob/master/dnsapi/README.md" target="_blank" rel="noopener">完整支持列表</a>。</p><p><img src="/images/2018/03/dnspod-api-token.png" alt="dnspod-api-token"></p><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> DP_Id=<span class="string">"49644"</span></span><br><span class="line"><span class="built_in">export</span> DP_Key=<span class="string">"******************************"</span></span><br><span class="line">acme.sh --issue --dns dns_dp -d example.com -d *.example.com</span><br></pre></td></tr></table></figure><p>如果一切顺利，可以看到证书已经生成了。如果要安装到 nginx 中，还需要转换成 PEM 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">acme.sh --install-cert -d example.com -d *.example.com \</span><br><span class="line">        --key-file       /etc/nginx/certificates/example.com/key.pem  \</span><br><span class="line">        --fullchain-file /etc/nginx/certificates/example.com/cert.pem</span><br></pre></td></tr></table></figure><p>最后把 nginx 配置从 HTTP 修改成 HTTPS：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name www.example.com example.com;</span><br><span class="line"></span><br><span class="line">    ssl_certificate /etc/nginx/certificates/example.com/cert.pem;</span><br><span class="line">    ssl_certificate_key /etc/nginx/certificates/example.com/key.pem;</span><br><span class="line"></span><br><span class="line">    # other configurations ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 nginx，大功告成。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;免费 SSL 证书提供商 Let’s Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日&lt;a href=&quot;https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;正式发布&lt;/a&gt;了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。&lt;/p&gt;
&lt;p&gt;ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Google Spanner 事务在存储层的实现</title>
    <link href="https://ericfu.me/google-spanner-storage-impl/"/>
    <id>https://ericfu.me/google-spanner-storage-impl/</id>
    <published>2018-03-07T18:05:56.000Z</published>
    <updated>2018-04-13T02:10:32.065Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/03/banner-google-spanner.jpg" alt="banner-google-spanne"></p><p>Google Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。</p><p>不过，<strong>论文对存储层实现只作了模糊的阐述</strong>：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：</p><ul><li><strong>read-only 事务</strong>: 读取最新或给定时间戳 $t_{read}$ 的快照，也就是 snapshot read</li><li><strong>read-write 事务</strong>：读取事务开始时间戳 $t_{start}$ 的快照，而写入操作在提交时间戳 $t_{commit}$ 生效</li></ul><p>本文从 Spanner 本身设计出发，并结合开源实现 TiDB 和 CockroachDB，谈谈如何为 Spanner 设计一个存储层。本文假设读者阅读过原论文 <a href="https://research.google.com/archive/spanner.html" target="_blank" rel="noopener">Spanner: Google’s Globally-Distributed Database</a>。</p><a id="more"></a><h2 id="数据的-KV-表示"><a href="#数据的-KV-表示" class="headerlink" title="数据的 KV 表示"></a>数据的 KV 表示</h2><p>Spanner 对外提供（半）关系型数据模型：每张表定义了一个或多个主键列，以及其他的非主键列。这和我们熟知的 SQL 关系型模型几乎一摸一样，唯一的不同是 schema 定义中必须含有主键。</p><p>Spanner 早期的设计中大量复用了 BigTable（开源实现即 HBase）的代码。回忆一下 BigTable 的数据模型：每一条数据包含 <code>(Key, Column, Timestamp)</code> 三个维度，满足我们需要的 MVCC 特性。从 BigTable 开始的确是个不错的选择。</p><p>不过，从性能上考虑 Bigtable 毕竟是分布式的 KV 存储系统，在存储这一层我们大可不用搞的那么复杂，分布式的问题例如 scale-out 和 replication 应当留给上层的 sharding 机制和 Paxos 解决。事实上，一个单机的存储引擎足矣。</p><p>Google 自家的 LSM-Tree + SSTable 的实现 LevelDB 是个可选项。它接口非常简单，是一个标准的 KV 存储，可以方便的在它基础上实现我们想要的数据模型。主要接口其实就是两个：</p><ul><li><code>Write(WriteBatch *)</code> 原子地写入一个 WriteBatch，包含一组 <code>Put(K, V)</code> 和 <code>Delete(K)</code> 操作</li><li><code>Iterator()</code> 及 <code>Seek()</code> 从指定位置开始顺序扫描读取 (K, V) 数据</li></ul><p>如何实现列和时间戳呢？举个例子，有如下数据表 <code>Accounts</code>。在数据库中，主键索引通常也是唯一的聚簇索引，它存放了真实的数据，而我们暂时不考虑其他索引。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| UserID (PK) | Balance | LastModified |</span><br><span class="line">|-------------|---------|--------------|</span><br><span class="line">| Alice       | 20      | 2018-02-20   |</span><br><span class="line">| Bob         | 10      | 2018-02-01   |</span><br></pre></td></tr></table></figure><p>Spanner 内部使用 MVCC 机制，所以还有一个隐藏的时间戳维度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| UserID | Timestamp | Balance | LastModified |</span><br><span class="line">|--------|-----------|---------|--------------|</span><br><span class="line">| Alice  | 103       | 20      | 2018-02-20   |</span><br><span class="line">| Alice  | 101       | 15      | 2018-01-20   |</span><br><span class="line">| Bob    | 102       | 10      | 2018-02-01   |</span><br></pre></td></tr></table></figure><p>上述数据表用 KV 模型存储，可以表示为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">| Key                             | Value      |</span><br><span class="line">|---------------------------------|------------|</span><br><span class="line">| Accounts/Alice/Balance/103      | 20         |</span><br><span class="line">| Accounts/Alice/Balance/101      | 15         |</span><br><span class="line">| Accounts/Alice/LastModified/103 | 2018-02-20 |</span><br><span class="line">| Accounts/Alice/LastModified/101 | 2018-01-20 |</span><br><span class="line">| Accounts/Bob/Balance/102        | 10         |</span><br><span class="line">| Accounts/Bob/LastModified/102   | 2018-02-01 |</span><br></pre></td></tr></table></figure><p>上表中 <code>/</code> 表示一个分隔符，真实情况要更复杂。Key 这样编码：从左到右依次是表名（因为可以有不止一张表）、主键字段、列的标识符、时间戳（通常倒序排列，Tips. 取反即可）。Value 则对应原表中的数据。</p><p>显然，对于半关系型数据一定能由表名、主键字段、列名唯一地确定一个值，所以这个编码方式能满足我们的要求。</p><blockquote><p>如果一张表只有主键怎么办呢？这种情况下可以为每个主键填充一个 placeholder 的 value 即可。</p></blockquote><h2 id="事务的原子性"><a href="#事务的原子性" class="headerlink" title="事务的原子性"></a>事务的原子性</h2><p>众所周知，事务具有四个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。其中一致性和持久性其实是数据库系统的特性，对于事务，我们更多讨论的是<strong>原子性</strong>和<strong>隔离性</strong>。</p><p>对于存储层而言，为上层提供原子性 commit 的接口是必须的功能。如何在 KV 存储的基础上实现原子性呢？以下思路是一种常见的方案：</p><ol><li>首先，准备一个<strong>开关</strong>，初始状态为 off，当我们把开关打开的那一刻，意味着 commit 生效可见；</li><li>将所有变更以一种可回滚的方式（e.g. 不能覆盖现有的值）写入存储中。开关同时决定了其它 reader 的视图，由于开关还是 off 状态，现在写入的变更不会被其它事务看到。</li><li>之后，写入开关状态为 on，标志着 commit 的成功，新数据生效，即所谓 commit point。这个写入操作本身的原子性由 LevelDB 保证。</li><li>最后，清除掉中间状态（比如第 2 步中的临时数据）并写入最终的数据。这一步可以异步的完成，因为在第 3 步中事实上 commit 已经成功了，无需等待。</li></ol><p><img src="/images/2018/03/commit-4-steps.svg" alt="commit-4-steps"></p><p>保证原子性的关键在于 commit point。例如，在单机数据库中，commit point 是 commit 的 redo-log 写入磁盘的一瞬间；在 XA 两阶段提交中，commit point 是协调器将事务状态置为 Committed 的一瞬间。</p><p>在我们的存储中，commit point 也就是第 3 步的写入操作。如果提交过程意外终止在 commit point 之前，我们会在读取时发现第 2 步中的临时写入，然后轻松地清除它；如果意外终止在 commit point 之后，部分临时状态没有被清除，只需继续执行 4 即可。</p><p>上述只是一个解决问题的思路。具体的解决方案可以参考 Percolator 的事务实现。这同时也是 TiDB 的做法，CockroachDB 做法略有不同，但同样遵从这个模式。</p><h3 id="Percolator-事务方案"><a href="#Percolator-事务方案" class="headerlink" title="Percolator 事务方案"></a>Percolator 事务方案</h3><p>Percolator 是 Google 早期的分布式事务解决方案，用于进行大规模增量数据处理。Percolator 在 BigTable 基础上基于 2PL 思想实现了分布式事务。这个算法很简单，你可以把它看作是是封装了一系列 BigTable 的 API 访问（本身无状态），所以可以容易地移植到 KV 存储模型上。</p><p>Percolator 事务模型基于单调递增的时间戳，来源于集群中唯一的 timestamp oracle。每个事务拥有提交时间戳 $t_{commit}$ 和开始时间戳 $t_{start}$。Percolator 事务模型和之前说到的 write-read 事务一致：事务中总是读取 $t_{start}$ 时的 snapshot，而写入则全部在 $t_{commit}$ 生效。这也意味着事务中所有写入都被 buffer 到最后进行，不支持类似于 read-write-read 这样的模式。</p><p><img src="/images/2018/03/percolator-transaction-model-read-write.jpg" alt=""></p><p><em>如图，事务 2 看到的是事务 1 提交前的状态，而事务 3 看到的是事务 1、2 提交后的状态。</em></p><p>Percolator 基于 BigTable 的事务实现如下：</p><p>除了数据本身（bal:data 列）以外，我们给数据再加上两列：lock 和 write。</p><ul><li>write 列存放了一个<em>指针</em>，指向写入的 data 的时间戳</li><li>lock 列用于 2PL，加锁时也保存了 primary lock 的位置。</li></ul><p>primary lock 不仅代表当前行的锁状态，还兼任上文中“开关”的作用。通常选取第一个写入的数据作为 primary lock。</p><p>以下表为例。表中 <code>6: data @ 5</code> 表示：$ts=6$ 时事务提交，确定了 <code>Bob</code> 对应的值是 <code>5: $10</code>（所以推测出该事务 $t_{start}=5$）。其他事务读取时，为了避免读到 uncommitted 的数据，都会先从 write 列开始找，然后再读出其指向的 data。</p><p><img src="/images/2018/03/percolator-commit-1.jpg" alt=""></p><p>现在，用户要从 Bob 账户里转 \$7 给 Joe，为此必须开启一个事务。$ts = 7$ 时，转账事务开始，向 Bob 和 Joe 的 data 写入新的余额。</p><p>$ts = 8$ 时，用户 commit 事务。事务的第一阶段（Prewrite）亦即是 2PL 的加锁阶段，先为 Bob 和 Joe 都加上锁。如下图所示，lock 不为空即代表加上了锁，其内容指向 primary lock 的位置。简单起见，不妨设第一条被锁的数据为 primary row。</p><p><img src="/images/2018/03/percolator-commit-2.jpg" alt=""></p><p>下一步很关键：<strong>清除 primary row 的 lock 并向 write 列写入新 data 的位置</strong>。这也就是所谓 commit point，这个写入的成功或失败决定了事务提交成功与否：</p><ul><li>若写入成功，则代表整个事务成功。之后会遍历所有加锁的行，解除 lock 并向 write 列写入新的 data 位置。这样一来，其他事务就能读到当前事务写入的数据。</li><li>否则，整个事务失败。之后会遍历所有加锁的行，解除 lock 并清除之前写入的 data，恢复原状。</li></ul><p>回到例子中，当 commit point 完成后，表的状态如下：</p><p><img src="/images/2018/03/percolator-commit-3.jpg" alt=""></p><p>解除 Joe 的 lock 并向 write 列写入新 data 的位置，至此事务 commit 完成：</p><p><img src="/images/2018/03/percolator-commit-4.jpg" alt=""></p><p>Commit point 这一步本身的原子性由 BigTable 行事务保证。对于 commit point 前后的其他操作，如果系统当机重启，恢复线程可以通过检查 commit point 操作的结果，来确定该 roll forward 还是 roll back。具体而言：</p><ul><li>通过 lock 找到 primary lock，如果已经解除，说明 commit point 已经完成，需要 roll forward 事务。</li><li>否则，如果 primary lock 还在，说明 commit point 还没到，只能 roll back 事务。</li></ul><p>于是，通过 2PL，我们成功地在 BigTable 的行级事务基础上实现了表级事务。</p><p>上述过程很容易的能映射到 KV 存储模型上。按照前一节描述的方法，将 lock 和 write 列都视作普通的列即可。这里不再赘述。</p><h2 id="事务的隔离性"><a href="#事务的隔离性" class="headerlink" title="事务的隔离性"></a>事务的隔离性</h2><p>上述的讨论只考虑了单个事务的原子性保证——如何<strong>确保</strong>能从从中间状态恢复到未提交或已提交的状态，而没有考虑多线程并发的情况。如果同时有多个 client 在运行多个事务，如何保证严格互相隔离？（Serializable级别）</p><p>Percolator 是一个典型的 Snapshot Isolation 实现。Percolator 包含一个被称为 Strict-SI 的改进：在事务 commit 中，如果发现有一个高于 $t_{start}$ 的版本出现，则放弃 commit。这能避免 lost update 问题。但是 write-skew 问题依然存在。</p><p>Spanner 提供 Serializable 隔离性保证，其算法被称为 Serializable Snapshot Isolation (SSI)。</p><h3 id="冲突图理论"><a href="#冲突图理论" class="headerlink" title="冲突图理论"></a>冲突图理论</h3><p>首先对以上问题建模。考虑两个事务对同一条数据先后发生两次读或写操作，于是有 4 种情况：</p><ul><li>Read-Read：这是OK的，它不会引起冲突；</li><li>Read-Write：后发生的操作覆盖了前一个读的数据，这是一种冲突；</li><li>Write-Read：读到另一个事务的写入，这是一种冲突。</li><li>Write-Write：即覆盖写，这是一种冲突。</li></ul><p>上述三种冲突的情况，并不是一定会导致问题。举个例子：事务$T_2$仅仅是覆盖了事务$T_1$写入的数据，那么$T_1$和$T_2$仍然是符合 serializable 的，只要逻辑上认为$T_2$发生在$T_1$之后。</p><p>哪些情况会违反 serializable 呢？简单来说，如果冲突A迫使我们规定 $T_1$ 先于 $T_2$，冲突B迫使我们规定 $T_2$ 先于 $T_1$，这个因果关系就没法成立了，$T_1$、$T_2$无法以任何方式串行化。形式化的说：<strong>以所有事务 $T$ 作为节点、以所有冲突 $C$ 作为有向边构成一张有向图（这被称为冲突图或依赖图），如果这张图是有向无环图（DAG）则满足 serializable；否则（有环）不满足</strong>。</p><p>举个例子：</p><p>这是一个有向无环图，$T_1$、$T_2$、$T_3$ 满足 serializable。</p><p><img src="/images/2018/03/serializable-1.jpg" alt=""></p><p>这是一个有环的图，$T_1$、$T_2$、$T_3$ 无法被串行化。</p><p><img src="/images/2018/03/serializable-2.jpg" alt=""></p><p>图论告诉我们，如果一张图是 DAG，<strong>等价于</strong>我们能为它进行拓扑排序，即给每个节点 assign 一个编号，使得所有边都是从编号小的节点指向编号大的。换而言之，<strong>如果我们能给每个节点 assign 一个这样的编号，则可以反推出原图是 DAG，进而证明 T 集合满足 serializable</strong>。</p><blockquote><p>你可能已经隐约感觉到，这个编号和事务发生的顺序有关！事实上，编号代表 serializable 后的逻辑顺序，大多数时候，这个顺序和真实的时间顺序都是一致的。</p><p>Spanner 中强调自己满足的是比 serializable 更强的一致性：linearizable，说的就是不仅能序列化，而且序列化的“逻辑顺序”和时间上的“物理顺序”也一致。</p></blockquote><h3 id="Serializable-Snapshot-Isolation-SSI"><a href="#Serializable-Snapshot-Isolation-SSI" class="headerlink" title="Serializable Snapshot Isolation (SSI)"></a>Serializable Snapshot Isolation (SSI)</h3><p>不妨把事务开始的时间戳 $t_{start}$ 作为这个编号。将上述约束条件略微加强一些，就得到了简单有效的判断法则：<strong>对于冲突 $T_1 \rightarrow T_2$，如果时间戳满足 $t_1 &lt; t_2$ 则允许发生；如果 $t_1 &gt; t_2$ 则终止事务。</strong></p><p>具体的来说，对于三种冲突，分别用以下方式处理：</p><ul><li><p><strong>Write-Read 冲突</strong>：感谢 MVCC，这是不会发生的，在 Percolator 的事务模型中，读操作一定是从一个过去时间点的 snapshot 上读取，而不会读到一个正在进行中事务的脏数据。（但是 MVCC 会引发另一个问题——staled read。见下文）</p></li><li><p><strong>Write-Write 冲突</strong>：如果 Write 发生的时候，出现了一条 $t_{start}$ 比较大的记录，则终止写事务。</p></li></ul><blockquote><p>Percolator 的 SI 实现使用了更强的约束：如果出现另一条比开始时间大的记录，无论其时间戳如何都会终止当前提交，这与 SSI 的机制有所区别。</p><p>由于 SI 无法完全避免 Read-Write 冲突（例如 write-skew 问题），所以在 Write-Write 冲突的处理上更为激进；但 SSI 已经解决了 Read-Write 冲突检测，不必用更强的约束。</p></blockquote><ul><li><strong>Read-Write 冲突</strong>：为了知道 Write 和另一个事务的 Read 冲突，必须要以某种方式记录下所有被读过的数据、以及读取事务的 $t_{start}$。这通常用范围锁（range lock）来实现——将所有查询的 TableScan 范围记录在内存中，如果某一条写入的数据满足某个 where 条件，则有必要检查一下二者的时间戳先后顺序。如果不满足上述判断法则，需要终止写事务。</li></ul><p><img src="/images/2018/03/read-write-conflict-1.jpg" alt=""></p><ul><li>由于 MVCC 的存在，<strong>Read-Write 冲突</strong>还有另一种形式：$T_2$ 的 Read 发生地更迟，但是由于 MVCC 它读到的是 $T_1$ 写之前的值（staled read），而且这里 $T_1 $ 先于 $ T_2$ 从而构成 Read-Write 冲突。</li></ul><p><img src="/images/2018/03/read-write-conflict-2.jpg" alt=""></p><p>对此，一个简单的解决方案是：如果 $T_2$ 发现 $T_1$ 写入的中间数据（lock），则立即终止自己。经典 SSI 的做法是，在 $T_2$ commit 时如果发现 $T_1$ 已经 commit 则放弃本次提交。</p><p>综上，通过给每个事务赋予一个时间戳，并保证每个冲突都符合时间戳顺序，达到 serializable 隔离级别。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>将 <code>(Table, Key, Column, Timestamp)</code> 作为 Key 的编码，从而把（半）关系型数据存储在 KV 引擎中；</li><li>用两阶段锁（2PL）的方式在 KV 引擎上实现事务的原子性提交。</li><li>禁止冲突违反时间戳先后顺序，从而保证 serializable 的隔离性。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol start="0"><li><a href="https://research.google.com/archive/spanner.html" target="_blank" rel="noopener">Spanner: Google’s Globally-Distributed Database (OSDI’12)</a></li><li><a href="https://research.google.com/pubs/pub36726.html" target="_blank" rel="noopener">Large-scale Incremental Processing Using Distributed Transactions and Notifications - USENIX 2010 - Daniel Peng, Frank Dabek</a></li><li><a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/" target="_blank" rel="noopener">How CockroachDB Does Distributed, Atomic Transactions - Cockroach Labs</a></li><li><a href="https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/" target="_blank" rel="noopener">Serializable, Lockless, Distributed: Isolation in CockroachDB - Cockroach Labs</a></li><li>Designing Data‑Intensive Applications - Martin Kleppmann</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/03/banner-google-spanner.jpg&quot; alt=&quot;banner-google-spanne&quot;&gt;&lt;/p&gt;
&lt;p&gt;Google Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。&lt;/p&gt;
&lt;p&gt;不过，&lt;strong&gt;论文对存储层实现只作了模糊的阐述&lt;/strong&gt;：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;read-only 事务&lt;/strong&gt;: 读取最新或给定时间戳 $t_{read}$ 的快照，也就是 snapshot read&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read-write 事务&lt;/strong&gt;：读取事务开始时间戳 $t_{start}$ 的快照，而写入操作在提交时间戳 $t_{commit}$ 生效&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文从 Spanner 本身设计出发，并结合开源实现 TiDB 和 CockroachDB，谈谈如何为 Spanner 设计一个存储层。本文假设读者阅读过原论文 &lt;a href=&quot;https://research.google.com/archive/spanner.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spanner: Google’s Globally-Distributed Database&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>《指数基金投资指南》读书笔记</title>
    <link href="https://ericfu.me/index-fund-guide-notes/"/>
    <id>https://ericfu.me/index-fund-guide-notes/</id>
    <published>2018-02-04T16:08:14.000Z</published>
    <updated>2018-03-16T02:50:24.546Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/02/index-fund-guide-book.jpg" alt=""></p><p>最近程序员圈子悄悄流行起斯坦福的 <a href="https://cs007.blog/" target="_blank" rel="noopener">CS 007: Personal Finance For Engineers</a> 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。</p><p>在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。</p><a id="more"></a><h2 id="指数基金的分类"><a href="#指数基金的分类" class="headerlink" title="指数基金的分类"></a>指数基金的分类</h2><p>从投资范围来看，可以分为宽基指数和行业指数。</p><h3 id="宽基指数"><a href="#宽基指数" class="headerlink" title="宽基指数"></a>宽基指数</h3><h4 id="国内市场指数"><a href="#国内市场指数" class="headerlink" title="国内市场指数"></a>国内市场指数</h4><table><thead><tr><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr><td>上证 50</td><td>沪市规模最大的 50 支股票</td><td>大盘蓝筹</td></tr><tr><td>沪深 300</td><td>沪市和深市规模最大的 300 支股票</td><td>大盘蓝筹，最有代表性</td></tr><tr><td>中证 500</td><td>除了沪深 300 之外最大的 500 支股票</td><td>中小型企业</td></tr><tr><td>创业板指数</td><td>创业板规模最大的 100 支股票</td><td>小型企业</td></tr><tr><td>上证红利</td><td>沪市股息率最高的 50 支股票</td><td>大盘股为主</td></tr><tr><td>中证红利</td><td>沪市和深市股息率最高的 100 支股票</td><td>大盘股为主</td></tr><tr><td>红利机会</td><td>过去 3 年盈利增长、1 年净利润为正等限制下股息率前 100 支</td><td>优化筛选</td></tr></tbody></table><h4 id="国外市场指数"><a href="#国外市场指数" class="headerlink" title="国外市场指数"></a>国外市场指数</h4><table><thead><tr><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr><td>恒生指数</td><td>香港股市规模最大的 50 支股票</td><td>投资港股</td></tr><tr><td>上证 50AH 优选</td><td>27 支 A 股 + 23 支 A/H 股中便宜的一方</td><td>优化上证 50</td></tr><tr><td>纳斯达克 100</td><td>NASDAQ 规模最大的的 100 支股票</td><td>美股，互联网科技股</td></tr><tr><td>标普 500</td><td>美股中各行业领导者</td><td>美股大盘蓝筹</td></tr></tbody></table><h3 id="行业指数"><a href="#行业指数" class="headerlink" title="行业指数"></a>行业指数</h3><table><thead><tr><th>行业</th><th>推荐理由</th></tr></thead><tbody><tr><td>消费</td><td>需求稳定</td></tr><tr><td>医疗</td><td>需求稳定，我国老龄化利好</td></tr><tr><td>可选消费</td><td>消费升级利好</td></tr><tr><td>养老</td><td>老龄化利好</td></tr><tr><td>金融</td><td>牛市上涨多</td></tr></tbody></table><h2 id="如何挑选指数基金"><a href="#如何挑选指数基金" class="headerlink" title="如何挑选指数基金"></a>如何挑选指数基金</h2><h3 id="格雷厄姆的理论"><a href="#格雷厄姆的理论" class="headerlink" title="格雷厄姆的理论"></a>格雷厄姆的理论</h3><p>格雷厄姆对价值投资总结出三个重要的理论：</p><ul><li><strong>价格与价值的关系。</strong>股票价格围绕其内在价值上下波动，但长期来看是趋于一致的。</li><li><strong>能力圈。</strong>只投资自己了解的品种，只有具备能力才能判断出内在价值。</li><li><strong>安全边际。</strong>只有当股价大幅低于内在价值的时候，我们才会买入。</li></ul><h3 id="常用估值指标"><a href="#常用估值指标" class="headerlink" title="常用估值指标"></a>常用估值指标</h3><h4 id="市盈率"><a href="#市盈率" class="headerlink" title="市盈率"></a>市盈率</h4><p>$$<br>市盈率 = 市值 / 盈利<br>$$</p><p>$$<br>PE = P / E<br>$$</p><p>市盈率反应了我们愿意每 1 元的利润付出多少代价。</p><p>市盈率适用于流通性好、盈利稳定的品种。</p><h4 id="盈利收益率"><a href="#盈利收益率" class="headerlink" title="盈利收益率"></a>盈利收益率</h4><p>$$<br>盈利收益率 = 盈利 / 市值<br>$$</p><p>盈利收益率是市盈率的倒数。适用条件同上。</p><h4 id="市净率"><a href="#市净率" class="headerlink" title="市净率"></a>市净率</h4><p>$$<br>市净率 = 市值 / 净资产<br>$$</p><p>$$<br>PB = P / B<br>$$</p><p>净资产等于资产减负债，净资产相对盈利比较稳定。</p><p>当企业的资产大多是比较容易衡量价值的有形资产，并且是长期保值的资产时，比较适合用市净率估值。</p><h4 id="股息率"><a href="#股息率" class="headerlink" title="股息率"></a>股息率</h4><p>股息率是过去一年的现金派息额除以公司总市值。</p><h3 id="盈利收益率法"><a href="#盈利收益率法" class="headerlink" title="盈利收益率法"></a>盈利收益率法</h3><ol><li>当盈利收益率大于 10% 时，开始定投；</li><li>当盈利收益率低于 10% 大于 6.4% 时，停止定投并持有已有份额。</li><li>当盈利收益率低于 6.4% 时，分批卖出。</li></ol><p>适用条件比较苛刻，只适用于流通性比较好、盈利比较稳定的品种，不适用于增长速度快、或盈利波动比较大的品种。例如：上证 50 指数、上证红利指数、恒生指数、H 股指数等。</p><h3 id="博格公式法"><a href="#博格公式法" class="headerlink" title="博格公式法"></a>博格公式法</h3><p>指数基金未来的年复合收益率由三个部分构成：</p><ul><li>初始股息率</li><li>未来每年的市盈率变化率</li><li>未来每年的盈利变化率</li></ul><p>分析一下：</p><ul><li>初始股息率在买入时就确定了</li><li>市盈率在某个范围内呈现周期性变化</li><li>盈利长期来看是会上涨的</li></ul><p>所以，我们的策略是：</p><ol><li>在市盈率处于历史较低位置时定投买入；</li><li>等市盈率回归正常估值，暂停定投，继续持有；</li><li>当市盈率进入历史较高区域时卖出。</li></ol><p>适用于盈利增长较快的品种、波动较大的品种，例如：沪深 300 指数、中证 500 指数、创业板指数、红利机会指数、消费行业指数、医药行业指数、养老行业指数等。</p><h3 id="博格公式变种"><a href="#博格公式变种" class="headerlink" title="博格公式变种"></a>博格公式变种</h3><p>对于盈利波动大的行业，用市净率代替市盈率。</p><p>在市净率率处于历史较低位置时买入。买入后，等待市净率回归正常即可。</p><p>适用于周期性较强、盈利不稳定的行业，例如：证券行业指数、非银金融行业指数、地产行业指数等。</p><h2 id="定投收益"><a href="#定投收益" class="headerlink" title="定投收益"></a>定投收益</h2><p>提高定投收益的技巧：</p><ol><li><p>降低交易基金的费用。</p></li><li><p>正确处理分红：相当于一笔现金收入，投入到相对低估的指数基金中。</p></li><li><p>频率选择：按周定投收益更稳定，但长期来看差距很小，按个人习惯选择即可。</p></li><li><p>定期不定额：<br>$$每个月定投金额 = \left( \frac{当月的盈利收益率}{首次的盈利收益率} \right) ^ n$$<br>$n$ 是放大倍数，建议 $n=1$。</p></li><li><p>定期不定额（博格公式法）<br>$$每个月定投金额 = \left( \frac{首次的市盈率}{当月的市盈率} \right) ^ n$$</p></li></ol><h2 id="制定定投计划"><a href="#制定定投计划" class="headerlink" title="制定定投计划"></a>制定定投计划</h2><ol><li>梳理现金流，确定每月用于定投的数额；</li><li>选择适合定投的指数基金；</li><li>选择每月／每周的某一天为定投日，按之前的方法制定策略；</li><li>做好定投记录。</li></ol><p>定投记录表示例：</p><table><thead><tr><th>日期</th><th>操作</th><th>交易品种</th><th>金额</th><th>成交单价</th><th>成交份额</th><th>当前估值</th></tr></thead><tbody><tr><td>2017-05-05</td><td>买入</td><td>501029 红利指数</td><td>750</td><td>0.994</td><td>753</td><td>12.64</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/02/index-fund-guide-book.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近程序员圈子悄悄流行起斯坦福的 &lt;a href=&quot;https://cs007.blog/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CS 007: Personal Finance For Engineers&lt;/a&gt; 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。&lt;/p&gt;
&lt;p&gt;在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Learned Index Structures 论文解读</title>
    <link href="https://ericfu.me/learned-index-paper-notes/"/>
    <id>https://ericfu.me/learned-index-paper-notes/</id>
    <published>2017-12-23T09:12:44.000Z</published>
    <updated>2018-03-16T02:49:05.735Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/12/learned-index-cover.jpg" alt=""></p><p>数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。</p><p>这些事，似乎拿预测模型都是可以做的。Yes, but…B 树那是精确的映射关系，和预测模型能一样吗？</p><p>所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。</p><a id="more"></a><h2 id="范围索引"><a href="#范围索引" class="headerlink" title="范围索引"></a>范围索引</h2><p>B 树和一个预测 model 是很相似的：</p><ul><li>B 树能定位某行数据所在的 page，可以看作是确定了一个区间：[pos, pos + pagesize]</li><li>预测模型也能做相似的事，假设我们把预测错误率成功 bound 在 min/max_err 以内，那么也就可以确定，数据位于区间 [pos - min_err, pos + max_err]</li></ul><p>一图以概之：</p><p><img src="/images/2017/12/figure-1.jpg" alt=""></p><p>于是问题变成，<strong>如何把预测错误率 bound 在 max_err 以内</strong>？</p><p>答案非常简单！通过训练。你可以把训练集的上的 error 降到多小都行，只要你模型的表现力足够强。这个问题和绝大多数机器学习的问题都不同，<strong>我们只要照顾好训练集（也就是被索引的数据）就可以了，没有测试集！</strong>当然也就不会有过拟合，模型的泛化能力是不用考虑的，比想象的简单吧！</p><p>说到模型表现力强，很容易想到神经网络。除此以外，NN 还带来了另一个好处，在 GPU（或其他专用芯片，比如 Google 的 TPU）上，NN 能获得惊人的计算速度。NN 的结构决定了它并行起来非常快，时间复杂度上把 $O(\log{n})$ 的 B 树等甩在身后。</p><p>说到这里，我们来做个思维拓展：如果把 key 作为横轴，key 在有序数组中的位置 pos 作为纵轴，画出目标函数的曲线，那应该长成这个样子：</p><p><img src="/images/2017/12/figure-2.jpg" alt=""></p><p>这是一个 CDF (累积分布函数，累积的是各个 key 对应数据的长度）。从这个角度看，无论是 B 树还是预测模型都是在拟合这个函数，只是 approach 完全不同。</p><p>这时候再回头看我们熟悉的 B 树，有没有一点顿悟—— Aha! 这不是就是决策树吗？</p><h2 id="递归模型索引-RM-Index"><a href="#递归模型索引-RM-Index" class="headerlink" title="递归模型索引 RM-Index"></a>递归模型索引 RM-Index</h2><p>以上已经说完了核心思想，接下来就是要找到一个合适的预测模型来代替 B 树。实验发现，直接上 DNN 效果并不好：单次计算代价太大，只能用 GPU（而调用 GPU 会产生不小的 overhead）；而且网络很庞大，retrain（增删改）代价很大。</p><p>Naive 的预测模型之所以做的不好，一个重要原因是：把如此大量的数据每条误差 bound 在 min/max_err 之内，实在太难了（所谓 <em>last mile</em> 问题）。为解决这个问题，决策树给我们做了个很好的提示，<strong>如果一个模型解决不了问题，就再加几层</strong>。</p><p>举个例子：为 100M 记录训练一个足够精确的预测器太难，那就分成 3 层树状结构。根节点分类器把记录分出 100 份，每份大约有 1M 记录；第二层再分出 100 份，每份大约只剩 10K 记录；第三层再分出 100 份，每份大约有 100 条记录——假设 100 条纪录足够把误差在 min/max_err 之内。</p><p><img src="/images/2017/12/figure-3.jpg" alt=""></p><p><em>注意，上图并不是一棵树，例如 Model 2.1 和 2.2 都可以选择 Model 3.2 作为下一层的分类模型。</em></p><p>这样做的好处是，每层要做的事情简单多了（每层 precision gain = 100），模型可以变得简单得多。每个 NN 模型就像一个精通自己领域的专家，他只要学习某个很小子集的 keys 就可以了。这也同时解决了 <em>last mile</em> 难题，大不了为这一百左右个 keys 过拟合一下也无妨。</p><h2 id="混合索引"><a href="#混合索引" class="headerlink" title="混合索引"></a>混合索引</h2><p>上图中的三层网络结构还带来一个额外的优势：每个 Model 其实是独立的，我们可以用除了 NN 以外的预测方法代替之，包括线性回归等简单算法，甚至是 B 树。</p><p>事实上，最后选用了两种 Model：</p><ul><li>简单的神经网络（0～2 层全连接的 hidden layer，ReLU 激发函数，每层最多 32 个神经元）</li><li>当叶节点的 NN 模型 error rate 超过阈值时，替换成 B 树</li></ul><p>训练算法如下，</p><p><img src="/images/2017/12/algorithm-1.jpg" alt=""></p><p>简单说就是：</p><ol><li>固定整个 RM-Index 的结构，比如层数、每层 Model 数量等（可以用网格法调参）；</li><li>用全部数据训练根节点，然后用根节点分类后的数据训练第二层模型，再用第二层分类后的数据训练第三层；</li><li>对于第三层（叶节点），如果 max_error 大于预设的阈值，就换成 B 树。</li></ol><p>有了 Step 3，这个 RM-Index 的分类能力也就有了下界，即使面对纯噪声数据（毫无规律可循），至少能和 B 树保持差不多的性能。</p><blockquote><p>索引的 keys 常常是字符串，而我们前文说的 NN 模型的输入是向量。Luckily，字符串向量化是 ML 里研究很多的一个课题了，这里不再讨论，有兴趣的可以看下原论文（抛砖引玉为主）。</p></blockquote><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><p>为了对比 RM-Index 和 B 树的性能，论文作者找了 4 个数据集，分别用 RM-Index 和 B 树作二级索引。</p><ul><li>Weblogs 数据集：访问时间 timestamp -&gt; log entry （约 200M）</li><li>Maps 数据集：纬度 longitude -&gt; locations （约 200M）</li><li>Web-documents 数据集：documents（字符串）-&gt; document-id（约 10M）</li><li>Lognormal 数据集：按对数正态分布随机生成的数据</li></ul><p>测试中用了不同参数的 Learned Index 和 B 树，B 树也用了一个高度优化的实现。</p><p>总体来说，Size savings 都相当可观（下降 1～2 个数量级），而速度也有所优化，最多能快 1 倍。</p><p><img src="/images/2017/12/figure-5.jpg" alt=""></p><blockquote><p>对于每个数据集，论文都给出了详细的测试结果，有兴趣的同学请看原文。</p></blockquote><p>可以说是符合预期的，个人看法是：</p><ul><li>因为算法 Step 3 的帮助，即使不经过调优，性能至少不输 B 树；</li><li>肯定能省下许多空间，因为 B 树是基于比较的查找，叶结点要保存 key 的内容，key 越多索引越大；而 NN 完全不受这个制约。</li></ul><p>但暂且不要太激动，这只是查找性能。索引的维护（增／删／改）代价如何也是要考虑的。用作者原话说，这是 learned index 的阿喀琉斯之踵，因为 NN 模型的 retrain 代价是不可预测的，这是多数 ML 算法和传统算法一个很大的不同点。对此，作者意见如下：</p><ul><li>如果恰好新的数据已经能被成功预测了，那就不用 retrain 了；但这太理想化，我们为达到 <em>last mile</em> 做的那些 overfitting 也导致了这个模型泛化性堪忧。</li><li>如果一定要 retrain，一个简单有效的优化是：把变更数据累积起来先放着，批量训练；</li><li>此外，retrain 可以借鉴一些 warm-start 的方法加快训练过程。</li></ul><h2 id="其他索引结构"><a href="#其他索引结构" class="headerlink" title="其他索引结构"></a>其他索引结构</h2><p>论文中这部分没有详细展开，因为原理和前文几乎没有区别，仅仅换一种用法。</p><h3 id="Point-Index"><a href="#Point-Index" class="headerlink" title="Point Index"></a>Point Index</h3><p>拳打完 B 树再来脚踢 hash-map。大家都知道 hash-map 兼具 $O(1)$ 的高效率和低效的空间使用率，想快就要减少碰撞，于是要牺牲更多的空间。所谓空间换时间。即使是 Google 的 Dense-hashmap 也会有 78% 的 overhead。</p><p>解决方案如图所示，用 RM-Index 模型替换掉 hash function。其思想是，利用数据的某些内在特征，帮助它找到一个最均匀（uniform）的映射方式，而不是用哈希彻底随机化。</p><p><img src="/images/2017/12/figure-9.jpg" alt=""></p><p>在三个数据集上的测试表明，这一方法的确提升了 slot 的空间利用率，减少了很多空 slot，减少的比例约 -6% ~ -78%。</p><h3 id="Existence-Index"><a href="#Existence-Index" class="headerlink" title="Existence Index"></a>Existence Index</h3><p>这回轮到的是 bloom filter，有两种思路：</p><ol><li>直接用一个二分类模型判断是否存在；</li><li>和上一小节的原理类似，把 hash 函数替换成 RMI 模型。</li></ol><p><img src="/images/2017/12/figure-12.jpg" alt=""></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>在 Jeff Dean 大神的光环之下，这篇文章很快引起热烈的讨论。</p><p>不得不说，这个脑洞开的很大，令人为之一振。一直在人们心中只能做“模糊”预测的 ML 算法竟然能代替经典的 B 树，放在 10 年前估计会被喷到体无完肤。</p><p>论文的亮点在于，大家一直在“训练——预测”这样一个思维下作 ML 而忽视了一点：至少对于已知的数据，ML 算法也是能输出一个确定的结果的！换句话说，<strong>在全集上训练，把错误率强行 bound 住其实很容易</strong>。</p><p>下面说说优缺点。</p><p>Learned index 对于规律性强的数据是大杀器，作这种数据的二级索引再合适不过了。<strong>内在规律越强，就意味着 B 树、哈希这些通用算法浪费的越多，这也是 ML 算法能捡到便宜的地方</strong>。</p><p>就像很多 DBMS 引入全文索引一样，未来的 DBMS 也也可以尝试给用户更多其他选项，为某些特别有规律的 column 建立非 B 树／Hash 的二级索引。甚至可以让 DMBS 智能化，自己尝试寻找一些规律，将 B 树索引透明的替换成其他索引。</p><p>缺点也是明显的：<strong>增删改代价难以控制</strong>，可想而知，对于规律性不那么明显的数据，这足以抹平它查找的速度优势。但我相信之后会有更多改进的 index 模型出现，把这个代价尽可能减少。</p><p>一句话总结个人看法：</p><p><strong>B 树作为通用索引的地位仍然难以撼动，但特定数据场景下，learned index 将成为一个有力的补充。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/12/learned-index-cover.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。&lt;/p&gt;
&lt;p&gt;这些事，似乎拿预测模型都是可以做的。Yes, but…B 树那是精确的映射关系，和预测模型能一样吗？&lt;/p&gt;
&lt;p&gt;所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>Sharding-JDBC 的事务处理小结</title>
    <link href="https://ericfu.me/sharding-jdbc-transaction-overview/"/>
    <id>https://ericfu.me/sharding-jdbc-transaction-overview/</id>
    <published>2017-12-13T02:17:13.000Z</published>
    <updated>2017-12-13T02:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。</p><p>本文主要介绍 Sharding-JDBC 的事务处理。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><blockquote><p>Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。</p></blockquote><ul><li>默认使用 “弱XA” 事务</li><li>可选使用柔性事务：<ul><li>BED（最大努力送达型）事务</li><li>TCC（补偿型）事务</li></ul></li></ul><a id="more"></a><h2 id="“弱XA”事务"><a href="#“弱XA”事务" class="headerlink" title="“弱XA”事务"></a>“弱XA”事务</h2><p>之所以加引号，是因为这个和 MySQL 的 XA 其实没有关系。</p><p>它的实现是很自然的：</p><ul><li><strong>Prepare 阶段</strong>（执行 SQL）过程遇到异常，则中止当前事务，对所有分库上的事务连接执行 <code>conn.rollback()</code></li><li><strong>Commit 阶段</strong>（提交事务）时对所有分库依次（其实可以并行）做 <code>conn.commit()</code>；如果某个事务连接 commit 时抛出异常，由于无法回滚其他连接，所以仅仅是收集起来报给调用者，交由用户处理。不影响其他分库 commit。</li></ul><p>文档中对使用注意事项也写的很明确：</p><blockquote><p>不支持因网络、硬件异常导致的跨库事务。例如：同一事务中，跨两个库更新，更新完毕后、未提交之前，第一个库死机，则只有第二个库数据提交。</p></blockquote><h2 id="柔性事务"><a href="#柔性事务" class="headerlink" title="柔性事务"></a>柔性事务</h2><p>分布式场景下传统数据库 ACID 无法满足业务的性能要求，所以诞生了 BASE 理论。BASE是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写，其中<em>软状态</em>是指允许系统中的数据存在中间状态。</p><p>Sharding-JDBC 的柔性事务是需要调用者显式开启的。以 BED 事务为例，客户需要先通过柔性事务管理器创建出 <em>BEDSoftTransaction</em> 对象，然后使用 <code>begin()</code> 开始柔性事务。</p><h2 id="BED（最大努力送达型）事务"><a href="#BED（最大努力送达型）事务" class="headerlink" title="BED（最大努力送达型）事务"></a>BED（最大努力送达型）事务</h2><p>用户保证该数据库的操作最终一定可以成功，所以通过最大努力<strong>反复尝试</strong>。</p><p><img src="media/15130580762465/15130630328613.png" alt=""></p><p>结合上图，执行过程可以分为 4 种情况：</p><ol><li>同步执行成功</li><li>同步执行失败，同步重试成功</li><li>同步执行失败，同步重试失败，异步重试成功</li><li>同步执行失败，同步重试失败，异步重试失败，事务日志保留（人工介入处理）</li></ol><p>BED 不保证 exactly once，所以使用 BED 的 SQL 需要满足幂等性，例如不能用 <code>UPDATE SET x = x + 1</code> 这样的 SQL。</p><h2 id="TCC（补偿型）事务"><a href="#TCC（补偿型）事务" class="headerlink" title="TCC（补偿型）事务"></a>TCC（补偿型）事务</h2><p>目前还在规划中，没有实现。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://shardingjdbc.io/docs/02-guide/transaction/" target="_blank" rel="noopener">Sharding-JDBC 文档 - 事务支持</a></li><li><a href="http://blog.csdn.net/yanyan19880509/article/details/78335935" target="_blank" rel="noopener">sharding-jdbc 事务解读</a></li><li><a href="http://www.iocoder.cn/Sharding-JDBC/transaction-bed/" target="_blank" rel="noopener">Sharding-JDBC 源码分析 —— 分布式事务（一）之最大努力型</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。&lt;/p&gt;
&lt;p&gt;本文主要介绍 Sharding-JDBC 的事务处理。&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;默认使用 “弱XA” 事务&lt;/li&gt;
&lt;li&gt;可选使用柔性事务：&lt;ul&gt;
&lt;li&gt;BED（最大努力送达型）事务&lt;/li&gt;
&lt;li&gt;TCC（补偿型）事务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>博客从 Ghost 迁移到 Hexo</title>
    <link href="https://ericfu.me/move-blog-from-ghost-to-hexo/"/>
    <id>https://ericfu.me/move-blog-from-ghost-to-hexo/</id>
    <published>2017-10-24T11:10:17.000Z</published>
    <updated>2017-10-29T11:38:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>不知道是不是我的错觉，<a href="https://ghost.org/" target="_blank" rel="noopener">Ghost</a> 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。</p><p>当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 <a href="http://zh.mweb.im/" target="_blank" rel="noopener">MWeb 编辑器</a> 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）</p><p>下面说正经的。</p><a id="more"></a><h2 id="Pros-Hexo-有哪里好？"><a href="#Pros-Hexo-有哪里好？" class="headerlink" title="Pros: Hexo 有哪里好？"></a>Pros: Hexo 有哪里好？</h2><ul><li>Markdown 格式，方便本地保存以及迁移</li><li>社区活跃，使用问题基本都能 google 到答案</li><li>Hexo + NexT 主题基本不用折腾就能满足我的所有要求，比如自带 MathJax</li><li>最近墙变高了，很容易迁移到国内的静态页面托管平台（比如 coding.net）</li></ul><h2 id="Cons-迁移代价"><a href="#Cons-迁移代价" class="headerlink" title="Cons: 迁移代价"></a>Cons: 迁移代价</h2><ul><li>因为之前的用了 <a href="https://posativ.org/isso/" target="_blank" rel="noopener">isso 开源评论系统</a>懒得迁过来了，丢失一堆评论</li><li>你可能因此浪费一个周末。</li></ul><h2 id="从-Ghost-导出内容"><a href="#从-Ghost-导出内容" class="headerlink" title="从 Ghost 导出内容"></a>从 Ghost 导出内容</h2><p>Ghost 一直很良心地在实验室页面保留了一个 <code>export</code> 按钮，导出后是一个巨大的 JSON，包含所有文章以及一些元数据：修改日期、Tags 等等。</p><p>但是 Ghost 1.X 开始启动了一个<a href="https://github.com/bustle/mobiledoc-kit/blob/master/MOBILEDOC.md" target="_blank" rel="noopener">叫 <code>mobiledoc</code> 的文档格式</a>。突然得知这一消息的我是一脸懵逼的，说好的 Markdown 呢？根据<a href="https://github.com/TryGhost/Ghost/issues/7429" target="_blank" rel="noopener">这个 Issue</a>所述：</p><blockquote><p>你知道的，我们现在的编辑器非常烂，不好用，还有一堆 Bug!</p><p>所以我们决定 Mobiledoc 重新做一个！Mobiledoc 很棒，mobiledoc 就是未来！</p></blockquote><p>Emmmm…… 好吧，你开心就好！</p><p>后果就是无论是 Ghost 导出的 JSON 还是数据库，都只有 mobiledoc 文档而没有 Markdown。所幸的是，Ghost 产生的 mobiledoc 也很奇葩，是把 Markdown 强行塞进去了。</p><p>后面会在导入时处理这个问题，先点实验室里的 <code>export</code> 拿到 JSON。</p><p>对于图片等资源，我们到 <code>assets</code> 文件夹下，打包下载下来即可。</p><h2 id="将内容导入-Hexo"><a href="#将内容导入-Hexo" class="headerlink" title="将内容导入 Hexo"></a>将内容导入 Hexo</h2><p>根据 <a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">Hexo 官方的安装指引</a>在本地装好 Hexo。</p><p>时间充裕的话，可以顺手把 <a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">NexT 主题</a>也给装了。</p><h3 id="导入文章-Markdown"><a href="#导入文章-Markdown" class="headerlink" title="导入文章 Markdown"></a>导入文章 Markdown</h3><p>之后我们开始导入文章。Hexo 是留了 migrator 插件接口的，GitHub 上能找到一个名为 hexo-migrator-ghost 的插件，但是已经年久失修。于是我帮他修了一下 bug，其中也包括从 mobiledoc 中解出 Markdown<br>代码。</p><p>你可以直接把<a href="https://github.com/fuyufjh/hexo-migrator-ghost" target="_blank" rel="noopener">我的 Repo</a> <code>git clone</code> 到 node_modules 里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/fuyufjh/hexo-migrator-ghost.git ./node_modules/hexo-migrator-ghost</span><br></pre></td></tr></table></figure><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo migrate ghost ghost-export.json</span><br></pre></td></tr></table></figure><h3 id="导入图片等资源"><a href="#导入图片等资源" class="headerlink" title="导入图片等资源"></a>导入图片等资源</h3><p>原来的资源都是放在 contents 目录下的，格式大概是 <code>contents/2017/10/29/imagename.jpg</code>。</p><p>只要把 contents 改成 images 放到 _post 目录下，然后，用你喜爱的编辑器（或者 <code>sed</code>）对所有文章做一次全局替换：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/images/ -&gt; /contents/</span><br></pre></td></tr></table></figure><p><code>hexo serve</code> 试一下，应该可以看到图片了！</p><h3 id="最后，做一些必要的配置"><a href="#最后，做一些必要的配置" class="headerlink" title="最后，做一些必要的配置"></a>最后，做一些必要的配置</h3><ul><li><a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">Hexo 配置</a></li><li><a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">NexT 配置</a></li><li><a href="http://theme-next.iissnan.com/theme-settings.html" target="_blank" rel="noopener">NexT 高级配置</a></li></ul><p>自己琢磨去吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道是不是我的错觉，&lt;a href=&quot;https://ghost.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ghost&lt;/a&gt; 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。&lt;/p&gt;
&lt;p&gt;当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 &lt;a href=&quot;http://zh.mweb.im/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MWeb 编辑器&lt;/a&gt; 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）&lt;/p&gt;
&lt;p&gt;下面说正经的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="misc" scheme="https://ericfu.me/tags/misc/"/>
    
  </entry>
  
  <entry>
    <title>Python 2 字符串编码踩坑小结</title>
    <link href="https://ericfu.me/python-2-str-and-unicode/"/>
    <id>https://ericfu.me/python-2-str-and-unicode/</id>
    <published>2017-08-31T21:55:08.000Z</published>
    <updated>2017-09-01T07:59:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Tips:</strong> 如果您已经充分理解问题是什么，请直接跳到 <a href="#问题出在哪里">#问题出在哪里</a> 一节。</p><h2 id="字符串和编码"><a href="#字符串和编码" class="headerlink" title="字符串和编码"></a>字符串和编码</h2><p>先从概念说起，字符串和它的编码是两个不同的概念：</p><ul><li><strong>字符串</strong>是一段文字本身，可以是中文可以是英文，以及各种语言</li><li>字符串的<strong>编码</strong>是计算机存储、处理字符串的方式；作为一种<strong>数据</strong>，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。</li></ul><p>字符串经过<strong>编码（encode）</strong> 就成为了一堆数据，反过来，数据经过<strong>解码（decode）</strong> 就变回我们认识的字符串。</p><p><img src="/images/2017/09/encode_decode.png" alt="encode_decode"></p><a id="more"></a><h2 id="从-Python-3-说起"><a href="#从-Python-3-说起" class="headerlink" title="从 Python 3 说起"></a>从 Python 3 说起</h2><p>这个编码问题（坑）可以说是 Python 2 被吐槽最多的黑点，没有之一。为了防止上来就掉进 Python 2 的坑里，我们先来看看 Python 3 里“改进”后是什么样子的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">len</span><span class="params">(s)</span></span></span><br><span class="line"><span class="class">9</span></span><br></pre></td></tr></table></figure><p>哈，没有任何问题！（数长度的时候别漏了空格）</p><p>查阅文档，我们发现 <code>str</code> 有个函数叫 <code>encode()</code>，它看起来很眼熟，让我们来试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">b'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>这个 <code>b&#39;&#39;</code> 的前缀表示返回值是一个 <code>bytes</code> 变量，也就是一堆数据了。</p><blockquote><p><strong>为什么这里面”Hello”还是原来的样子，但是“世界”变成一坨 <code>\x??</code> 了？</strong></p><p>这是因为 ASCII 实在太有名了，程序员们都看得懂：这个 <code>H</code> 其实表示的是一字节 <code>0x48</code>。而后面“世界”的编码不在 ASCII 的编码范围内，所以只能用 <code>\x??</code> 表示了。</p><p>这样看起来也许更清晰一些：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt;&gt;&gt; b.hex()</span><br><span class="line">&gt; <span class="string">'48656c6c6f2c20e4b896e7958c'</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>有 <code>encode()</code> 当然也有 <code>decode()</code>。我们对刚刚拿到的 <code>bytes</code>  <code>b</code> 解码，果然会变成原来的字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'Hello, 世界'</span></span><br></pre></td></tr></table></figure><p>OK，现在你已经明白了奥义所在，是时候去踩坑了。</p><h2 id="Python-2-的世界"><a href="#Python-2-的世界" class="headerlink" title="Python 2 的世界"></a>Python 2 的世界</h2><h3 id="初见茅庐"><a href="#初见茅庐" class="headerlink" title="初见茅庐"></a>初见茅庐</h3><p>先来一道开胃菜：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;type <span class="string">'str'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</span><br><span class="line"><span class="number">13</span></span><br></pre></td></tr></table></figure><p>▲ 为什么这个长度是 13 ？明明是 9 个字符啊！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>▲ <code>s</code> 你怎么坏掉了？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br></pre></td></tr></table></figure><p>▲ 我可能用了假的 <code>encode()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br></pre></td></tr></table></figure><p>▲ 喵喵喵？</p><p><strong>以上，Python 2 中字符串并不像我们想的那样工作。</strong></p><h3 id="问题出在哪里"><a href="#问题出在哪里" class="headerlink" title="问题出在哪里"></a>问题出在哪里</h3><p>其实说起来也简单，Python 是一门诞生于 1989 年的古老语言，比 Unicode 还要早两年，当时的程序员并不在乎编码问题，因为 ASCII 已经足够了。</p><p>如果你熟悉 C/C++ 会发现同样的问题：<code>char*</code> 被同时用于表示字符串和字节数组。Python 2 里也是同样，<code>str</code> 其实是个字节数组，却被挂上了字符串的名字。二十年后用着中文字符的我们被坑惨了。</p><p>后来 Python 2 为了支持 Unicode，增加了 <code>unicode</code> 类型，然而并没有卵用——程序员们不记得在每个字符串前面加上 <code>u</code>，这也不够优雅。</p><p>Python 3 设计之初就立志解决这个问题，不惜<strong>彻底修改了<code>str</code>的定义，把 <code>str</code> 这个名字让给了原来的 <code>unicode</code>！</strong>，而新增的 <code>bytes</code> 类型才是字节数组。如下表所示：</p><table><thead><tr><th></th><th>Python 2</th><th>Python 3</th></tr></thead><tbody><tr><td>字符串（Unicode）</td><td>unicode</td><td>str</td></tr><tr><td>字节数组</td><td>str (bytes)</td><td>bytes</td></tr></tbody></table><p>所以，在 Python 2 里，如果遇到非英语字符，一定要记得用 unicode。效果是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">u'Hello, 世界'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</span><br><span class="line">&lt;type <span class="string">'unicode'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">u'Hello, \u4e16\u754c'</span></span><br></pre></td></tr></table></figure><p>至于为什么 <code>str</code> 也有 <code>encode()</code>，主要是为了尽可能保持和 Python 3 的兼容性，以让部分程序能在 2、3 同时运行。<del>于是事情变得更糟糕了。</del></p><h3 id="原来如此"><a href="#原来如此" class="headerlink" title="原来如此"></a>原来如此</h3><p>现在我们可以解释刚刚遇到的奇怪情况了：</p><ul><li>“为什么这个长度是 13 ？明明是 9 个字符啊！”——因为 Python 自动帮你编码了，编码后是 13 个字节，常见的汉字在 UTF-8 编码下为 3 个字节</li><li>“<code>s</code> 你怎么坏掉了？” ——<code>str</code> 本来就是字节数组</li><li>“我可能用了假的 <code>encode()</code>”——你不应该对 <code>str</code> 变量做 <code>encode</code>，它本来就是编码后的</li><li>“喵喵喵？”——这是正常的，只是因为 Python 2 没有把 Unicode 字符显示成中文字符，用 print 就没问题了：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> s</span><br><span class="line">Hello, 世界</span><br></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li>永远记住 str 其实是 bytes，字符串应该用 unicode，尤其是包含中文时</li><li>如果能说服你的老板和同事，尽快把 Python 2 升级到 3</li></ol><p>最后，如果你需要写出兼容 Python 2\3 的程序，<a href="http://python-future.org/compatible_idioms.html#strings-and-bytes" target="_blank" rel="noopener">这篇文档</a>可以给你一些帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Tips:&lt;/strong&gt; 如果您已经充分理解问题是什么，请直接跳到 &lt;a href=&quot;#问题出在哪里&quot;&gt;#问题出在哪里&lt;/a&gt; 一节。&lt;/p&gt;
&lt;h2 id=&quot;字符串和编码&quot;&gt;&lt;a href=&quot;#字符串和编码&quot; class=&quot;headerlink&quot; title=&quot;字符串和编码&quot;&gt;&lt;/a&gt;字符串和编码&lt;/h2&gt;&lt;p&gt;先从概念说起，字符串和它的编码是两个不同的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;字符串&lt;/strong&gt;是一段文字本身，可以是中文可以是英文，以及各种语言&lt;/li&gt;
&lt;li&gt;字符串的&lt;strong&gt;编码&lt;/strong&gt;是计算机存储、处理字符串的方式；作为一种&lt;strong&gt;数据&lt;/strong&gt;，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;字符串经过&lt;strong&gt;编码（encode）&lt;/strong&gt; 就成为了一堆数据，反过来，数据经过&lt;strong&gt;解码（decode）&lt;/strong&gt; 就变回我们认识的字符串。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/09/encode_decode.png&quot; alt=&quot;encode_decode&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>用 Bandit 做 Python 代码静态安全分析</title>
    <link href="https://ericfu.me/bandit-the-python-static-analyzer/"/>
    <id>https://ericfu.me/bandit-the-python-static-analyzer/</id>
    <published>2017-08-16T01:50:47.000Z</published>
    <updated>2017-08-21T00:55:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Bandit-是什么？"><a href="#Bandit-是什么？" class="headerlink" title="Bandit 是什么？"></a>Bandit 是什么？</h2><p>Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。</p><p>安装说明：参见 <a href="https://github.com/openstack/bandit" target="_blank" rel="noopener">GitHub 项目主页</a>。</p><a id="more"></a><h2 id="编写自定义的检查"><a href="#编写自定义的检查" class="headerlink" title="编写自定义的检查"></a>编写自定义的检查</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@bandit.checks('Call')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prohibit_unsafe_deserialization</span><span class="params">(context)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'unsafe_load'</span> <span class="keyword">in</span> context.call_function_name_qual:</span><br><span class="line">        <span class="keyword">return</span> bandit.Issue(</span><br><span class="line">            severity=bandit.HIGH,</span><br><span class="line">            confidence=bandit.HIGH,</span><br><span class="line">            text=<span class="string">"Unsafe deserialization detected."</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><ul><li><a href="mailto:`@bandit.checks" target="_blank" rel="noopener">`@bandit.checks</a>(‘Call’)<code>: 仅仅检查类型为</code>Call` 的 AST Node</li><li><code>return bandit.Issue(...)</code>: 返回一个 Security Issue</li></ul><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>入口在 <code>cli/main.py</code> 的 <code>main()</code></p><p>先初始化了一堆参数，然后在这里创建了一个关键的 BanditManager 对象，之后的事情都是由它来完成的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b_mgr = b_manager.BanditManager(b_conf, args.agg_type, args.debug,</span><br><span class="line">                                profile=profile, verbose=args.verbose,</span><br><span class="line">                                ignore_nosec=args.ignore_nosec)</span><br></pre></td></tr></table></figure><h3 id="扫描文件"><a href="#扫描文件" class="headerlink" title="扫描文件"></a>扫描文件</h3><p>紧接着就能看到这行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initiate file discovery step within Bandit Manager</span></span><br><span class="line">b_mgr.discover_files(args.targets, args.recursive, args.excluded_paths)</span><br></pre></td></tr></table></figure><p>让我们跟进去，然后看看 <code>discover_files()</code> 都做了些什么：（代码太长就不贴了）</p><ul><li>处理 include/exclude 参数<ul><li>如果有 include 就只看这里面的文件，否则扫描所有文件</li><li>如果有 exclude，之后扫描的时候要去掉这些文件</li></ul></li><li>对所有指定的目标进行扫描<ul><li>如果设置了 recursive 选项，就递归地遍历子目录。</li></ul></li></ul><p>最后把遍历的结果排序并以列表的形式存放在 <code>self.files_list</code> 中。</p><h3 id="运行-Tests"><a href="#运行-Tests" class="headerlink" title="运行 Tests"></a>运行 Tests</h3><p>回到 <code>main()</code> 函数中，再往下看一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initiate execution of tests within Bandit Manager</span></span><br><span class="line">b_mgr.run_tests()</span><br></pre></td></tr></table></figure><p>看来这里是核心的一步，当然要走进去看看：（代码不贴了）</p><ul><li>枚举刚刚列表中的所有文件，读出来、并调用 <code>_parse_file()</code> 处理之。</li><li>如果处理失败了，也记下来，最后汇总输出会用到。</li></ul><p>继续跟进 <code>_parse_file()</code>，发现只是个包装，进入 <code>_execute_ast_visitor()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_execute_ast_visitor</span><span class="params">(self, fname, data, nosec_lines)</span>:</span></span><br><span class="line">    score = []</span><br><span class="line">    res = b_node_visitor.BanditNodeVisitor(fname, self.b_ma,</span><br><span class="line">                                           self.b_ts, self.debug,</span><br><span class="line">                                           nosec_lines, self.metrics)</span><br><span class="line"></span><br><span class="line">    score = res.process(data)</span><br><span class="line">    self.results.extend(res.tester.results)</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>这个 BanditNodeVisitor 虽然没有继承标准库里的 ast.NodeVisitor 但实际上做的工作就是那样的——遍历所有 AST Node，同时对各个类型的 Node 执行对应的函数。</p><p>在 BanditNodeVisitor 中定义了很多类似 <code>visit_Call</code>, <code>visit_FunctionDef</code>, <code>visit_Str</code> 这样奇怪名字的函数，顾名思义就是对各个类型的 Node 所运行的函数。遍历的逻辑看 <code>visit</code> 函数。</p><p>以 <code>visit_Call</code> 为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visit_Call</span><span class="params">(self, node)</span>:</span></span><br><span class="line">    self.context[<span class="string">'call'</span>] = node</span><br><span class="line">    qualname = b_utils.get_call_name(node, self.import_aliases)</span><br><span class="line">    name = qualname.split(<span class="string">'.'</span>)[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    self.context[<span class="string">'qualname'</span>] = qualname</span><br><span class="line">    self.context[<span class="string">'name'</span>] = name</span><br><span class="line"></span><br><span class="line">    self.update_scores(self.tester.run_tests(self.context, <span class="string">'Call'</span>))</span><br></pre></td></tr></table></figure><p>其实很简单，把对应的一些上下文信息 extract 出来并存到 <code>self.context</code>，然后用 <code>tester.run_tests</code> 执行所有对应 Call Node 的检查。</p><p>所以 <code>run_tests()</code> 的逻辑你应该能猜到个大概了：</p><ol><li>拿到所有类型为 <code>checktype</code> 的检查</li><li>对每个检查，以当前的 <code>context</code> 作为参数做检查，如果检查出问题就存起来</li></ol><h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><p>再回到 <code>main()</code> 函数中，再往下就是输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trigger output of results by Bandit Manager</span></span><br><span class="line">sev_level = constants.RANKING[args.severity - <span class="number">1</span>]</span><br><span class="line">conf_level = constants.RANKING[args.confidence - <span class="number">1</span>]</span><br><span class="line">b_mgr.output_results(args.context_lines,</span><br><span class="line">                     sev_level,</span><br><span class="line">                     conf_level,</span><br><span class="line">                     args.output_file,</span><br><span class="line">                     args.output_format)</span><br></pre></td></tr></table></figure><p>Severity 和 Confidence 都是用来过滤的。最后输出到指定的形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Bandit-是什么？&quot;&gt;&lt;a href=&quot;#Bandit-是什么？&quot; class=&quot;headerlink&quot; title=&quot;Bandit 是什么？&quot;&gt;&lt;/a&gt;Bandit 是什么？&lt;/h2&gt;&lt;p&gt;Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。&lt;/p&gt;
&lt;p&gt;安装说明：参见 &lt;a href=&quot;https://github.com/openstack/bandit&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub 项目主页&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
      <category term="security" scheme="https://ericfu.me/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>获得第三届阿里中间件性能大赛冠军</title>
    <link href="https://ericfu.me/aliware-performance-contest-first-prize/"/>
    <id>https://ericfu.me/aliware-performance-contest-first-prize/</id>
    <published>2017-07-18T16:48:53.000Z</published>
    <updated>2017-08-18T00:48:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。</p><p><img src="/images/2017/07/alibaba-praise.jpg" alt="alibaba-praise"></p><p>Emmm… 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。</p><a id="more"></a><h2 id="第一赛季：消息中间件"><a href="#第一赛季：消息中间件" class="headerlink" title="第一赛季：消息中间件"></a>第一赛季：消息中间件</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/middlewarerace2017/open-messaging-demo" target="_blank" rel="noopener">这里</a>）</p><p>设计一个消息中间件，它支持 Topic 和 Queue，每个 Topic 可以被一个或多个 Queue 订阅。显然，发往 Topic 的消息会被所有的订阅者接收到，而发往某个特定 Queue 的消息只会被它接收到。</p><p>消息不能丢失，且对于各个 Queue 消息要保持有序。在此前提下，吞吐量最优者获胜。</p><p>要注意的是，生产和消费是分两个阶段进行的：先进行生产，结束后再进行消费。</p></blockquote><p>我的思路很简单，对于每个 Topic 或者 Queue，把所有消息序列化后写入对应的一个文件中。消费时，从磁盘中顺序读取即可。</p><p>本题最后瓶颈落在了磁盘 IO 上，所以很多优化也变得无关紧要了。为了充分利用磁盘 IO，一定要确保两点：</p><ul><li>顺序写入和读取（参见我的<a href="https://ericfu.me/varnish-notes-from-archetect/">这篇文章</a>）</li><li>使用 Linux 内存映射（mmap）技术，也就是 Java nio 包里的 MappedByteBuffer</li></ul><p>此外，序列化的过程中尽可能减少内存拷贝，以及避免不必要的 String 序列化、反序列化。</p><p>做到以上几点，进入前 20 不是问题。初赛只要前 200 名就可以晋级了（吐槽：那很容易啊！！但是一开始不知道初赛成绩也要计入最后评分的）</p><p><a href="https://code.aliyun.com/fuyufjh/open-messaging-demo-zero/" target="_blank" rel="noopener">代码</a>托管在阿里云 code 上。</p><h2 id="第二赛季：数据库同步"><a href="#第二赛季：数据库同步" class="headerlink" title="第二赛季：数据库同步"></a>第二赛季：数据库同步</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/wanshao/IncrementalSync/" target="_blank" rel="noopener">这里</a>）</p><p>模拟数据库的主备复制。给定一批 binlog 文本数据，你的任务是重放所有 binlog 从而得到数据表的最新状态（假设原本状态是空的）。重放的操作包含 Insert/Update/Delete 三种操作，注意主键也可能被更新！</p><p>验证结果的方式是，客户端给定一个 PK 的区间，输出该区间的所有行。</p><p>注意，程序分 Server 和 Client 端，机器配置都很高（16核CPU），但是 JVM 的堆大小被限制为 1G 新生代 + 2G 老年代。此外，必须顺序读取 binlog。</p></blockquote><p>思路如下：并行化是一定要的，如何并行呢？答案（当然）是按主键哈希分桶。然而这样主键更新怎么处理？这是最大的难点。</p><p>为此我们想出了一种并行化的算法——可以说这就是最终获得冠军的原因。具体思路在决赛答辩 PPT 里写的很清楚啦。</p><p>答辩PPT（包括完整的解题思路）：</p><p><iframe src="//www.slideshare.net/slideshow/embed_code/key/jURs7pfS57Uz7d" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><p>这题思考了很久，最后交出的答卷含金量也很高。哈希分桶作为经典算法中常见的一种模式，在很多分布式系统中都有应用。</p><p>此外，比赛初期还没有加上“必须顺序读取”这个条件。如果没有这个条件又会有别的奇思妙想的算法来解决。留给读者自己思考（喵）</p><p><a href="https://code.aliyun.com/fuyufjh/IncrementalSyncSequential" target="_blank" rel="noopener">代码</a><br>托管在阿里云上，同时还有<br><a href="https://code.aliyun.com/fuyufjh/tianchi-middleware-doc" target="_blank" rel="noopener">设计文档</a> 也很有帮助（如果你觉得 PPT 还不够细致）。</p><h2 id="24-小时极客-PK-赛"><a href="#24-小时极客-PK-赛" class="headerlink" title="24 小时极客 PK 赛"></a>24 小时极客 PK 赛</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/fuyufjh/topkn_final" target="_blank" rel="noopener">这里</a>）</p><p>分页排序。给定一批数据，求解按顺序从小到大，顺序排名从第k下标序号之后连续的n个数据，类似于 <code>order by id limit k, n</code>，n 会很大，k &lt;= 100</p><p>数据是文本文件，每行是一个长度在 256 字符以内的字符串。排序的方式是：先按长度从小到大、再按字典序。</p><p>注意一共有 2 台 Worker 和 1 台 Master，Worker 上分别放了 5G 的数据，最后 Master 要请求到完整的排序结果。</p></blockquote><p>和之前的两场比赛不同，24小时赛的成绩是不计入最后评价的，而且是以个人名义参赛。奖品是去硅谷开（玩）会（耍）！（PS. 然而不能带女朋友，最后去了一帮基佬啊摔！）</p><p>外排序是很容易想到的思路，然而十分复杂，也不能很好的并行。那怎么办？依然是分桶，Bucket Sort。</p><p>假设字符串长度的分布是均匀的，字符出现的概率也是相近的，则我们可以用以下的值作为每个字符串的 key：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;length&gt; &lt;text[0]&gt; &lt;text[1]&gt; &lt;text[2]&gt;</span><br></pre></td></tr></table></figure><p>（如果长度小于 3 就用 ‘\0’ 填充一下）</p><p>这样我们就能对每个 key 做统计了——有多少个字符串是在这个桶里呢？假设这个结果放在数组 <code>count</code> 中，那再对 <code>count</code> 求一个累计和，就能用二分查找找到第 N 个数应该位于哪一个桶里。</p><p>同样的也可以找到第 N+K-1 个数应该位于哪个桶里。然后就简单了，把区间内的桶里的值都取出来，让 Master 排个序就好了。</p><p>把区间内的桶里的值都取出来？——这个过程可以扫描整个文件。也可以做个预处理的优化：把每个字符串的 key 和 offset 分别存到两个数组里，比如 <code>keys[]</code> 和 <code>offsets[]</code>，这样只要扫描数组就可以了！</p><p>然而我并没有想到这个优化，所以是第二名。哈哈哈！</p><p><a href="https://code.aliyun.com/fuyufjh/topkn_final?spm=a2111a.8458726.0.0.59556667DTd4xf" target="_blank" rel="noopener">代码</a>托管在阿里云上。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol><li>算法是好的，要多刷 HackerRank</li><li>阿里搞的这个比赛啊，Excited ！</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/07/alibaba-praise.jpg&quot; alt=&quot;alibaba-praise&quot;&gt;&lt;/p&gt;
&lt;p&gt;Emmm… 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Data Streaming Algorithms Slideshare</title>
    <link href="https://ericfu.me/data-streaming-algorithms-slideshare/"/>
    <id>https://ericfu.me/data-streaming-algorithms-slideshare/</id>
    <published>2017-07-05T19:52:36.000Z</published>
    <updated>2017-08-19T07:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/2p95byDFIJLeWO" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><a id="more"></a><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol><li><a href="https://mapr.com/blog/some-important-streaming-algorithms-you-should-know-about/" target="_blank" rel="noopener">Some Important Streaming Algorithms You Should Know About</a></li><li><a href="https://www.slideshare.net/SandeepJoshi55/data-streaming-algorithms-65575952" target="_blank" rel="noopener">Data streaming algorithms</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/2p95byDFIJLeWO&quot; width=&quot;100%&quot; height=&quot;500&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;/div&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Paxos &amp; Raft 分布一致性算法 Slideshare</title>
    <link href="https://ericfu.me/paxos-raft-share/"/>
    <id>https://ericfu.me/paxos-raft-share/</id>
    <published>2017-05-11T04:47:39.000Z</published>
    <updated>2017-08-19T07:26:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/tPlG02EI7ijaKE" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><a id="more"></a><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol><li><a href="https://zh.wikipedia.org/zh-hans/Paxos%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">Paxos算法 - 维基百科</a></li><li><a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">In Search of an Understandable Consensus Algorithm<br>(Extended Version)</a></li><li><a href="http://codemacro.com/2014/10/15/explain-poxos/" target="_blank" rel="noopener">图解分布式一致性协议Paxos - loop in code</a></li></ol><h4 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h4><ol><li><a href="https://github.com/renquinn/Paxos-Demo" target="_blank" rel="noopener">Demo - Paxos</a> （有小错误）</li><li><a href="thesecretlivesofdata.com/raft/">Demo - Raft</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/tPlG02EI7ijaKE&quot; width=&quot;100%&quot; height=&quot;500&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;/div&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>性能测试：ArrayBlockingQueue vs. Go channel</title>
    <link href="https://ericfu.me/perf-test-blockingqueue-vs-channel/"/>
    <id>https://ericfu.me/perf-test-blockingqueue-vs-channel/</id>
    <published>2017-04-30T23:56:23.000Z</published>
    <updated>2017-08-19T07:28:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>测试方案：</p><ol><li>主线程将一个 Integer 对象发到 Channel 0</li><li>线程 i 将对象从 Channel i 不断搬运到 Channel i+1</li><li>最后一个线程从 Channel N-1 中拿到对象，做加和</li></ol><p>为了保证公平，Go 中自行封装一个 Integer 而不是用 int 型；考虑到实际中大多数情况下 channel 里走的都是对象而非基本类型，这样是合理的。</p><a id="more"></a><p><strong>发现二者完成时间基本都在 3.8s ~ 4.0s 之间，可以说没有差异。</strong> ArrayBlockingQueue 的性能看来还是很高的。</p><p>PS. 尝试了容量不限的 ListBlockingQueue，时间在 5s 左右，也还可以接受。</p><p>测试代码在<a href="https://gist.github.com/fuyufjh/9657835df8202b29af8be1610c8327ad" target="_blank" rel="noopener">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;测试方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主线程将一个 Integer 对象发到 Channel 0&lt;/li&gt;
&lt;li&gt;线程 i 将对象从 Channel i 不断搬运到 Channel i+1&lt;/li&gt;
&lt;li&gt;最后一个线程从 Channel N-1 中拿到对象，做加和&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了保证公平，Go 中自行封装一个 Integer 而不是用 int 型；考虑到实际中大多数情况下 channel 里走的都是对象而非基本类型，这样是合理的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="golang" scheme="https://ericfu.me/tags/golang/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>AWS Solution Architect 认证学习笔记</title>
    <link href="https://ericfu.me/aws-solution-architect-cert-associate/"/>
    <id>https://ericfu.me/aws-solution-architect-cert-associate/</id>
    <published>2017-04-30T22:22:55.000Z</published>
    <updated>2017-08-22T00:27:38.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/05/Solutions-Architect-Associate.png" alt="Solutions-Architect-Associate"></p><p>经过一个月的学习，成功通过了 AWS 考试认证。不得不说相比从零开始闷声摸索，学习带来的收获还是很大的，避免了一些 AWS 的大坑，也了解了一些 best practice 的系统设计。</p><p>可惜这个博客并不是在 AWS 上（原因可能是贫穷）。</p><p>以下是我看过的学习资料：</p><a id="more"></a><ul><li><a href="https://acloud.guru" target="_blank" rel="noopener">ACloudGuru 的课程</a>，需要购买</li><li>阅读各个 AWS Service FAQ</li><li><a href="http://jayendrapatil.com/" target="_blank" rel="noopener">Jayendra’s Blog</a>，知识点整理以及样题</li></ul><p>个人学习笔记（整理自 ACloudGuru 课程每章总结）</p><ul><li><a href="https://ericfu.me/aws-notes-iam/">AWS 学习笔记之 IAM</a></li><li><a href="https://ericfu.me/aws-notes-s3/">AWS 学习笔记之 S3</a></li><li><a href="https://ericfu.me/aws-notes-ec2/">AWS 学习笔记之 EC2</a></li><li><a href="https://ericfu.me/aws-notes-route53/">AWS 学习笔记之 Route53</a></li><li><a href="https://ericfu.me/aws-notes-database/">AWS 学习笔记之数据库</a></li><li><a href="https://ericfu.me/aws-notes-vpc/">AWS 学习笔记之 VPC</a></li><li><a href="https://ericfu.me/aws-notes-sqs-swf-sns/">AWS 学习笔记之 SQS/SWF/SNS 等</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/05/Solutions-Architect-Associate.png&quot; alt=&quot;Solutions-Architect-Associate&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过一个月的学习，成功通过了 AWS 考试认证。不得不说相比从零开始闷声摸索，学习带来的收获还是很大的，避免了一些 AWS 的大坑，也了解了一些 best practice 的系统设计。&lt;/p&gt;
&lt;p&gt;可惜这个博客并不是在 AWS 上（原因可能是贫穷）。&lt;/p&gt;
&lt;p&gt;以下是我看过的学习资料：&lt;/p&gt;
    
    </summary>
    
      <category term="series" scheme="https://ericfu.me/categories/series/"/>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
  <entry>
    <title>译：Varnish 架构师笔记</title>
    <link href="https://ericfu.me/varnish-notes-from-archetect/"/>
    <id>https://ericfu.me/varnish-notes-from-archetect/</id>
    <published>2017-04-30T22:06:55.000Z</published>
    <updated>2017-08-19T07:31:16.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>找到这篇文章是在阅读 Kafka 文档时，一个<a href="https://kafka.apache.org/documentation/#persistence" target="_blank" rel="noopener">名为 “Don’t fear the filesystem!”的段落</a>中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单：</p><ul><li>因为操作系统 PageCache 的存在，磁盘操作可能很快</li><li>虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快</li><li>CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要</li></ul><p><a href="http://varnish-cache.org/docs/trunk/phk/notes.html" target="_blank" rel="noopener">原文链接</a></p></blockquote><p>当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。</p><p>多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。</p><a id="more"></a><p>所以 Varnish 这个项目一开始并没能激起我很大的兴趣。但我渐渐意识到 Varnish 虽然是一个用户态应用，但却也是一个能充分发挥我长久以来积累的关于硬件和内核的经验知识的理想场所。目前 Varnish 的开发已经进展到alpha版本的阶段，而我觉得应当承认自己相当享受这一段历程。</p><h3 id="1975的编程方式到底出了什么问题？"><a href="#1975的编程方式到底出了什么问题？" class="headerlink" title="1975的编程方式到底出了什么问题？"></a>1975的编程方式到底出了什么问题？</h3><p>简而言之：计算机系统再也不应该被看作是由两个存储层次构成的了。</p><p>首先是主存：从水银延迟线，磁芯存储器，到现在可供随机访问的RAM。</p><p>然后是辅存：从纸带，磁带，到磁盘。最早的磁盘有一屋子大，然后缩小到洗衣机的尺寸，到今天硬盘可以被放进随身携带的 MP3 播放器中。</p><p>于是大家就按照这样的划分，在内存中分配变量，在磁盘中存取数据，以这样的方式进行编程工作。</p><p>还是拿 Squid 这个1975年风格的系统为例：你事先配置它的内存和硬盘用量，然后它会花费大把时间来追踪哪些HTTP对象驻留内存中，哪些存放在硬盘上，并且根据不同情况来调整它们的放置策略。</p><p>然而实际上呢，现今的计算机应被视为只使用一种统一的存储系统，这个系统完全基于硬盘（磁盘，固态硬盘或者其他什么东西），而传统的内存呢，在操作系统内核和虚拟内存硬件机制的帮助下可以被看作是硬盘的缓存。</p><p>回过头来看 Squid 的策略，它精心设计的存储管理机制实际上却陷入了与操作系统内核同样精巧的管理策略的激烈冲突。而就像所有内战一样，这样的冲突必然一事无成。</p><p>我们可以尝试从细节角度来看整个流程：一开始 Squid 会请求内存用来创建了一个 HTTP 对象，它往往会在创建之初被频繁访问多次，然后闲置一段时间。而后当内核接收到其他内存分配请求时，会将这些它认为闲置的内存数据放到硬盘交换分区去，而把这些回收的内存页用于更活跃的任务。在Squid下一次访问这一对象时，操作系统又会把暂存在交换分区的数据取回来供它使用。 这些内核对于内存的操作对于 Squid 是透明的，在它看来这个 HTTP 对象就像从没离开过内存一样，而实际上物理内存页得到了更有效的使用。</p><p>这就是虚拟内存机制。</p><p>如果事情到此为止的话就好了，但接下来1975年式的编程风格就出现了。</p><p>一段时间之后，Squid 也和内核一样注意到了这个对象闲置了，于是打算把它放到硬盘上去，省出一些内存来给更频繁访问的数据使用。所以它打开一个文件把这个对象写了进去。</p><p>打开慢镜头来看这个写入的过程：</p><p>Squid 通过系统调用 write 将 HTTP 对象的虚拟内存地址传递给内核。<br>由于物理页已经被内核交换出去，这个内存访问将引发一个缺页中断。<br>为了重新载入这个内存页，内核不得不交换出另一个正在使用的内存页（很可能包含另一 Squid 的对象），修复页表，然后继续执行系统调用。<br>Squid 对这些一无所知，它自以为只是进行了一次再普通不过的访存操作而已。</p><p>接下来 Squid 可以终于拿这块内存放别的数据了。而当这个对象再次被访问到时，Squid 又不得不把它从硬盘中取回来。首先它需要空闲的内存来存放这个对象，于是它根据某种淘汰算法选中另一个最近不常用的对象，把它写到硬盘上去（上面那些步骤重演了一遍）。然后打开文件读出这次所需的那个对象，最后通过网络套接字发送出去。</p><p>这一切显然充满了各种无用功。</p><p>让我们看看 Varnish 是怎么做的</p><p>Varnish 会直接请求大块虚拟内存，并由操作系统将这个内存空间映射到一个硬盘文件。当需要访问某个 HTTP 对象时，只需要正确索引这个对象相应的虚拟内存地址，剩下的交给操作系统就好了。当内核需要回收一些内存页时，它会自行决定将一些 Varnish 的内存数据写回到映射的文件中。而当 Varnish 再次访问这一块虚拟内存时，内核自然会腾出内存页来将文件中的数据读回使用。</p><p>仅此而已。</p><p>Varnish 不去尝试控制哪些数据应该被缓存在内存中，哪些应该放到硬盘上去。内核代码和硬件会处理这些事情，而且处理得很漂亮。</p><p>此外，与 Squid 不同的是 Varnish 只需要一个文件而不是为每个 HTTP 对象创建单独的文件。没有任何理由需要在 HTTP 对象和文件系统对象间建立一一对应的关系，也没有理由把时间浪费在文件系统的命名空间处理上。Varnish 需要处理的只是虚拟内存的指针和所需对象的长度值而已。</p><p>虚拟内存的出现为数据大于物理内存的场景提供了一种便利的机制，但人们似乎并没有领悟这一点。</p><h3 id="更多的缓存"><a href="#更多的缓存" class="headerlink" title="更多的缓存"></a>更多的缓存</h3><p>CPU 的时钟频率目前看来基本止步于4GHz了。即便如此，为了避免内存读写的瓶颈，硬件工程师们不得不使用使用一级，二级，有时候甚至是三级 CPU cache（现在我们可以认为 RAM 是第四级缓存了），此外还有写缓冲，流水线，页模式读取等各种技术，而这些都是为了加快访存来匹配CPU的处理速度。</p><p>虽然时钟频率方面受限，但硅工艺的进步缩小了器件尺寸，集成了更多的晶体管。所以多核 CPU 的设计逐渐成为主流，但这从编程模型角度看来实在是很糟糕的一件事。</p><p>虽然多核系统存在已久，但编写能够利用上多个 CPU 的代码依然是一件棘手的事。而要在多核系统上写出高性能的程序就更是如此了。</p><p>比如我们有两个计数器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> n_foo;</span><br><span class="line"><span class="keyword">unsigned</span> n_bar;</span><br></pre></td></tr></table></figure><p>在一个 CPU 上执行了 <code>n_foo++</code> 的操作会使得CPU读取 <code>n_foo</code> 的值然后写回。</p><p>读取一个内存位置首先会检查它是否在 CPU L1 cache 中命中，这挺难的除非它刚刚被使用过。接下来是 L2 cache，我们不妨假设依然没有命中吧。</p><p>如果是在一个单核系统上，CPU 会去内存读取数据然后就完事。但在多核系统中，我们必须去检查其他CPU核心是否缓存并修改了 <code>n_foo</code> 的数值。我们会发起一个特殊的总线事务做这种检查，如果其他 CPU 答复说它确实持有这样一份拷贝，它就需要将它写回到内存中。如果硬件设计良好，我们可能可以通过总线监听获得这份新数据，不然的话就需要去内存里读取它。</p><p>我们终于可以修改 <code>n_foo</code> 的值了，但其实修改完了之后一般不会直接将它写回到内存中。为了之后操作的快速存取，很可能我们会把它缓存起来。</p><p>现在假设另一个 CPU 需要执行 <code>n_bar++</code> 的操作，它能够直接进行吗？答案是否定的。因为缓存的单位并不是字节而是 cache-line（典型值是 8-128 个字节）。所以当第一个 CPU 忙着处理 <code>n_foo</code> 时，第二个 CPU 必须等待，因为虽然它想获取的是另一个变量，但却不幸落在了同一个 cache-line 中。</p><p>明白了吧？没错，这有点丑。</p><h3 id="我们该怎么办"><a href="#我们该怎么办" class="headerlink" title="我们该怎么办"></a>我们该怎么办</h3><p><strong>尽一切可能，减少内存操作。</strong></p><p>下面是 Varnish 的一些做法。</p><p>当需要处理一个 HTTP 请求或响应时，我们会持有一组指针和一个内存工作区。我们并不需要在处理每个 HTTP 报头时都调用 malloc，而是一次性分配整个工作区的内存，然后按需从中获取所需空间。而当我们一次性释放全部报头时，只要将指针重置到工作区的起始位置即可。</p><p>当需要将 HTTP 报头从一个请求拷贝到另一个请求（或从从一个响应复制到另一个响应）时，并不需要进行字符串拷贝，而只要拷贝指针。如果源报头在这个过程中不会被不释放或改写，这样做是非常安全的。比如从客户端请求到后台请求的拷贝就是这样一个例子。</p><p>但在一些新构建的报头生命周期长于源报头的场景中，就需要另外分配内存了。例如当我们会缓存新 HTTP 对象时，首先就计算整个报头所需空间，然后通过一次 malloc 调用来获取内存。</p><p>另外我们会尽可能重用那些正被缓存的内存数据。</p><p>比如 Varnish 的 worker 线程是以<em>最近最忙的</em>方式调度的，也即是说一个 worker 线程空闲后会被放回到队列的最前端，使得它更有机会马上去处理下一个新请求，这样它的数据，栈空间和变量等很可能可以在 CPU 缓存中被重用，而不是再次从RAM中读取。</p><p>同时对于 worker 线程经常使用的数据，我们会把它们分配在每个线程的栈变量中，并且确保它们占据完整的内存页。这样我们就可以尽可能避免 cache-line 的竞争。</p><p>如果对你来说这些听起来都很陌生，我可以告诉你它们是确实有效的：Varnish 处理一个命中缓存的请求最多只需18个系统调用，而且其中不少只是为了获得时间戳来满足统计的需要。</p><p>这些技术并不新鲜，我们已经在内核开发中使用了10多年，现在该轮到你们来学习了:-)</p><p>如此，欢迎进入 Varnish，一个 2006风格架构的程序。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;找到这篇文章是在阅读 Kafka 文档时，一个&lt;a href=&quot;https://kafka.apache.org/documentation/#persistence&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;名为 “Don’t fear the filesystem!”的段落&lt;/a&gt;中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因为操作系统 PageCache 的存在，磁盘操作可能很快&lt;/li&gt;
&lt;li&gt;虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快&lt;/li&gt;
&lt;li&gt;CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;http://varnish-cache.org/docs/trunk/phk/notes.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。&lt;/p&gt;
&lt;p&gt;多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="os" scheme="https://ericfu.me/tags/os/"/>
    
  </entry>
  
  <entry>
    <title>HackerRank: Spanning Tree Fraction 题解</title>
    <link href="https://ericfu.me/hackerrank-spanning-tree-fraction/"/>
    <id>https://ericfu.me/hackerrank-spanning-tree-fraction/</id>
    <published>2017-04-20T08:50:21.000Z</published>
    <updated>2017-08-19T07:44:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>题目：<a href="https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction" target="_blank" rel="noopener">https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction</a><br>一张连通图G=(V,E)上，每条边有a和b两个整数，求一个生成树使得Sum(a) / Sum(b) 最大，输出这个最大值的分数形式 p/q</p></blockquote><p>设 $\frac{\sum{a_i}}{\sum{b_i}} \ge c$ ，经过变换可得，$\sum{(a_i - b_i c)} \ge 0$，这种形式下 <code>ai - bi * c</code> 就退化为一条边的 cost，能方便得用 Prim 或 Kruskal 算法求出 cost 最大的生成树。</p><a id="more"></a><p>因为我们知道 c 的取值是受限制的——显然不能任意大。根据题意，我们要求 c 的最大可能值 max(c)，借助二分查找：如果 c 取值 <code>(L+R)/2</code> 时找不到生成树满足 sum cost &gt;= 0，说明 max(c) 在右半边；反之，如果 c 的取值 <code>(L+R)/2</code> 时可行，说明在左半边（因为 C 是实数，这里说的都是闭区间）。 </p><p>Prim 和 Kruskal 算法的实现也是一个难点。本题中 Kruskal 算法更简单，从小到大取所有的边，利用并查集可以快速判断这条边是否已经被连通了，若还为连通就要选取此边。</p><p>最后的 p/q 怎么求呢？显然从实数 c 变回 p/q 是不可能的。假如循环 N 次，最后一次循环可以认为已经收敛了，最后一次计算中途的 $\sum{a_i}$ 和 $\sum{b_i}$ 就是最优 case 下的取值，将 $\frac{\sum{a_i}}{\sum{b_i}}$ 化简、消除公约数即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;numeric&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">100002</span>;</span><br><span class="line"><span class="keyword">int</span> u[N], v[N], a[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> w[N];</span><br><span class="line"><span class="keyword">int</span> p[N];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> <span class="built_in">set</span>[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_set</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">set</span>[x] == x) <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>[x] = find_set(<span class="built_in">set</span>[x]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">union_set</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">    a = find_set(a);</span><br><span class="line">    b = find_set(b);</span><br><span class="line">    <span class="keyword">if</span> (a == b) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="built_in">set</span>[a] = b;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> A, B;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">double</span> c)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) w[i] = a[i] - b[i] * c;</span><br><span class="line">    sort(p, p + m, [](<span class="keyword">int</span> i, <span class="keyword">int</span> j)&#123; <span class="keyword">return</span> w[i] &gt; w[j]; &#125;);</span><br><span class="line">    iota(<span class="built_in">set</span>, <span class="built_in">set</span> + n, <span class="number">0</span>);</span><br><span class="line">    A = B = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> e = p[i];</span><br><span class="line">        <span class="keyword">if</span> (union_set(u[e], v[e])) &#123;</span><br><span class="line">            A += a[e];</span><br><span class="line">            B += b[e];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> A &gt;= B * c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (a &amp;&amp; b) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a &gt; b) a %= b;</span><br><span class="line">        <span class="keyword">else</span> b %= a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a | b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d %d %d"</span>, &amp;u[i], &amp;v[i], &amp;a[i], &amp;b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    iota(p, p + m, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">double</span> lo = <span class="number">0</span>, hi = <span class="number">1e5</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> t = <span class="number">0</span>; t &lt; <span class="number">100</span>; t++) &#123;</span><br><span class="line">        <span class="keyword">double</span> c = (hi + lo) * <span class="number">0.5</span>;</span><br><span class="line">        <span class="keyword">if</span> (check(c)) lo = c;</span><br><span class="line">        <span class="keyword">else</span> hi = c;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> g = gcd(A, B);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d/%d\n"</span>, A/g, B/g);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;题目：&lt;a href=&quot;https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction&lt;/a&gt;&lt;br&gt;一张连通图G=(V,E)上，每条边有a和b两个整数，求一个生成树使得Sum(a) / Sum(b) 最大，输出这个最大值的分数形式 p/q&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;设 $\frac{\sum{a_i}}{\sum{b_i}} \ge c$ ，经过变换可得，$\sum{(a_i - b_i c)} \ge 0$，这种形式下 &lt;code&gt;ai - bi * c&lt;/code&gt; 就退化为一条边的 cost，能方便得用 Prim 或 Kruskal 算法求出 cost 最大的生成树。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>AWS 学习笔记之 SQS/SWF/SNS 等</title>
    <link href="https://ericfu.me/aws-notes-sqs-swf-sns/"/>
    <id>https://ericfu.me/aws-notes-sqs-swf-sns/</id>
    <published>2017-04-11T02:34:49.000Z</published>
    <updated>2017-08-19T07:46:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SQS-Simple-Queue-Service"><a href="#SQS-Simple-Queue-Service" class="headerlink" title="SQS - Simple Queue Service"></a>SQS - Simple Queue Service</h2><p>SQS 是 AWS 的消息队列服务，用于暂存消息并等待接收者处理。</p><ul><li>不保证 FIFO，可能乱序到达</li><li>Visibility Timeout 最长 12 小时</li><li>保证每条消息至少被传递一次（At least once），这意味着某些情况下可能传递多次，所以你写程序的时候要注意处理重复的消息</li><li>每条消息最大 256 KB<ul><li>然而，依据 64 KB 的 Chunk 数量计费</li><li>所以一个 256 KB 的消息可能产生 4 次费用：4 * 64 KB chunks</li></ul></li><li>SQS 的消息没有优先级；如果你需要优先级，最佳实践是创建多个 SQS 队列</li></ul><a id="more"></a><h2 id="SWF-Simple-Workflow-service"><a href="#SWF-Simple-Workflow-service" class="headerlink" title="SWF - Simple Workflow service"></a>SWF - Simple Workflow service</h2><p>SWF 提供 Workflow 服务，比如 Amazon 用它来处理客户订单的流程——下单、支付、配货、发货……</p><ul><li>一个 Workflow 可以长达 1 年，而 SQS 的消息最多保留 14 天</li><li>SWF 的 API 面向 task，而 SQS 的 API 面向 message</li><li>SWF 保证一个 task 只会分配一次、不会重复；SQS 不保证这一点</li><li>SWF 会跟踪每个 task 和 event 的处理；SQS 没有这个功能</li></ul><p>SWF 有以下 3 种 Actors：</p><ul><li>Workflow Starter - 启动 workflow 的程序，比如电商网站的下单操作</li><li>Decider - 控制 task 的执行流程，如果某个步骤完成／失败，decider 决定下一步做什么</li><li>Activity Worker - 执行任务，可以是人类</li></ul><h2 id="SNS-Simple-Notification-Service"><a href="#SNS-Simple-Notification-Service" class="headerlink" title="SNS - Simple Notification Service"></a>SNS - Simple Notification Service</h2><p>SNS 提供消息通知服务，例如当 CloudTrail 发出警告时给用户发 Email 通知。</p><p>SNS 的订阅者可以是以下这些：</p><ul><li>HTTP / HTTPS</li><li>Email / Email (JSON)</li><li>SQS</li><li>Lambda</li><li>Application</li></ul><p>SNS vs SQS：</p><ul><li>相同点：都是消息服务</li><li>不同点：SNS 是 Push，而 SQS 是 Pull (Poll)</li></ul><h2 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h2><ul><li>API Gateway 很便宜，并能自动伸缩</li><li>API Gateway 可以通过 cache 提升性能，减轻后端服务器的负载</li><li>API Gateway 可以 throttle 流量从而防止被攻击</li><li>可以把所有的访问记到 CloudWatch</li><li>如果你用到了跨域 AJAX，记得开启 CORS (Cross-Origin Resource Sharing)</li></ul><h2 id="Elastic-Transcoder"><a href="#Elastic-Transcoder" class="headerlink" title="Elastic Transcoder"></a>Elastic Transcoder</h2><p>提供云转码服务，将原始视频格式转换到不同的格式，从而方便在电脑、平板或手机上播放。AWS 提供了很多预设的格式，你不用自己调参数，直接选择对应的设备就可以了。</p><p>根据转码的时间以及分辨率收费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;SQS-Simple-Queue-Service&quot;&gt;&lt;a href=&quot;#SQS-Simple-Queue-Service&quot; class=&quot;headerlink&quot; title=&quot;SQS - Simple Queue Service&quot;&gt;&lt;/a&gt;SQS - Simple Queue Service&lt;/h2&gt;&lt;p&gt;SQS 是 AWS 的消息队列服务，用于暂存消息并等待接收者处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不保证 FIFO，可能乱序到达&lt;/li&gt;
&lt;li&gt;Visibility Timeout 最长 12 小时&lt;/li&gt;
&lt;li&gt;保证每条消息至少被传递一次（At least once），这意味着某些情况下可能传递多次，所以你写程序的时候要注意处理重复的消息&lt;/li&gt;
&lt;li&gt;每条消息最大 256 KB&lt;ul&gt;
&lt;li&gt;然而，依据 64 KB 的 Chunk 数量计费&lt;/li&gt;
&lt;li&gt;所以一个 256 KB 的消息可能产生 4 次费用：4 * 64 KB chunks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SQS 的消息没有优先级；如果你需要优先级，最佳实践是创建多个 SQS 队列&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
</feed>
