<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding Husky</title>
  
  <subtitle>Thoughts, stories and ideas.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ericfu.me/"/>
  <updated>2018-04-26T04:23:25.563Z</updated>
  <id>https://ericfu.me/</id>
  
  <author>
    <name>Eric Fu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>处理海量数据：列式存储综述（存储篇）</title>
    <link href="https://ericfu.me/columnar-storage-overview-storage/"/>
    <id>https://ericfu.me/columnar-storage-overview-storage/</id>
    <published>2018-04-12T05:17:02.000Z</published>
    <updated>2018-04-26T04:23:25.563Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/04/banner-warehouse.jpg" alt="banner-warehouse"></p><p><strong>列式存储（Column-oriented Storage）</strong>并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。</p><p>总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。</p><a id="more"></a><h2 id="什么是列式存储"><a href="#什么是列式存储" class="headerlink" title="什么是列式存储"></a>什么是列式存储</h2><p>传统 OLTP 数据库通常采用行式存储。以下图为例，所有的列依次排列构成一行，以行为单位存储，再配合以 B+ 树或 SS-Table 作为索引，就能快速通过主键找到相应的行数据。</p><p><img src="/images/2018/04/row-oriented-example.png" alt="Figure 1.1"></p><p>行式存储对于 OLTP 场景是很自然的：大多数操作都以实体（entity）为单位，即大多为<strong>增删改查一整行记录</strong>，显然把一行数据存在物理上相邻的位置是个很好的选择。</p><p>然而，对于 OLAP 场景，一个典型的查询需要遍历整个表，进行分组、排序、聚合等操作，这样一来按行存储的优势就不复存在了。更糟糕的是，分析型 SQL 常常不会用到所有的列，而仅仅对其中某些感兴趣的列做运算，那一行中那些无关的列也不得不参与扫描。</p><p>列式存储就是为这样的需求设计的。如下图所示，同一列的数据被一个接一个紧挨着存放在一起，表的每列构成一个长数组。</p><p><img src="/images/2018/04/column-oriented-example.png" alt="Figure 1.2"></p><p>显然，列式存储对于 OLTP 不友好，一行数据的写入需要同时修改多个列。但对 OLAP 场景有着很大的优势：</p><ul><li>当查询语句只涉及部分列时，只需要扫描相关的列</li><li>每一列的数据都是相同类型的，彼此间相关性更大，对列数据压缩的效率较高</li></ul><blockquote><p><strong>BigTable（HBase）是列式存储吗？</strong></p><p>很多文章将 BigTable 归为列式存储。但严格地说，BigTable 并非列式存储，虽然论文中提到借鉴了 C-Store 等列式存储的某些设计，但 BigTable 本身按 Key-Value Pair 存储数据，和列式存储并无关系。</p><p><img src="/images/2018/04/bigtable-column-families-example.png" alt="Figure 1.3"></p><p>有一点迷惑的是 BigTable 的列簇（column family）概念，列簇可以被指定给某个 locality group，决定了该列簇数据的物理位置，从而可以让同一主键的各个列簇分别存放在最优的物理节点上。由于 column family 内的数据通常具有相似性，对它做压缩要比对整个表压缩效果更好。</p><p>另外，值得强调的一点是：列式数据库可以是关系型、也可以是 NoSQL，这和是否是列式并无关系。本文中讨论的 C-Store 就采用了关系模型。</p></blockquote><h2 id="起源：DSM-分页模式"><a href="#起源：DSM-分页模式" class="headerlink" title="起源：DSM 分页模式"></a>起源：DSM 分页模式</h2><p>我们知道，由于机械磁盘受限于磁头寻址过程，读写通常都以一块（block）为单位，<strong>故在操作系统中被抽象为块设备</strong>，与流设备相对。这能帮助上层应用更好地管理储存空间、增加读写效率等。这一特性直接影响了数据库储存格式的设计：数据库的 Page 对应一个或几个物理扇区，让数据库的 Page 和扇区对齐，提升读写效率。</p><p>那如何将数据存放到页上呢？</p><p>大多数服务于在线查询的 DBMS 采用 NSM (N-ary Storage Model) 即按行存储的方式，将完整的行（即关系 relation）从 Header 开始依次存放。页的最后有一个索引，存放了页内各行的起始偏移量。由于每行长度不一定是固定的，索引可以帮助我们快速找到需要的行，而无需逐个扫描。</p><p>NSM 的缺点在于，如果每次查询只涉及很小的一部分列，那多余的列依然要占用掉宝贵的内存以及 CPU Cache，从而导致更多的 IO；为了避免这一问题，很多分析型数据库采用 DSM (Decomposition Storage Model) 即按列分页：将 relation 按列拆分成多个 sub-relation。类似的，页的尾部存放了一个索引。</p><p><img src="/images/2018/04/nsm-dsm-pax-comparation.png" alt="nsm-dsm-pax-comparation"></p><p>顺便一提，2001 年 Ailamaki 等人提出 PAX (Partition Attributes Cross) 格式，尝试将 DSM 的一些优点引入 NSM，将两者的优点相结合。具体来说，NSM 能更快速的取出一行记录，这是因为一行的数据相邻保存在同一页；DSM 能更好的利用 CPU Cache 以及使用更紧凑的压缩。PAX 的做法是将一个页划分成多个 minipage，minipage 内按列存储，而一页中的各个 minipage 能组合成完整的若干 relation。</p><p>如今，随着分布式文件系统的普及和磁盘性能的提高，<strong>很多先进的 DBMS 已经抛弃了按页存储的模式</strong>，但是其中的某些思想，例如<strong>数据分区、分区内索引、行列混合</strong>等，仍然处处可见于这些现代的系统中。</p><blockquote><p>分布式储存系统虽然不再有页的概念，但是仍然会将文件切割成分块进行储存，但分块的粒度要远远大于一般扇区的大小（如 HDFS 的 Block Size 一般是 128MB）。更大的读写粒度是为了适应网络 IO 更低的带宽以获得更大的吞吐量，但另一方面也牺牲了细粒度随机读写。</p></blockquote><h2 id="列数据的编码与压缩"><a href="#列数据的编码与压缩" class="headerlink" title="列数据的编码与压缩"></a>列数据的编码与压缩</h2><p>无论对于磁盘还是内存数据库，IO 相对于 CPU 通常都是系统的性能瓶颈，<strong>合理的压缩手段不仅能节省空间，也能减少 IO 提高读取性能</strong>。列式存储在数据编码和压缩上具有天然的优势。</p><p>以下介绍的是 C-Store 中的数据编码方式，具有一定的代表性。根据 1) 数据本身是否按顺序排列（self-order） 2) 数据有多少不同的取值（distinct values），分成以下 4 种情况讨论：</p><ul><li><p><strong>有序且 distict 值不多</strong>。使用一系列的三元组 $(v, f, n)$ 对列数据编码，表示数值 $v$ 从第 $f$ 行出现，一共有 $n$ 个（即 $f$ 到 $f+n-1$ 行）。例如：数值 4 出现在 12-18 行，则编码为 $(4, 12, 7)$。</p></li><li><p><strong>无序且 distict 值不多</strong>。对于每个取值 $v$ 构造一个二进制串 $b$，表示 $v$ 所在位置的 bitmap。例如：如果一列的数据是 $0,0,1,1,2,1,0,2,1$，则编码为 <code>(0, 110000100)</code>、<code>(1, 001101001)</code> 和 <code>(2,000010010)</code>。由于 bitmap 是稀疏的，可以对其再进行行程编码。</p></li><li><p><strong>有序且 distict 值多</strong>。对于这种情况，把每个数值表示为前一个数值加上一个变化量（delta），当然第一个数值除外。例如，对于一列数据 $1,4,7,7,8,12$，可以表示为序列 $1,3,3,0,1,4$。显然编码后的数据更容易被 densepack，且压缩比更高。</p></li><li><p><strong>无序且 distict 值多</strong>。对于这种情况没有很好的编码方式。</p></li></ul><p>编码之后，还可以对数据进行压缩。由于一列的数据本身具有相似性，即使不做特殊编码，也能取得相对较好的压缩效果。通常采用 Snappy 等支持流式处理、吞吐量高的压缩算法。</p><p>最后，编码和压缩不仅是节约空间的手段，更多时候也是组织数据的手段。在 PowerDrill、Dremel 等系统中，我们会看到<strong>很多编码本身也兼具了索引的功能</strong>，例如在扫描中跳过不需要的分区，甚至完全改表查询执行的方式。</p><h2 id="列式存储与分布式文件系统"><a href="#列式存储与分布式文件系统" class="headerlink" title="列式存储与分布式文件系统"></a>列式存储与分布式文件系统</h2><p>在现代的大数据架构中，GFS、HDFS 等分布式文件系统已经成为存放大规模数据集的主流方式。分布式文件系统相比单机上的磁盘，具备多副本高可用、容量大、成本低等诸多优势，但也带来了一些单机架构所没有的问题：</p><ol><li>读写均要经过网络，吞吐量可以追平甚至超过硬盘，但是<strong>延迟要比硬盘大得多</strong>，且受网络环境影响很大。</li><li>可以进行大吞吐量的顺序读写，但随机访问性能很差，大多<strong>不支持随机写入</strong>。为了抵消网络的 overhead，通常写入都以几十 MB 为单位。</li></ol><p>上述缺点对于重度依赖随机读写的 OLTP 场景来说是致命的。所以我们看到，很多定位于 OLAP 的列式存储选择放弃 OLTP 能力，从而能构建在分布式文件系统之上。</p><p>要想将分布式文件系统的性能发挥到极致，无非有几种方法：<strong>按块（分片）读取数据、流式读取、追加写入等</strong>。我们在后面会看到一些开源界流行的列式存储模型，将这些优化方法体现在存储格式的设计中。</p><h2 id="列式存储系统案例"><a href="#列式存储系统案例" class="headerlink" title="列式存储系统案例"></a>列式存储系统案例</h2><h3 id="C-Store-2005-Vertica"><a href="#C-Store-2005-Vertica" class="headerlink" title="C-Store (2005) / Vertica"></a>C-Store (2005) / Vertica</h3><p>大多数 DBMS 都是为写优化，而 C-Store 是第一个为读优化的 OLTP 数据库系统，虽然从今天的视角看它应当算作 HTAP 。在 ad-hoc 的分析型查询、ORM 的在线查询等场景中，大多数操作都是查询而非写入，在这些场景中列式存储能取得更好的性能。像主流的 DBMS 一样，C-Store 支持标准的关系型模型。</p><p>就像本文开头即提到——列式存储不是新鲜事。C-Store 的主要贡献有以下几点：<strong>通过精心设计的 projection 同时实现列数据的多副本和多种索引方式；用读写分层的方式兼顾了（少量）写入的性能</strong>。此外，C-Store 可能是第一个现代的列式存储数据库实现，其的设计启发了无数后来的商业或开源数据库，就比如 <a href="https://www.vertica.com/" target="_blank" rel="external">Vertica</a>。</p><h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>C-Store 是关系型数据库，它的逻辑表和其他数据库中的并没有什么不同。但是在 C-Store 内部，逻辑表被纵向拆分成 projections，每个 projection 可以包含一个或多个列，甚至可以包含来自其他逻辑表的列（构成索引）。当然，每个列至少会存在于一个 projections 上。</p><p>下图的例子中，EMP 表被存储为 3 个 projections，DEPT 被存储为 1 个 projection。每个 projection 按照各自的 sort key 排序，在图中用下划线表示 sort key。</p><p><img src="/images/2018/04/c-store-projections-example.png" alt="cstore-architecture"></p><p>Projection 内是以列式存储的：里面的每个列分别用一个数据结构存放。为了避免列太长引起问题，也支持每个 projection 以 sort key 的值做横向切分。</p><p>查询时 C-Store 会先选择一组能覆盖结果中所有列的 projections 集合作为 covering set，然后进行 join 计算重构出原来的行。为了能高效地进行 projections 的 join（即按照另一个 key 重新排序），引入 join index 作为辅助，其中存储了 proj1 到 proj2 的下标映射关系。</p><p><strong>Projection 是有冗余性的</strong>，常常 1 个列会出现在多个 projection 中，但是它们的顺序也就是 sort key 并不相同，因此 <strong>C-Store 在查询时可以选用最优的一组 projections</strong>，使得查询执行的代价最小。</p><p>巧妙的是，<strong>C-Store 的 projection 冗余性还用来实现 K-safe 高可用</strong>（容忍最多 K 台机器故障），当部分节点宕机时，只要 C-Store 还能找到某个 covering set 就能执行查询，虽然不一定是最优的 covering set 组合。</p><blockquote><p>从另一个角度看，C-Store 的 Projection 可以看作是一种物化（materialized）的查询结果，即查询结果在查询执行前已经被预先计算好；并且由于每个列至少出现在一个 Projection 当中，没有必要再保存原来的逻辑表。</p><p>为任意查询预先计算好结果显然不现实，但是如果物化某些经常用到的中间视图，就能在预计算代价和查询代价之间获得一个平衡。C-Store 物化的正是以某个 sort key 排好序（甚至 JOIN 了其他表）的一组列数据，同时预计算的还有 join index。</p></blockquote><p>C-Store 对写入的处理将在下一篇文章中呈现。</p><h3 id="Apache-ORC"><a href="#Apache-ORC" class="headerlink" title="Apache ORC"></a>Apache ORC</h3><p>Apache ORC 最初是为支持 Hive 上的 OLAP 查询开发的一种文件格式，如今在 Hadoop 生态系统中有广泛的应用。ORC 支持各种格式的字段，包括常见的 int、string 等，也包括 struct、list、map 等组合字段；字段的 meta 信息就放在 ORC 文件的尾部（这被称为自描述的）。</p><h4 id="数据结构及索引"><a href="#数据结构及索引" class="headerlink" title="数据结构及索引"></a>数据结构及索引</h4><p><strong>为分区构造索引是一种常见的优化方案</strong>，ORC 的数据结构分成以下 3 个层级，在每个层级上都有索引信息来加速查询。</p><p><img src="/images/2018/04/orc-file-structure.png" alt="orc-file-structure"></p><ul><li><strong>File Level</strong>：即一个 ORC 文件，Footer 中保存了数据的 meta 信息，还有文件数据的索引信息，例如各列数据的最大最小值（范围）、NULL 值分布、布隆过滤器等，这些信息可用来<strong>快速确定该文件是否包含要查询的数据</strong>。每个 ORC 文件中包含多个 Stripe。</li><li><strong>Stripe Level</strong> 对应原表的一个范围分区，里面包含该分区内各列的值。每个 Stripe 也有自己的一个索引放在 footer 里，和 file-level 索引类似。</li><li><strong>Row-Group Level</strong> ：一列中的每 10000 行数据构成一个 row-group，每个 row-group 拥有自己的 row-level 索引，信息同上。</li></ul><p>ORC 里的 Stripe 就像传统数据库的页，它是 ORC 文件批量读写的基本单位。这是由于分布式储存系统的读写延迟较大，一次 IO 操作只有批量读取一定量的数据才划算。这和按页读写磁盘的思路也有共通之处。</p><blockquote><p>像其他很多储存格式一样，ORC 选择将统计数据和 Metadata 放在 File 和 Stripe 的尾部而不是头部。</p><p>但 ORC 在 Stripe 的读写上还有一点优化，那就是把分区粒度小于 Stripe 的结构（如 Column 和 Row-Group）的索引统一抽取出来放到 Stripe 的头部。这是因为在批处理计算中一般是把整个 Stripe 读入批量处理的，将这些索引抽取出来可以减少在批处理场景下需要的 IO（批处理读取可以跳过这一部分）。</p></blockquote><h4 id="ACID-支持"><a href="#ACID-支持" class="headerlink" title="ACID 支持"></a>ACID 支持</h4><p>Apache ORC 提供有限的 ACID 事务支持。受限于分布式文件系统的特点，文件不能随机写，那如何把修改保存下来呢？</p><p>类似于 LSM-Tree 中的 MVCC 那样，writer 并不是直接修改数据，而是为每个事务生成一个 delta 文件，文件中的修改被叠加在原始数据之上。当 delta 文件越来越多时，通过 minor compaction 把连续多个 delta 文件合成一个；当 delta 变得很大时，再执行 major compaction 将 delta 和原始数据合并。</p><p><strong>这种保持基线数据不变、分层叠加 delta 数据的优化方式在列式存储系统中十分常见，是一种通用的解决思路</strong>。</p><blockquote><p>别忘了 ORC 的 delta 文件也是写入到分布式储存中的，因此每个 Delta 文件的内容不宜过短。这也解释了 ORC 文件虽然支持事务，但是主要是对批量写入的事务比较友好，不适合频繁且细小的写入事务的原因。</p></blockquote><h3 id="Dremel-2010-Apache-Parquet"><a href="#Dremel-2010-Apache-Parquet" class="headerlink" title="Dremel (2010) / Apache Parquet"></a>Dremel (2010) / Apache Parquet</h3><p>Dremel 是 Google 研发的用于大规模只读数据的查询系统，用于进行快速的 ad-hoc 查询，弥补 MapReduce 交互式查询能力的不足。为了避免对数据的二次拷贝，Dremel 的数据就放在原处，通常是 GFS 这样的分布式文件系统，为此需要设计一种通用的文件格式。</p><p>Dremel 的系统设计和大多 OLAP 的列式数据库并无太多创新点，但是其精巧的存储格式却变得流行起来，Apache Parquet 就是它的开源复刻版。注意 Parquet 和 ORC 一样都是一种存储格式，而非完整的系统。</p><h4 id="嵌套数据模型"><a href="#嵌套数据模型" class="headerlink" title="嵌套数据模型"></a>嵌套数据模型</h4><p>Google 内部大量使用 Protobuf 作为跨平台、跨语言的数据序列化格式，相比 JSON 要更紧凑并具有更强的表达能力。Protobuf 不仅允许用户定义必须（required）和可选（optinal）字段，<strong>还允许用户定义 repeated 字段，意味着该字段可以出现 0～N 次，类似变长数组</strong>。</p><p>Dremel 格式的设计目的就是按列来存储 Protobuf 的数据。由于 repeated 字段的存在，这要比按列存储关系型的数据困难一些。一般的思路可能是用终止符表示每个 repeat 结束，<strong>但是考虑到数据可能很稀疏</strong>，Dremel 引入了一种更为紧凑的格式。</p><p>作为例子，下图左半边展示了数据的 schema 和 2 个 Document 的实例，右半边是序列化之后的各个列。序列化之后的列多出了 R、D 两列，分别代表 Repetition Level 和 Definition Level，<strong>通过这两个值就能确保唯一地反序列化出原本的数据</strong>。</p><p><img src="/images/2018/04/google-dremel-example.png" alt="google-dremel-example"></p><p><strong>Repetition Level</strong> 表示当前值在哪一个级别上重复。对于非 repeated 字段只要填上 trivial 值 0 即可；否则，只要这个字段可能出现重复（无论本身是 repeated 还是外层结构是 repeated），应当为 R 填上当前值在哪一层上 repeat。</p><p>举个例子说明：对于 Name.Language.Code 我们一共有三条非 NULL 的记录。</p><ol><li>第一个是 <code>en-us</code>，出现在第一个 Name 的第一个 Lanuage 的第一个 Code 里面。在此之前，这三个元素是没有重复过的，都是第一次出现。所以其 R=0</li><li>第二个是 <code>en</code>，出现在下一个 Language 里面。也就是说 Language 是重复的元素。Name.Language.Code 中Language 排第二个，所以其 R=2</li><li>第三个是 <code>en-gb</code>，出现在下一个 Name 中，Name 是重复元素，排第一个，所以其 R=1</li></ol><p>注意到 <code>en-gb</code> 是属于第3个 Name 的而非第2个Name，为了表达这个事实，我们在 <code>en</code> 和 <code>en-gb</code>中间放了一个 R=1 的 NULL。</p><p><strong>Definition Level</strong> 是为了说明 NULL 被定义在哪一层，也就宣告那一层的 repeat 到此为止。对于非 NULL 字段只要填上 trivial 值，即数据本身所在的 level 即可。</p><p>同样举个例子，对于 Name.Language.Country 列</p><ol><li><code>us</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R1 内部，表示当前 Name 之内、后续所有 Language 都不含有 Country 字段。所以D为2。</li><li><code>NULL</code> 在 R1 内部，表示当前 Document 之内、后续所有 Name 都不含有 Country 字段。所以D为1。</li><li><code>gb</code> 非 NULL 值填上 Country 字段的 level 即 D=3</li><li><code>NULL</code> 在 R2 内部，表示后续所有 Document 都不含有 Country 字段。所以D为0。</li></ol><p>可以证明，结合 R、D 两个数值一定能唯一构建出原始数据。<strong>为了高效编解码，Dremel 在执行时首先构建出状态机，之后利用状态机处理列数据</strong>。不仅如此，状态机还会结合查询需求和数据的 structure 直接跳过无关的数据。</p><blockquote><p>状态机实现可以说是 Dremel 论文的最大贡献。但是受限于篇幅，有兴趣的同学请参考原论文。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了列式存储的存储结构设计。抛开种种繁复的细节，我们看到，以下这些思想或设计是具有共性的。</p><ol><li><strong>跳过无关的数据</strong>。从行存到列存，就是消除了无关列的扫描；ORC 中通过三层索引信息，能快速跳过无关的数据分片。</li><li><strong>编码既是压缩，也是索引</strong>。Dremel 中用精巧的嵌套编码避免了大量 NULL 的出现；C-Store 对 distinct 值的编码同时也是对 distinct 值的索引；PowerDrill 则将字典编码用到了极致（见下一篇文章）。</li><li><strong>假设数据不可变</strong>。无论 C-Store、Dremel 还是 ORC，它们的编码和压缩方式都完全不考虑数据更新。如果一定要有更新，暂时写到别处、读时合并即可。</li><li><strong>数据分片</strong>。处理大规模数据，既要纵向切分也要横向切分，不必多说。</li></ol><p>下一篇文章中，将会结合 C-Store、MonetDB、Apache Kudu、PowerDrill 等现代列式数据库系统，侧重描述列式 DBMS 的整体架构设计以及独特的查询执行过程。<strong>敬请期待！</strong></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://dbmsmusings.blogspot.jp/2010/03/distinguishing-two-major-types-of_29.html" target="_blank" rel="external">Distinguishing Two Major Types of Column-Stores - Daniel Abadi</a></li><li><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html" target="_blank" rel="external">Columnar Storage - Amazon Redshift</a></li><li><a href="http://www.vldb.org/conf/2001/P169.pdf" target="_blank" rel="external">Weaving Relations for Cache Performance - A Ailamaki, DJ DeWitt, MD Hill, M Skounakis</a></li><li><a href="http://glinden.blogspot.jp/2006/05/c-store-and-google-bigtable.html" target="_blank" rel="external">C-Store and Google BigTable - Greg Linden</a></li><li><a href="http://db.csail.mit.edu/pubs/abadi-column-stores.pdf" target="_blank" rel="external">The Design and Implementation of Modern Column-Oriented Database Systems - D Abadi, P Boncz, S Harizopoulos…</a></li><li><a href="http://people.csail.mit.edu/tdanford/6830papers/stonebraker-cstore.pdf" target="_blank" rel="external">C-store: a column-oriented DBMS - M Stonebraker, DJ Abadi, A Batkin, X Chen…</a></li><li><a href="https://orc.apache.org/docs/" target="_blank" rel="external">Apache ORC Docs</a></li><li><a href="https://research.google.com/pubs/archive/36632.pdf" target="_blank" rel="external">Dremel: Interactive Analysis of Web-Scale Datasets - S Melnik, A Gubarev, JJ Long, G Romer…</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/04/banner-warehouse.jpg&quot; alt=&quot;banner-warehouse&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;列式存储（Column-oriented Storage）&lt;/strong&gt;并不是一项新技术，最早可以追溯到 1983 年的论文 Cantor。然而，受限于早期的硬件条件和使用场景，主流的事务型数据库（OLTP）大多采用行式存储，直到近几年分析型数据库（OLAP）的兴起，列式存储这一概念又变得流行。&lt;/p&gt;
&lt;p&gt;总的来说，列式存储的优势一方面体现在存储上能节约空间、减少 IO，另一方面依靠列式数据结构做了计算上的优化。本文中着重介绍列式存储的数据组织方式，包括数据的布局、编码、压缩等。在下一篇文章中将介绍计算层以及 DBMS 整体架构设计。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Let&#39;s Encrypt 免费的通配符域名证书</title>
    <link href="https://ericfu.me/lets-encrypt-wildcard-cert/"/>
    <id>https://ericfu.me/lets-encrypt-wildcard-cert/</id>
    <published>2018-03-15T06:15:14.000Z</published>
    <updated>2018-03-16T16:31:48.673Z</updated>
    
    <content type="html"><![CDATA[<p>免费 SSL 证书提供商 Let’s Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日<a href="https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579" target="_blank" rel="external">正式发布</a>了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。</p><p>ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。</p><a id="more"></a><p>截至目前为止，官方推荐的 ACME 客户端 <a href="https://github.com/certbot/certbot" target="_blank" rel="external">Certbot</a> 还没有支持 ACME v2，这里推荐另一个小巧的客户端 <a href="https://github.com/Neilpang/acme.sh" target="_blank" rel="external">acme.sh</a>，它完全用 shell 脚本构成，快速安装方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl https://get.acme.sh | sh</div></pre></td></tr></table></figure><p>为了使用 DNS-01 challenge，先在 DNS 服务商那里启用 Token 访问。这里以 DNSPod 为例，其它服务商参见<a href="https://github.com/Neilpang/acme.sh/blob/master/dnsapi/README.md" target="_blank" rel="external">完整支持列表</a>。</p><p><img src="/images/2018/03/dnspod-api-token.png" alt="dnspod-api-token"></p><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> DP_Id=<span class="string">"49644"</span></div><div class="line"><span class="built_in">export</span> DP_Key=<span class="string">"******************************"</span></div><div class="line">acme.sh --issue --dns dns_dp -d example.com -d *.example.com</div></pre></td></tr></table></figure><p>如果一切顺利，可以看到证书已经生成了。如果要安装到 nginx 中，还需要转换成 PEM 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">acme.sh --install-cert -d example.com -d *.example.com \</div><div class="line">        --key-file       /etc/nginx/certificates/example.com/key.pem  \</div><div class="line">        --fullchain-file /etc/nginx/certificates/example.com/cert.pem</div></pre></td></tr></table></figure><p>最后把 nginx 配置从 HTTP 修改成 HTTPS：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">server &#123;</div><div class="line">    listen 443 ssl;</div><div class="line">    server_name www.example.com example.com;</div><div class="line"></div><div class="line">    ssl_certificate /etc/nginx/certificates/example.com/cert.pem;</div><div class="line">    ssl_certificate_key /etc/nginx/certificates/example.com/key.pem;</div><div class="line"></div><div class="line">    # other configurations ...</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>重启 nginx，大功告成。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;免费 SSL 证书提供商 Let’s Encrypt 去年承诺的 ACME v2 以及通配符证书（Wildcard Certificate）终于在 3 月 14 日&lt;a href=&quot;https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;正式发布&lt;/a&gt;了。ACME 全称“自动化证书管理环境”，用于自动验证域名所有权并颁发 SSL 证书；本次发布的新版 ACME v2 添加了通配符证书的支持，再也不用每次添加子域名都重新申请证书。&lt;/p&gt;
&lt;p&gt;ACME v2 目前只有一种方案支持通配符证书的验证—— DNS-01 challenge，即：通过向域名添加一条 TXT 记录来证明用户对域名的所有权。一般只要提供了 Token 访问的 DNS 服务商都可以支持，例如 GoDaddy、CloudFlare、DNSPod 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Google Spanner 事务在存储层的实现</title>
    <link href="https://ericfu.me/google-spanner-storage-impl/"/>
    <id>https://ericfu.me/google-spanner-storage-impl/</id>
    <published>2018-03-07T18:05:56.000Z</published>
    <updated>2018-04-13T02:10:32.065Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/03/banner-google-spanner.jpg" alt="banner-google-spanne"></p><p>Google Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。</p><p>不过，<strong>论文对存储层实现只作了模糊的阐述</strong>：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：</p><ul><li><strong>read-only 事务</strong>: 读取最新或给定时间戳 $t_{read}$ 的快照，也就是 snapshot read</li><li><strong>read-write 事务</strong>：读取事务开始时间戳 $t_{start}$ 的快照，而写入操作在提交时间戳 $t_{commit}$ 生效</li></ul><p>本文从 Spanner 本身设计出发，并结合开源实现 TiDB 和 CockroachDB，谈谈如何为 Spanner 设计一个存储层。本文假设读者阅读过原论文 <a href="https://research.google.com/archive/spanner.html" target="_blank" rel="external">Spanner: Google’s Globally-Distributed Database</a>。</p><a id="more"></a><h2 id="数据的-KV-表示"><a href="#数据的-KV-表示" class="headerlink" title="数据的 KV 表示"></a>数据的 KV 表示</h2><p>Spanner 对外提供（半）关系型数据模型：每张表定义了一个或多个主键列，以及其他的非主键列。这和我们熟知的 SQL 关系型模型几乎一摸一样，唯一的不同是 schema 定义中必须含有主键。</p><p>Spanner 早期的设计中大量复用了 BigTable（开源实现即 HBase）的代码。回忆一下 BigTable 的数据模型：每一条数据包含 <code>(Key, Column, Timestamp)</code> 三个维度，满足我们需要的 MVCC 特性。从 BigTable 开始的确是个不错的选择。</p><p>不过，从性能上考虑 Bigtable 毕竟是分布式的 KV 存储系统，在存储这一层我们大可不用搞的那么复杂，分布式的问题例如 scale-out 和 replication 应当留给上层的 sharding 机制和 Paxos 解决。事实上，一个单机的存储引擎足矣。</p><p>Google 自家的 LSM-Tree + SSTable 的实现 LevelDB 是个可选项。它接口非常简单，是一个标准的 KV 存储，可以方便的在它基础上实现我们想要的数据模型。主要接口其实就是两个：</p><ul><li><code>Write(WriteBatch *)</code> 原子地写入一个 WriteBatch，包含一组 <code>Put(K, V)</code> 和 <code>Delete(K)</code> 操作</li><li><code>Iterator()</code> 及 <code>Seek()</code> 从指定位置开始顺序扫描读取 (K, V) 数据</li></ul><p>如何实现列和时间戳呢？举个例子，有如下数据表 <code>Accounts</code>。在数据库中，主键索引通常也是唯一的聚簇索引，它存放了真实的数据，而我们暂时不考虑其他索引。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">| UserID (PK) | Balance | LastModified |</div><div class="line">|-------------|---------|--------------|</div><div class="line">| Alice       | 20      | 2018-02-20   |</div><div class="line">| Bob         | 10      | 2018-02-01   |</div></pre></td></tr></table></figure><p>Spanner 内部使用 MVCC 机制，所以还有一个隐藏的时间戳维度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">| UserID | Timestamp | Balance | LastModified |</div><div class="line">|--------|-----------|---------|--------------|</div><div class="line">| Alice  | 103       | 20      | 2018-02-20   |</div><div class="line">| Alice  | 101       | 15      | 2018-01-20   |</div><div class="line">| Bob    | 102       | 10      | 2018-02-01   |</div></pre></td></tr></table></figure><p>上述数据表用 KV 模型存储，可以表示为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">| Key                             | Value      |</div><div class="line">|---------------------------------|------------|</div><div class="line">| Accounts/Alice/Balance/103      | 20         |</div><div class="line">| Accounts/Alice/Balance/101      | 15         |</div><div class="line">| Accounts/Alice/LastModified/103 | 2018-02-20 |</div><div class="line">| Accounts/Alice/LastModified/101 | 2018-01-20 |</div><div class="line">| Accounts/Bob/Balance/102        | 10         |</div><div class="line">| Accounts/Bob/LastModified/102   | 2018-02-01 |</div></pre></td></tr></table></figure><p>上表中 <code>/</code> 表示一个分隔符，真实情况要更复杂。Key 这样编码：从左到右依次是表名（因为可以有不止一张表）、主键字段、列的标识符、时间戳（通常倒序排列，Tips. 取反即可）。Value 则对应原表中的数据。</p><p>显然，对于半关系型数据一定能由表名、主键字段、列名唯一地确定一个值，所以这个编码方式能满足我们的要求。</p><blockquote><p>如果一张表只有主键怎么办呢？这种情况下可以为每个主键填充一个 placeholder 的 value 即可。</p></blockquote><h2 id="事务的原子性"><a href="#事务的原子性" class="headerlink" title="事务的原子性"></a>事务的原子性</h2><p>众所周知，事务具有四个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。其中一致性和持久性其实是数据库系统的特性，对于事务，我们更多讨论的是<strong>原子性</strong>和<strong>隔离性</strong>。</p><p>对于存储层而言，为上层提供原子性 commit 的接口是必须的功能。如何在 KV 存储的基础上实现原子性呢？以下思路是一种常见的方案：</p><ol><li>首先，准备一个<strong>开关</strong>，初始状态为 off，当我们把开关打开的那一刻，意味着 commit 生效可见；</li><li>将所有变更以一种可回滚的方式（e.g. 不能覆盖现有的值）写入存储中。开关同时决定了其它 reader 的视图，由于开关还是 off 状态，现在写入的变更不会被其它事务看到。</li><li>之后，写入开关状态为 on，标志着 commit 的成功，新数据生效，即所谓 commit point。这个写入操作本身的原子性由 LevelDB 保证。</li><li>最后，清除掉中间状态（比如第 2 步中的临时数据）并写入最终的数据。这一步可以异步的完成，因为在第 3 步中事实上 commit 已经成功了，无需等待。</li></ol><p><img src="/images/2018/03/commit-4-steps.svg" alt="commit-4-steps"></p><p>保证原子性的关键在于 commit point。例如，在单机数据库中，commit point 是 commit 的 redo-log 写入磁盘的一瞬间；在 XA 两阶段提交中，commit point 是协调器将事务状态置为 Committed 的一瞬间。</p><p>在我们的存储中，commit point 也就是第 3 步的写入操作。如果提交过程意外终止在 commit point 之前，我们会在读取时发现第 2 步中的临时写入，然后轻松地清除它；如果意外终止在 commit point 之后，部分临时状态没有被清除，只需继续执行 4 即可。</p><p>上述只是一个解决问题的思路。具体的解决方案可以参考 Percolator 的事务实现。这同时也是 TiDB 的做法，CockroachDB 做法略有不同，但同样遵从这个模式。</p><h3 id="Percolator-事务方案"><a href="#Percolator-事务方案" class="headerlink" title="Percolator 事务方案"></a>Percolator 事务方案</h3><p>Percolator 是 Google 早期的分布式事务解决方案，用于进行大规模增量数据处理。Percolator 在 BigTable 基础上基于 2PL 思想实现了分布式事务。这个算法很简单，你可以把它看作是是封装了一系列 BigTable 的 API 访问（本身无状态），所以可以容易地移植到 KV 存储模型上。</p><p>Percolator 事务模型基于单调递增的时间戳，来源于集群中唯一的 timestamp oracle。每个事务拥有提交时间戳 $t_{commit}$ 和开始时间戳 $t_{start}$。Percolator 事务模型和之前说到的 write-read 事务一致：事务中总是读取 $t_{start}$ 时的 snapshot，而写入则全部在 $t_{commit}$ 生效。这也意味着事务中所有写入都被 buffer 到最后进行，不支持类似于 read-write-read 这样的模式。</p><p><img src="/images/2018/03/percolator-transaction-model-read-write.jpg" alt=""></p><p><em>如图，事务 2 看到的是事务 1 提交前的状态，而事务 3 看到的是事务 1、2 提交后的状态。</em></p><p>Percolator 基于 BigTable 的事务实现如下：</p><p>除了数据本身（bal:data 列）以外，我们给数据再加上两列：lock 和 write。</p><ul><li>write 列存放了一个<em>指针</em>，指向写入的 data 的时间戳</li><li>lock 列用于 2PL，加锁时也保存了 primary lock 的位置。</li></ul><p>primary lock 不仅代表当前行的锁状态，还兼任上文中“开关”的作用。通常选取第一个写入的数据作为 primary lock。</p><p>以下表为例。表中 <code>6: data @ 5</code> 表示：$ts=6$ 时事务提交，确定了 <code>Bob</code> 对应的值是 <code>5: $10</code>（所以推测出该事务 $t_{start}=5$）。其他事务读取时，为了避免读到 uncommitted 的数据，都会先从 write 列开始找，然后再读出其指向的 data。</p><p><img src="/images/2018/03/percolator-commit-1.jpg" alt=""></p><p>现在，用户要从 Bob 账户里转 \$7 给 Joe，为此必须开启一个事务。$ts = 7$ 时，转账事务开始，向 Bob 和 Joe 的 data 写入新的余额。</p><p>$ts = 8$ 时，用户 commit 事务。事务的第一阶段（Prewrite）亦即是 2PL 的加锁阶段，先为 Bob 和 Joe 都加上锁。如下图所示，lock 不为空即代表加上了锁，其内容指向 primary lock 的位置。简单起见，不妨设第一条被锁的数据为 primary row。</p><p><img src="/images/2018/03/percolator-commit-2.jpg" alt=""></p><p>下一步很关键：<strong>清除 primary row 的 lock 并向 write 列写入新 data 的位置</strong>。这也就是所谓 commit point，这个写入的成功或失败决定了事务提交成功与否：</p><ul><li>若写入成功，则代表整个事务成功。之后会遍历所有加锁的行，解除 lock 并向 write 列写入新的 data 位置。这样一来，其他事务就能读到当前事务写入的数据。</li><li>否则，整个事务失败。之后会遍历所有加锁的行，解除 lock 并清除之前写入的 data，恢复原状。</li></ul><p>回到例子中，当 commit point 完成后，表的状态如下：</p><p><img src="/images/2018/03/percolator-commit-3.jpg" alt=""></p><p>解除 Joe 的 lock 并向 write 列写入新 data 的位置，至此事务 commit 完成：</p><p><img src="/images/2018/03/percolator-commit-4.jpg" alt=""></p><p>Commit point 这一步本身的原子性由 BigTable 行事务保证。对于 commit point 前后的其他操作，如果系统当机重启，恢复线程可以通过检查 commit point 操作的结果，来确定该 roll forward 还是 roll back。具体而言：</p><ul><li>通过 lock 找到 primary lock，如果已经解除，说明 commit point 已经完成，需要 roll forward 事务。</li><li>否则，如果 primary lock 还在，说明 commit point 还没到，只能 roll back 事务。</li></ul><p>于是，通过 2PL，我们成功地在 BigTable 的行级事务基础上实现了表级事务。</p><p>上述过程很容易的能映射到 KV 存储模型上。按照前一节描述的方法，将 lock 和 write 列都视作普通的列即可。这里不再赘述。</p><h2 id="事务的隔离性"><a href="#事务的隔离性" class="headerlink" title="事务的隔离性"></a>事务的隔离性</h2><p>上述的讨论只考虑了单个事务的原子性保证——如何<strong>确保</strong>能从从中间状态恢复到未提交或已提交的状态，而没有考虑多线程并发的情况。如果同时有多个 client 在运行多个事务，如何保证严格互相隔离？（Serializable级别）</p><p>Percolator 是一个典型的 Snapshot Isolation 实现。Percolator 包含一个被称为 Strict-SI 的改进：在事务 commit 中，如果发现有一个高于 $t_{start}$ 的版本出现，则放弃 commit。这能避免 lost update 问题。但是 write-skew 问题依然存在。</p><p>Spanner 提供 Serializable 隔离性保证，其算法被称为 Serializable Snapshot Isolation (SSI)。</p><h3 id="冲突图理论"><a href="#冲突图理论" class="headerlink" title="冲突图理论"></a>冲突图理论</h3><p>首先对以上问题建模。考虑两个事务对同一条数据先后发生两次读或写操作，于是有 4 种情况：</p><ul><li>Read-Read：这是OK的，它不会引起冲突；</li><li>Read-Write：后发生的操作覆盖了前一个读的数据，这是一种冲突；</li><li>Write-Read：读到另一个事务的写入，这是一种冲突。</li><li>Write-Write：即覆盖写，这是一种冲突。</li></ul><p>上述三种冲突的情况，并不是一定会导致问题。举个例子：事务$T_2$仅仅是覆盖了事务$T_1$写入的数据，那么$T_1$和$T_2$仍然是符合 serializable 的，只要逻辑上认为$T_2$发生在$T_1$之后。</p><p>哪些情况会违反 serializable 呢？简单来说，如果冲突A迫使我们规定 $T_1$ 先于 $T_2$，冲突B迫使我们规定 $T_2$ 先于 $T_1$，这个因果关系就没法成立了，$T_1$、$T_2$无法以任何方式串行化。形式化的说：<strong>以所有事务 $T$ 作为节点、以所有冲突 $C$ 作为有向边构成一张有向图（这被称为冲突图或依赖图），如果这张图是有向无环图（DAG）则满足 serializable；否则（有环）不满足</strong>。</p><p>举个例子：</p><p>这是一个有向无环图，$T_1$、$T_2$、$T_3$ 满足 serializable。</p><p><img src="/images/2018/03/serializable-1.jpg" alt=""></p><p>这是一个有环的图，$T_1$、$T_2$、$T_3$ 无法被串行化。</p><p><img src="/images/2018/03/serializable-2.jpg" alt=""></p><p>图论告诉我们，如果一张图是 DAG，<strong>等价于</strong>我们能为它进行拓扑排序，即给每个节点 assign 一个编号，使得所有边都是从编号小的节点指向编号大的。换而言之，<strong>如果我们能给每个节点 assign 一个这样的编号，则可以反推出原图是 DAG，进而证明 T 集合满足 serializable</strong>。</p><blockquote><p>你可能已经隐约感觉到，这个编号和事务发生的顺序有关！事实上，编号代表 serializable 后的逻辑顺序，大多数时候，这个顺序和真实的时间顺序都是一致的。</p><p>Spanner 中强调自己满足的是比 serializable 更强的一致性：linearizable，说的就是不仅能序列化，而且序列化的“逻辑顺序”和时间上的“物理顺序”也一致。</p></blockquote><h3 id="Serializable-Snapshot-Isolation-SSI"><a href="#Serializable-Snapshot-Isolation-SSI" class="headerlink" title="Serializable Snapshot Isolation (SSI)"></a>Serializable Snapshot Isolation (SSI)</h3><p>不妨把事务开始的时间戳 $t_{start}$ 作为这个编号。将上述约束条件略微加强一些，就得到了简单有效的判断法则：<strong>对于冲突 $T_1 \rightarrow T_2$，如果时间戳满足 $t_1 &lt; t_2$ 则允许发生；如果 $t_1 &gt; t_2$ 则终止事务。</strong></p><p>具体的来说，对于三种冲突，分别用以下方式处理：</p><ul><li><p><strong>Write-Read 冲突</strong>：感谢 MVCC，这是不会发生的，在 Percolator 的事务模型中，读操作一定是从一个过去时间点的 snapshot 上读取，而不会读到一个正在进行中事务的脏数据。（但是 MVCC 会引发另一个问题——staled read。见下文）</p></li><li><p><strong>Write-Write 冲突</strong>：如果 Write 发生的时候，出现了一条 $t_{start}$ 比较大的记录，则终止写事务。</p></li></ul><blockquote><p>Percolator 的 SI 实现使用了更强的约束：如果出现另一条比开始时间大的记录，无论其时间戳如何都会终止当前提交，这与 SSI 的机制有所区别。</p><p>由于 SI 无法完全避免 Read-Write 冲突（例如 write-skew 问题），所以在 Write-Write 冲突的处理上更为激进；但 SSI 已经解决了 Read-Write 冲突检测，不必用更强的约束。</p></blockquote><ul><li><strong>Read-Write 冲突</strong>：为了知道 Write 和另一个事务的 Read 冲突，必须要以某种方式记录下所有被读过的数据、以及读取事务的 $t_{start}$。这通常用范围锁（range lock）来实现——将所有查询的 TableScan 范围记录在内存中，如果某一条写入的数据满足某个 where 条件，则有必要检查一下二者的时间戳先后顺序。如果不满足上述判断法则，需要终止写事务。</li></ul><p><img src="/images/2018/03/read-write-conflict-1.jpg" alt=""></p><ul><li>由于 MVCC 的存在，<strong>Read-Write 冲突</strong>还有另一种形式：$T_2$ 的 Read 发生地更迟，但是由于 MVCC 它读到的是 $T_1$ 写之前的值（staled read），而且这里 $T_1 $ 先于 $ T_2$ 从而构成 Read-Write 冲突。</li></ul><p><img src="/images/2018/03/read-write-conflict-2.jpg" alt=""></p><p>对此，一个简单的解决方案是：如果 $T_2$ 发现 $T_1$ 写入的中间数据（lock），则立即终止自己。经典 SSI 的做法是，在 $T_2$ commit 时如果发现 $T_1$ 已经 commit 则放弃本次提交。</p><p>综上，通过给每个事务赋予一个时间戳，并保证每个冲突都符合时间戳顺序，达到 serializable 隔离级别。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>将 <code>(Table, Key, Column, Timestamp)</code> 作为 Key 的编码，从而把（半）关系型数据存储在 KV 引擎中；</li><li>用两阶段锁（2PL）的方式在 KV 引擎上实现事务的原子性提交。</li><li>禁止冲突违反时间戳先后顺序，从而保证 serializable 的隔离性。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://research.google.com/archive/spanner.html" target="_blank" rel="external">Spanner: Google’s Globally-Distributed Database (OSDI’12)</a></li><li><a href="https://research.google.com/pubs/pub36726.html" target="_blank" rel="external">Large-scale Incremental Processing Using Distributed Transactions and Notifications - USENIX 2010 - Daniel Peng, Frank Dabek</a></li><li><a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/" target="_blank" rel="external">How CockroachDB Does Distributed, Atomic Transactions - Cockroach Labs</a></li><li><a href="https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/" target="_blank" rel="external">Serializable, Lockless, Distributed: Isolation in CockroachDB - Cockroach Labs</a></li><li>Designing Data‑Intensive Applications - Martin Kleppmann</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/03/banner-google-spanner.jpg&quot; alt=&quot;banner-google-spanne&quot;&gt;&lt;/p&gt;
&lt;p&gt;Google Spanner 的论文于 2012 年发表，至今仍是世界上最先进的、规模最大的分布式数据库架构，毫无疑问对现代数据库设计产生了深远影响。其最大的亮点莫过于 TrueTime API，凭借原子钟和 GPS 的加持在全球范围实现了单调递增的时间戳，从而达到外部一致性；其次则是验证了分布式 MVCC 的高性能实现，为业界指明一条发展方向。&lt;/p&gt;
&lt;p&gt;不过，&lt;strong&gt;论文对存储层实现只作了模糊的阐述&lt;/strong&gt;：原文中说到 tablet 的实现类似于 Bigtable（复用了不少 Bigtable 的代码），底层基于 Colossus —— 继承 GFS 的下一代分布式文件系统。可以确定的一点是，存储层要为 read-only 和 read-write 事务提供支持：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;read-only 事务&lt;/strong&gt;: 读取最新或给定时间戳 $t_{read}$ 的快照，也就是 snapshot read&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read-write 事务&lt;/strong&gt;：读取事务开始时间戳 $t_{start}$ 的快照，而写入操作在提交时间戳 $t_{commit}$ 生效&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文从 Spanner 本身设计出发，并结合开源实现 TiDB 和 CockroachDB，谈谈如何为 Spanner 设计一个存储层。本文假设读者阅读过原论文 &lt;a href=&quot;https://research.google.com/archive/spanner.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Spanner: Google’s Globally-Distributed Database&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>《指数基金投资指南》读书笔记</title>
    <link href="https://ericfu.me/index-fund-guide-notes/"/>
    <id>https://ericfu.me/index-fund-guide-notes/</id>
    <published>2018-02-04T16:08:14.000Z</published>
    <updated>2018-03-16T02:50:24.546Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2018/02/index-fund-guide-book.jpg" alt=""></p><p>最近程序员圈子悄悄流行起斯坦福的 <a href="https://cs007.blog/" target="_blank" rel="external">CS 007: Personal Finance For Engineers</a> 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。</p><p>在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。</p><a id="more"></a><h2 id="指数基金的分类"><a href="#指数基金的分类" class="headerlink" title="指数基金的分类"></a>指数基金的分类</h2><p>从投资范围来看，可以分为宽基指数和行业指数。</p><h3 id="宽基指数"><a href="#宽基指数" class="headerlink" title="宽基指数"></a>宽基指数</h3><h4 id="国内市场指数"><a href="#国内市场指数" class="headerlink" title="国内市场指数"></a>国内市场指数</h4><table><thead><tr><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr><td>上证 50</td><td>沪市规模最大的 50 支股票</td><td>大盘蓝筹</td></tr><tr><td>沪深 300</td><td>沪市和深市规模最大的 300 支股票</td><td>大盘蓝筹，最有代表性</td></tr><tr><td>中证 500</td><td>除了沪深 300 之外最大的 500 支股票</td><td>中小型企业</td></tr><tr><td>创业板指数</td><td>创业板规模最大的 100 支股票</td><td>小型企业</td></tr><tr><td>上证红利</td><td>沪市股息率最高的 50 支股票</td><td>大盘股为主</td></tr><tr><td>中证红利</td><td>沪市和深市股息率最高的 100 支股票</td><td>大盘股为主</td></tr><tr><td>红利机会</td><td>过去 3 年盈利增长、1 年净利润为正等限制下股息率前 100 支</td><td>优化筛选</td></tr></tbody></table><h4 id="国外市场指数"><a href="#国外市场指数" class="headerlink" title="国外市场指数"></a>国外市场指数</h4><table><thead><tr><th>名称</th><th>选取标准</th><th>特点</th></tr></thead><tbody><tr><td>恒生指数</td><td>香港股市规模最大的 50 支股票</td><td>投资港股</td></tr><tr><td>上证 50AH 优选</td><td>27 支 A 股 + 23 支 A/H 股中便宜的一方</td><td>优化上证 50</td></tr><tr><td>纳斯达克 100</td><td>NASDAQ 规模最大的的 100 支股票</td><td>美股，互联网科技股</td></tr><tr><td>标普 500</td><td>美股中各行业领导者</td><td>美股大盘蓝筹</td></tr></tbody></table><h3 id="行业指数"><a href="#行业指数" class="headerlink" title="行业指数"></a>行业指数</h3><table><thead><tr><th>行业</th><th>推荐理由</th></tr></thead><tbody><tr><td>消费</td><td>需求稳定</td></tr><tr><td>医疗</td><td>需求稳定，我国老龄化利好</td></tr><tr><td>可选消费</td><td>消费升级利好</td></tr><tr><td>养老</td><td>老龄化利好</td></tr><tr><td>金融</td><td>牛市上涨多</td></tr></tbody></table><h2 id="如何挑选指数基金"><a href="#如何挑选指数基金" class="headerlink" title="如何挑选指数基金"></a>如何挑选指数基金</h2><h3 id="格雷厄姆的理论"><a href="#格雷厄姆的理论" class="headerlink" title="格雷厄姆的理论"></a>格雷厄姆的理论</h3><p>格雷厄姆对价值投资总结出三个重要的理论：</p><ul><li><strong>价格与价值的关系。</strong>股票价格围绕其内在价值上下波动，但长期来看是趋于一致的。</li><li><strong>能力圈。</strong>只投资自己了解的品种，只有具备能力才能判断出内在价值。</li><li><strong>安全边际。</strong>只有当股价大幅低于内在价值的时候，我们才会买入。</li></ul><h3 id="常用估值指标"><a href="#常用估值指标" class="headerlink" title="常用估值指标"></a>常用估值指标</h3><h4 id="市盈率"><a href="#市盈率" class="headerlink" title="市盈率"></a>市盈率</h4><p>$$<br>市盈率 = 市值 / 盈利<br>$$</p><p>$$<br>PE = P / E<br>$$</p><p>市盈率反应了我们愿意每 1 元的利润付出多少代价。</p><p>市盈率适用于流通性好、盈利稳定的品种。</p><h4 id="盈利收益率"><a href="#盈利收益率" class="headerlink" title="盈利收益率"></a>盈利收益率</h4><p>$$<br>盈利收益率 = 盈利 / 市值<br>$$</p><p>盈利收益率是市盈率的倒数。适用条件同上。</p><h4 id="市净率"><a href="#市净率" class="headerlink" title="市净率"></a>市净率</h4><p>$$<br>市净率 = 市值 / 净资产<br>$$</p><p>$$<br>PB = P / B<br>$$</p><p>净资产等于资产减负债，净资产相对盈利比较稳定。</p><p>当企业的资产大多是比较容易衡量价值的有形资产，并且是长期保值的资产时，比较适合用市净率估值。</p><h4 id="股息率"><a href="#股息率" class="headerlink" title="股息率"></a>股息率</h4><p>股息率是过去一年的现金派息额除以公司总市值。</p><h3 id="盈利收益率法"><a href="#盈利收益率法" class="headerlink" title="盈利收益率法"></a>盈利收益率法</h3><ol><li>当盈利收益率大于 10% 时，开始定投；</li><li>当盈利收益率低于 10% 大于 6.4% 时，停止定投并持有已有份额。</li><li>当盈利收益率低于 6.4% 时，分批卖出。</li></ol><p>适用条件比较苛刻，只适用于流通性比较好、盈利比较稳定的品种，不适用于增长速度快、或盈利波动比较大的品种。例如：上证 50 指数、上证红利指数、恒生指数、H 股指数等。</p><h3 id="博格公式法"><a href="#博格公式法" class="headerlink" title="博格公式法"></a>博格公式法</h3><p>指数基金未来的年复合收益率由三个部分构成：</p><ul><li>初始股息率</li><li>未来每年的市盈率变化率</li><li>未来每年的盈利变化率</li></ul><p>分析一下：</p><ul><li>初始股息率在买入时就确定了</li><li>市盈率在某个范围内呈现周期性变化</li><li>盈利长期来看是会上涨的</li></ul><p>所以，我们的策略是：</p><ol><li>在市盈率处于历史较低位置时定投买入；</li><li>等市盈率回归正常估值，暂停定投，继续持有；</li><li>当市盈率进入历史较高区域时卖出。</li></ol><p>适用于盈利增长较快的品种、波动较大的品种，例如：沪深 300 指数、中证 500 指数、创业板指数、红利机会指数、消费行业指数、医药行业指数、养老行业指数等。</p><h3 id="博格公式变种"><a href="#博格公式变种" class="headerlink" title="博格公式变种"></a>博格公式变种</h3><p>对于盈利波动大的行业，用市净率代替市盈率。</p><p>在市净率率处于历史较低位置时买入。买入后，等待市净率回归正常即可。</p><p>适用于周期性较强、盈利不稳定的行业，例如：证券行业指数、非银金融行业指数、地产行业指数等。</p><h2 id="定投收益"><a href="#定投收益" class="headerlink" title="定投收益"></a>定投收益</h2><p>提高定投收益的技巧：</p><ol><li><p>降低交易基金的费用。</p></li><li><p>正确处理分红：相当于一笔现金收入，投入到相对低估的指数基金中。</p></li><li><p>频率选择：按周定投收益更稳定，但长期来看差距很小，按个人习惯选择即可。</p></li><li><p>定期不定额：<br>$$每个月定投金额 = \left( \frac{当月的盈利收益率}{首次的盈利收益率} \right) ^ n$$<br>$n$ 是放大倍数，建议 $n=1$。</p></li><li><p>定期不定额（博格公式法）<br>$$每个月定投金额 = \left( \frac{首次的市盈率}{当月的市盈率} \right) ^ n$$</p></li></ol><h2 id="制定定投计划"><a href="#制定定投计划" class="headerlink" title="制定定投计划"></a>制定定投计划</h2><ol><li>梳理现金流，确定每月用于定投的数额；</li><li>选择适合定投的指数基金；</li><li>选择每月／每周的某一天为定投日，按之前的方法制定策略；</li><li>做好定投记录。</li></ol><p>定投记录表示例：</p><table><thead><tr><th>日期</th><th>操作</th><th>交易品种</th><th>金额</th><th>成交单价</th><th>成交份额</th><th>当前估值</th></tr></thead><tbody><tr><td>2017-05-05</td><td>买入</td><td>501029 红利指数</td><td>750</td><td>0.994</td><td>753</td><td>12.64</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2018/02/index-fund-guide-book.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近程序员圈子悄悄流行起斯坦福的 &lt;a href=&quot;https://cs007.blog/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CS 007: Personal Finance For Engineers&lt;/a&gt; 课程，每个人都应该有所了解。但在如何投资的问题上，课程并没有给出适合我国国情的操作建议。&lt;/p&gt;
&lt;p&gt;在雪球潜水多年，其中有位大 V 近期出版了适合所有人的《指数基金投资指南》，读后收益良多，故将读书笔记分享到这里，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
  </entry>
  
  <entry>
    <title>Learned Index Structures 论文解读</title>
    <link href="https://ericfu.me/learned-index-paper-notes/"/>
    <id>https://ericfu.me/learned-index-paper-notes/</id>
    <published>2017-12-23T09:12:44.000Z</published>
    <updated>2018-03-16T02:49:05.735Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/12/learned-index-cover.jpg" alt=""></p><p>数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。</p><p>这些事，似乎拿预测模型都是可以做的。Yes, but…B 树那是精确的映射关系，和预测模型能一样吗？</p><p>所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。</p><a id="more"></a><h2 id="范围索引"><a href="#范围索引" class="headerlink" title="范围索引"></a>范围索引</h2><p>B 树和一个预测 model 是很相似的：</p><ul><li>B 树能定位某行数据所在的 page，可以看作是确定了一个区间：[pos, pos + pagesize]</li><li>预测模型也能做相似的事，假设我们把预测错误率成功 bound 在 min/max_err 以内，那么也就可以确定，数据位于区间 [pos - min_err, pos + max_err]</li></ul><p>一图以概之：</p><p><img src="/images/2017/12/figure-1.jpg" alt=""></p><p>于是问题变成，<strong>如何把预测错误率 bound 在 max_err 以内</strong>？</p><p>答案非常简单！通过训练。你可以把训练集的上的 error 降到多小都行，只要你模型的表现力足够强。这个问题和绝大多数机器学习的问题都不同，<strong>我们只要照顾好训练集（也就是被索引的数据）就可以了，没有测试集！</strong>当然也就不会有过拟合，模型的泛化能力是不用考虑的，比想象的简单吧！</p><p>说到模型表现力强，很容易想到神经网络。除此以外，NN 还带来了另一个好处，在 GPU（或其他专用芯片，比如 Google 的 TPU）上，NN 能获得惊人的计算速度。NN 的结构决定了它并行起来非常快，时间复杂度上把 $O(\log{n})$ 的 B 树等甩在身后。</p><p>说到这里，我们来做个思维拓展：如果把 key 作为横轴，key 在有序数组中的位置 pos 作为纵轴，画出目标函数的曲线，那应该长成这个样子：</p><p><img src="/images/2017/12/figure-2.jpg" alt=""></p><p>这是一个 CDF (累积分布函数，累积的是各个 key 对应数据的长度）。从这个角度看，无论是 B 树还是预测模型都是在拟合这个函数，只是 approach 完全不同。</p><p>这时候再回头看我们熟悉的 B 树，有没有一点顿悟—— Aha! 这不是就是决策树吗？</p><h2 id="递归模型索引-RM-Index"><a href="#递归模型索引-RM-Index" class="headerlink" title="递归模型索引 RM-Index"></a>递归模型索引 RM-Index</h2><p>以上已经说完了核心思想，接下来就是要找到一个合适的预测模型来代替 B 树。实验发现，直接上 DNN 效果并不好：单次计算代价太大，只能用 GPU（而调用 GPU 会产生不小的 overhead）；而且网络很庞大，retrain（增删改）代价很大。</p><p>Naive 的预测模型之所以做的不好，一个重要原因是：把如此大量的数据每条误差 bound 在 min/max_err 之内，实在太难了（所谓 <em>last mile</em> 问题）。为解决这个问题，决策树给我们做了个很好的提示，<strong>如果一个模型解决不了问题，就再加几层</strong>。</p><p>举个例子：为 100M 记录训练一个足够精确的预测器太难，那就分成 3 层树状结构。根节点分类器把记录分出 100 份，每份大约有 1M 记录；第二层再分出 100 份，每份大约只剩 10K 记录；第三层再分出 100 份，每份大约有 100 条记录——假设 100 条纪录足够把误差在 min/max_err 之内。</p><p><img src="/images/2017/12/figure-3.jpg" alt=""></p><p><em>注意，上图并不是一棵树，例如 Model 2.1 和 2.2 都可以选择 Model 3.2 作为下一层的分类模型。</em></p><p>这样做的好处是，每层要做的事情简单多了（每层 precision gain = 100），模型可以变得简单得多。每个 NN 模型就像一个精通自己领域的专家，他只要学习某个很小子集的 keys 就可以了。这也同时解决了 <em>last mile</em> 难题，大不了为这一百左右个 keys 过拟合一下也无妨。</p><h2 id="混合索引"><a href="#混合索引" class="headerlink" title="混合索引"></a>混合索引</h2><p>上图中的三层网络结构还带来一个额外的优势：每个 Model 其实是独立的，我们可以用除了 NN 以外的预测方法代替之，包括线性回归等简单算法，甚至是 B 树。</p><p>事实上，最后选用了两种 Model：</p><ul><li>简单的神经网络（0～2 层全连接的 hidden layer，ReLU 激发函数，每层最多 32 个神经元）</li><li>当叶节点的 NN 模型 error rate 超过阈值时，替换成 B 树</li></ul><p>训练算法如下，</p><p><img src="/images/2017/12/algorithm-1.jpg" alt=""></p><p>简单说就是：</p><ol><li>固定整个 RM-Index 的结构，比如层数、每层 Model 数量等（可以用网格法调参）；</li><li>用全部数据训练根节点，然后用根节点分类后的数据训练第二层模型，再用第二层分类后的数据训练第三层；</li><li>对于第三层（叶节点），如果 max_error 大于预设的阈值，就换成 B 树。</li></ol><p>有了 Step 3，这个 RM-Index 的分类能力也就有了下界，即使面对纯噪声数据（毫无规律可循），至少能和 B 树保持差不多的性能。</p><blockquote><p>索引的 keys 常常是字符串，而我们前文说的 NN 模型的输入是向量。Luckily，字符串向量化是 ML 里研究很多的一个课题了，这里不再讨论，有兴趣的可以看下原论文（抛砖引玉为主）。</p></blockquote><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><p>为了对比 RM-Index 和 B 树的性能，论文作者找了 4 个数据集，分别用 RM-Index 和 B 树作二级索引。</p><ul><li>Weblogs 数据集：访问时间 timestamp -&gt; log entry （约 200M）</li><li>Maps 数据集：纬度 longitude -&gt; locations （约 200M）</li><li>Web-documents 数据集：documents（字符串）-&gt; document-id（约 10M）</li><li>Lognormal 数据集：按对数正态分布随机生成的数据</li></ul><p>测试中用了不同参数的 Learned Index 和 B 树，B 树也用了一个高度优化的实现。</p><p>总体来说，Size savings 都相当可观（下降 1～2 个数量级），而速度也有所优化，最多能快 1 倍。</p><p><img src="/images/2017/12/figure-5.jpg" alt=""></p><blockquote><p>对于每个数据集，论文都给出了详细的测试结果，有兴趣的同学请看原文。</p></blockquote><p>可以说是符合预期的，个人看法是：</p><ul><li>因为算法 Step 3 的帮助，即使不经过调优，性能至少不输 B 树；</li><li>肯定能省下许多空间，因为 B 树是基于比较的查找，叶结点要保存 key 的内容，key 越多索引越大；而 NN 完全不受这个制约。</li></ul><p>但暂且不要太激动，这只是查找性能。索引的维护（增／删／改）代价如何也是要考虑的。用作者原话说，这是 learned index 的阿喀琉斯之踵，因为 NN 模型的 retrain 代价是不可预测的，这是多数 ML 算法和传统算法一个很大的不同点。对此，作者意见如下：</p><ul><li>如果恰好新的数据已经能被成功预测了，那就不用 retrain 了；但这太理想化，我们为达到 <em>last mile</em> 做的那些 overfitting 也导致了这个模型泛化性堪忧。</li><li>如果一定要 retrain，一个简单有效的优化是：把变更数据累积起来先放着，批量训练；</li><li>此外，retrain 可以借鉴一些 warm-start 的方法加快训练过程。</li></ul><h2 id="其他索引结构"><a href="#其他索引结构" class="headerlink" title="其他索引结构"></a>其他索引结构</h2><p>论文中这部分没有详细展开，因为原理和前文几乎没有区别，仅仅换一种用法。</p><h3 id="Point-Index"><a href="#Point-Index" class="headerlink" title="Point Index"></a>Point Index</h3><p>拳打完 B 树再来脚踢 hash-map。大家都知道 hash-map 兼具 $O(1)$ 的高效率和低效的空间使用率，想快就要减少碰撞，于是要牺牲更多的空间。所谓空间换时间。即使是 Google 的 Dense-hashmap 也会有 78% 的 overhead。</p><p>解决方案如图所示，用 RM-Index 模型替换掉 hash function。其思想是，利用数据的某些内在特征，帮助它找到一个最均匀（uniform）的映射方式，而不是用哈希彻底随机化。</p><p><img src="/images/2017/12/figure-9.jpg" alt=""></p><p>在三个数据集上的测试表明，这一方法的确提升了 slot 的空间利用率，减少了很多空 slot，减少的比例约 -6% ~ -78%。</p><h3 id="Existence-Index"><a href="#Existence-Index" class="headerlink" title="Existence Index"></a>Existence Index</h3><p>这回轮到的是 bloom filter，有两种思路：</p><ol><li>直接用一个二分类模型判断是否存在；</li><li>和上一小节的原理类似，把 hash 函数替换成 RMI 模型。</li></ol><p><img src="/images/2017/12/figure-12.jpg" alt=""></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>在 Jeff Dean 大神的光环之下，这篇文章很快引起热烈的讨论。</p><p>不得不说，这个脑洞开的很大，令人为之一振。一直在人们心中只能做“模糊”预测的 ML 算法竟然能代替经典的 B 树，放在 10 年前估计会被喷到体无完肤。</p><p>论文的亮点在于，大家一直在“训练——预测”这样一个思维下作 ML 而忽视了一点：至少对于已知的数据，ML 算法也是能输出一个确定的结果的！换句话说，<strong>在全集上训练，把错误率强行 bound 住其实很容易</strong>。</p><p>下面说说优缺点。</p><p>Learned index 对于规律性强的数据是大杀器，作这种数据的二级索引再合适不过了。<strong>内在规律越强，就意味着 B 树、哈希这些通用算法浪费的越多，这也是 ML 算法能捡到便宜的地方</strong>。</p><p>就像很多 DBMS 引入全文索引一样，未来的 DBMS 也也可以尝试给用户更多其他选项，为某些特别有规律的 column 建立非 B 树／Hash 的二级索引。甚至可以让 DMBS 智能化，自己尝试寻找一些规律，将 B 树索引透明的替换成其他索引。</p><p>缺点也是明显的：<strong>增删改代价难以控制</strong>，可想而知，对于规律性不那么明显的数据，这足以抹平它查找的速度优势。但我相信之后会有更多改进的 index 模型出现，把这个代价尽可能减少。</p><p>一句话总结个人看法：</p><p><strong>B 树作为通用索引的地位仍然难以撼动，但特定数据场景下，learned index 将成为一个有力的补充。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/12/learned-index-cover.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;数据库的索引和机器学习里的预测模型其实有一些相似之处，比如 B 树是把 key 映射到一个有序数组中的某个位置，Hash 索引是把 key 映射到一个无序数组中的某个位置，bitmap 是把 key 映射成是一个布尔值（存在与否）。&lt;/p&gt;
&lt;p&gt;这些事，似乎拿预测模型都是可以做的。Yes, but…B 树那是精确的映射关系，和预测模型能一样吗？&lt;/p&gt;
&lt;p&gt;所以这就是本论文 NB 的地方了，以上的想法是可以实现的。实验表明，在某些数据集上，用 RM-Index 预测模型代替 B 树之类的数据结构，可以提升 70% 的速度、并节约相当可观的空间。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>Sharding-JDBC 的事务处理小结</title>
    <link href="https://ericfu.me/sharding-jdbc-transaction-overview/"/>
    <id>https://ericfu.me/sharding-jdbc-transaction-overview/</id>
    <published>2017-12-13T02:17:13.000Z</published>
    <updated>2017-12-13T02:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。</p><p>本文主要介绍 Sharding-JDBC 的事务处理。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><blockquote><p>Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。</p></blockquote><ul><li>默认使用 “弱XA” 事务</li><li>可选使用柔性事务：<ul><li>BED（最大努力送达型）事务</li><li>TCC（补偿型）事务</li></ul></li></ul><a id="more"></a><h2 id="“弱XA”事务"><a href="#“弱XA”事务" class="headerlink" title="“弱XA”事务"></a>“弱XA”事务</h2><p>之所以加引号，是因为这个和 MySQL 的 XA 其实没有关系。</p><p>它的实现是很自然的：</p><ul><li><strong>Prepare 阶段</strong>（执行 SQL）过程遇到异常，则中止当前事务，对所有分库上的事务连接执行 <code>conn.rollback()</code></li><li><strong>Commit 阶段</strong>（提交事务）时对所有分库依次（其实可以并行）做 <code>conn.commit()</code>；如果某个事务连接 commit 时抛出异常，由于无法回滚其他连接，所以仅仅是收集起来报给调用者，交由用户处理。不影响其他分库 commit。</li></ul><p>文档中对使用注意事项也写的很明确：</p><blockquote><p>不支持因网络、硬件异常导致的跨库事务。例如：同一事务中，跨两个库更新，更新完毕后、未提交之前，第一个库死机，则只有第二个库数据提交。</p></blockquote><h2 id="柔性事务"><a href="#柔性事务" class="headerlink" title="柔性事务"></a>柔性事务</h2><p>分布式场景下传统数据库 ACID 无法满足业务的性能要求，所以诞生了 BASE 理论。BASE是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写，其中<em>软状态</em>是指允许系统中的数据存在中间状态。</p><p>Sharding-JDBC 的柔性事务是需要调用者显式开启的。以 BED 事务为例，客户需要先通过柔性事务管理器创建出 <em>BEDSoftTransaction</em> 对象，然后使用 <code>begin()</code> 开始柔性事务。</p><h2 id="BED（最大努力送达型）事务"><a href="#BED（最大努力送达型）事务" class="headerlink" title="BED（最大努力送达型）事务"></a>BED（最大努力送达型）事务</h2><p>用户保证该数据库的操作最终一定可以成功，所以通过最大努力<strong>反复尝试</strong>。</p><p><img src="media/15130580762465/15130630328613.png" alt=""></p><p>结合上图，执行过程可以分为 4 种情况：</p><ol><li>同步执行成功</li><li>同步执行失败，同步重试成功</li><li>同步执行失败，同步重试失败，异步重试成功</li><li>同步执行失败，同步重试失败，异步重试失败，事务日志保留（人工介入处理）</li></ol><p>BED 不保证 exactly once，所以使用 BED 的 SQL 需要满足幂等性，例如不能用 <code>UPDATE SET x = x + 1</code> 这样的 SQL。</p><h2 id="TCC（补偿型）事务"><a href="#TCC（补偿型）事务" class="headerlink" title="TCC（补偿型）事务"></a>TCC（补偿型）事务</h2><p>目前还在规划中，没有实现。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://shardingjdbc.io/docs/02-guide/transaction/" target="_blank" rel="external">Sharding-JDBC 文档 - 事务支持</a></li><li><a href="http://blog.csdn.net/yanyan19880509/article/details/78335935" target="_blank" rel="external">sharding-jdbc 事务解读</a></li><li><a href="http://www.iocoder.cn/Sharding-JDBC/transaction-bed/" target="_blank" rel="external">Sharding-JDBC 源码分析 —— 分布式事务（一）之最大努力型</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sharding-JDBC 是由当当网推出的一款开源的分布式数据库中间件。它以 JDBC 的形式嵌入到应用程序中，无需额外部署。Sharding-JDBC 实现了分库分表、读写分离和分布式主键功能，并初步实现了柔性事务。&lt;/p&gt;
&lt;p&gt;本文主要介绍 Sharding-JDBC 的事务处理。&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Sharding-JDBC 由于性能方面的考量，决定不支持强一致性分布式事务。我们已明确规划线路图，未来会支持最终一致性的柔性事务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;默认使用 “弱XA” 事务&lt;/li&gt;
&lt;li&gt;可选使用柔性事务：&lt;ul&gt;
&lt;li&gt;BED（最大努力送达型）事务&lt;/li&gt;
&lt;li&gt;TCC（补偿型）事务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="database" scheme="https://ericfu.me/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>博客从 Ghost 迁移到 Hexo</title>
    <link href="https://ericfu.me/move-blog-from-ghost-to-hexo/"/>
    <id>https://ericfu.me/move-blog-from-ghost-to-hexo/</id>
    <published>2017-10-24T11:10:17.000Z</published>
    <updated>2017-10-29T11:38:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>不知道是不是我的错觉，<a href="https://ghost.org/" target="_blank" rel="external">Ghost</a> 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。</p><p>当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 <a href="http://zh.mweb.im/" target="_blank" rel="external">MWeb 编辑器</a> 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）</p><p>下面说正经的。</p><a id="more"></a><h2 id="Pros-Hexo-有哪里好？"><a href="#Pros-Hexo-有哪里好？" class="headerlink" title="Pros: Hexo 有哪里好？"></a>Pros: Hexo 有哪里好？</h2><ul><li>Markdown 格式，方便本地保存以及迁移</li><li>社区活跃，使用问题基本都能 google 到答案</li><li>Hexo + NexT 主题基本不用折腾就能满足我的所有要求，比如自带 MathJax</li><li>最近墙变高了，很容易迁移到国内的静态页面托管平台（比如 coding.net）</li></ul><h2 id="Cons-迁移代价"><a href="#Cons-迁移代价" class="headerlink" title="Cons: 迁移代价"></a>Cons: 迁移代价</h2><ul><li>因为之前的用了 <a href="https://posativ.org/isso/" target="_blank" rel="external">isso 开源评论系统</a>懒得迁过来了，丢失一堆评论</li><li>你可能因此浪费一个周末。</li></ul><h2 id="从-Ghost-导出内容"><a href="#从-Ghost-导出内容" class="headerlink" title="从 Ghost 导出内容"></a>从 Ghost 导出内容</h2><p>Ghost 一直很良心地在实验室页面保留了一个 <code>export</code> 按钮，导出后是一个巨大的 JSON，包含所有文章以及一些元数据：修改日期、Tags 等等。</p><p>但是 Ghost 1.X 开始启动了一个<a href="https://github.com/bustle/mobiledoc-kit/blob/master/MOBILEDOC.md" target="_blank" rel="external">叫 <code>mobiledoc</code> 的文档格式</a>。突然得知这一消息的我是一脸懵逼的，说好的 Markdown 呢？根据<a href="https://github.com/TryGhost/Ghost/issues/7429" target="_blank" rel="external">这个 Issue</a>所述：</p><blockquote><p>你知道的，我们现在的编辑器非常烂，不好用，还有一堆 Bug!</p><p>所以我们决定 Mobiledoc 重新做一个！Mobiledoc 很棒，mobiledoc 就是未来！</p></blockquote><p>Emmmm…… 好吧，你开心就好！</p><p>后果就是无论是 Ghost 导出的 JSON 还是数据库，都只有 mobiledoc 文档而没有 Markdown。所幸的是，Ghost 产生的 mobiledoc 也很奇葩，是把 Markdown 强行塞进去了。</p><p>后面会在导入时处理这个问题，先点实验室里的 <code>export</code> 拿到 JSON。</p><p>对于图片等资源，我们到 <code>assets</code> 文件夹下，打包下载下来即可。</p><h2 id="将内容导入-Hexo"><a href="#将内容导入-Hexo" class="headerlink" title="将内容导入 Hexo"></a>将内容导入 Hexo</h2><p>根据 <a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="external">Hexo 官方的安装指引</a>在本地装好 Hexo。</p><p>时间充裕的话，可以顺手把 <a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external">NexT 主题</a>也给装了。</p><h3 id="导入文章-Markdown"><a href="#导入文章-Markdown" class="headerlink" title="导入文章 Markdown"></a>导入文章 Markdown</h3><p>之后我们开始导入文章。Hexo 是留了 migrator 插件接口的，GitHub 上能找到一个名为 hexo-migrator-ghost 的插件，但是已经年久失修。于是我帮他修了一下 bug，其中也包括从 mobiledoc 中解出 Markdown<br>代码。</p><p>你可以直接把<a href="https://github.com/fuyufjh/hexo-migrator-ghost" target="_blank" rel="external">我的 Repo</a> <code>git clone</code> 到 node_modules 里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/fuyufjh/hexo-migrator-ghost.git ./node_modules/hexo-migrator-ghost</div></pre></td></tr></table></figure><p>然后运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo migrate ghost ghost-export.json</div></pre></td></tr></table></figure><h3 id="导入图片等资源"><a href="#导入图片等资源" class="headerlink" title="导入图片等资源"></a>导入图片等资源</h3><p>原来的资源都是放在 contents 目录下的，格式大概是 <code>contents/2017/10/29/imagename.jpg</code>。</p><p>只要把 contents 改成 images 放到 _post 目录下，然后，用你喜爱的编辑器（或者 <code>sed</code>）对所有文章做一次全局替换：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/images/ -&gt; /contents/</div></pre></td></tr></table></figure><p><code>hexo serve</code> 试一下，应该可以看到图片了！</p><h3 id="最后，做一些必要的配置"><a href="#最后，做一些必要的配置" class="headerlink" title="最后，做一些必要的配置"></a>最后，做一些必要的配置</h3><ul><li><a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="external">Hexo 配置</a></li><li><a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="external">NexT 配置</a></li><li><a href="http://theme-next.iissnan.com/theme-settings.html" target="_blank" rel="external">NexT 高级配置</a></li></ul><p>自己琢磨去吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道是不是我的错觉，&lt;a href=&quot;https://ghost.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ghost&lt;/a&gt; 这两年已经慢慢过了气。就像所有功成名就、开始赚钱的开源项目一样，一旦宣布商业化那基本就是宣告停止增长、甚至开始下滑，当然 Docker 这样牛的勉强可以除外。&lt;/p&gt;
&lt;p&gt;当然，也有可能是我喜新厌旧，现在看 Ghost 觉得并没有一定要用它的理由了。随着 Markdown 用的越来越熟练，以及购买了用的十分顺手的 &lt;a href=&quot;http://zh.mweb.im/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;MWeb 编辑器&lt;/a&gt; 之后，Ghost 引以为豪的 Editor 对我而言也优势全无。而 Hexo 近年来发展的很好，精致的主题也越来越多呢！（说白了还是因为脸）&lt;/p&gt;
&lt;p&gt;下面说正经的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="misc" scheme="https://ericfu.me/tags/misc/"/>
    
  </entry>
  
  <entry>
    <title>Python 2 字符串编码踩坑小结</title>
    <link href="https://ericfu.me/python-2-str-and-unicode/"/>
    <id>https://ericfu.me/python-2-str-and-unicode/</id>
    <published>2017-08-31T21:55:08.000Z</published>
    <updated>2017-09-01T07:59:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Tips:</strong> 如果您已经充分理解问题是什么，请直接跳到 <a href="#问题出在哪里">#问题出在哪里</a> 一节。</p><h2 id="字符串和编码"><a href="#字符串和编码" class="headerlink" title="字符串和编码"></a>字符串和编码</h2><p>先从概念说起，字符串和它的编码是两个不同的概念：</p><ul><li><strong>字符串</strong>是一段文字本身，可以是中文可以是英文，以及各种语言</li><li>字符串的<strong>编码</strong>是计算机存储、处理字符串的方式；作为一种<strong>数据</strong>，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。</li></ul><p>字符串经过<strong>编码（encode）</strong> 就成为了一堆数据，反过来，数据经过<strong>解码（decode）</strong> 就变回我们认识的字符串。</p><p><img src="/images/2017/09/encode_decode.png" alt="encode_decode"></p><a id="more"></a><h2 id="从-Python-3-说起"><a href="#从-Python-3-说起" class="headerlink" title="从 Python 3 说起"></a>从 Python 3 说起</h2><p>这个编码问题（坑）可以说是 Python 2 被吐槽最多的黑点，没有之一。为了防止上来就掉进 Python 2 的坑里，我们先来看看 Python 3 里“改进”后是什么样子的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></div><div class="line"><span class="class">&gt;&gt;&gt; <span class="title">len</span><span class="params">(s)</span></span></div><div class="line"><span class="class">9</span></div></pre></td></tr></table></figure><p>哈，没有任何问题！（数长度的时候别漏了空格）</p><p>查阅文档，我们发现 <code>str</code> 有个函数叫 <code>encode()</code>，它看起来很眼熟，让我们来试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">"utf-8"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line"><span class="string">b'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></div></pre></td></tr></table></figure><p>这个 <code>b&#39;&#39;</code> 的前缀表示返回值是一个 <code>bytes</code> 变量，也就是一堆数据了。</p><blockquote><p><strong>为什么这里面”Hello”还是原来的样子，但是“世界”变成一坨 <code>\x??</code> 了？</strong></p><p>这是因为 ASCII 实在太有名了，程序员们都看得懂：这个 <code>H</code> 其实表示的是一字节 <code>0x48</code>。而后面“世界”的编码不在 ASCII 的编码范围内，所以只能用 <code>\x??</code> 表示了。</p><p>这样看起来也许更清晰一些：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; &gt;&gt;&gt; b.hex()</div><div class="line">&gt; <span class="string">'48656c6c6f2c20e4b896e7958c'</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>有 <code>encode()</code> 当然也有 <code>decode()</code>。我们对刚刚拿到的 <code>bytes</code>  <code>b</code> 解码，果然会变成原来的字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</div><div class="line"><span class="string">'Hello, 世界'</span></div></pre></td></tr></table></figure><p>OK，现在你已经明白了奥义所在，是时候去踩坑了。</p><h2 id="Python-2-的世界"><a href="#Python-2-的世界" class="headerlink" title="Python 2 的世界"></a>Python 2 的世界</h2><h3 id="初见茅庐"><a href="#初见茅庐" class="headerlink" title="初见茅庐"></a>初见茅庐</h3><p>先来一道开胃菜：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Hello, 世界"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</div><div class="line">&lt;type <span class="string">'str'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</div><div class="line"><span class="number">13</span></div></pre></td></tr></table></figure><p>▲ 为什么这个长度是 13 ？明明是 9 个字符啊！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>s</div><div class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></div></pre></td></tr></table></figure><p>▲ <code>s</code> 你怎么坏掉了？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></div></pre></td></tr></table></figure><p>▲ 我可能用了假的 <code>encode()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</div><div class="line"><span class="string">u'Hello, \u4e16\u754c'</span></div></pre></td></tr></table></figure><p>▲ 喵喵喵？</p><p><strong>以上，Python 2 中字符串并不像我们想的那样工作。</strong></p><h3 id="问题出在哪里"><a href="#问题出在哪里" class="headerlink" title="问题出在哪里"></a>问题出在哪里</h3><p>其实说起来也简单，Python 是一门诞生于 1989 年的古老语言，比 Unicode 还要早两年，当时的程序员并不在乎编码问题，因为 ASCII 已经足够了。</p><p>如果你熟悉 C/C++ 会发现同样的问题：<code>char*</code> 被同时用于表示字符串和字节数组。Python 2 里也是同样，<code>str</code> 其实是个字节数组，却被挂上了字符串的名字。二十年后用着中文字符的我们被坑惨了。</p><p>后来 Python 2 为了支持 Unicode，增加了 <code>unicode</code> 类型，然而并没有卵用——程序员们不记得在每个字符串前面加上 <code>u</code>，这也不够优雅。</p><p>Python 3 设计之初就立志解决这个问题，不惜<strong>彻底修改了<code>str</code>的定义，把 <code>str</code> 这个名字让给了原来的 <code>unicode</code>！</strong>，而新增的 <code>bytes</code> 类型才是字节数组。如下表所示：</p><table><thead><tr><th></th><th>Python 2</th><th>Python 3</th></tr></thead><tbody><tr><td>字符串（Unicode）</td><td>unicode</td><td>str</td></tr><tr><td>字节数组</td><td>str (bytes)</td><td>bytes</td></tr></tbody></table><p>所以，在 Python 2 里，如果遇到非英语字符，一定要记得用 unicode。效果是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">u'Hello, 世界'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s</div><div class="line"><span class="string">u'Hello, \u4e16\u754c'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(s)</div><div class="line">&lt;type <span class="string">'unicode'</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>len(s)</div><div class="line"><span class="number">9</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = s.encode(<span class="string">'utf-8'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line"><span class="string">'Hello, \xe4\xb8\x96\xe7\x95\x8c'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b.decode(<span class="string">'utf-8'</span>)</div><div class="line"><span class="string">u'Hello, \u4e16\u754c'</span></div></pre></td></tr></table></figure><p>至于为什么 <code>str</code> 也有 <code>encode()</code>，主要是为了尽可能保持和 Python 3 的兼容性，以让部分程序能在 2、3 同时运行。<del>于是事情变得更糟糕了。</del></p><h3 id="原来如此"><a href="#原来如此" class="headerlink" title="原来如此"></a>原来如此</h3><p>现在我们可以解释刚刚遇到的奇怪情况了：</p><ul><li>“为什么这个长度是 13 ？明明是 9 个字符啊！”——因为 Python 自动帮你编码了，编码后是 13 个字节，常见的汉字在 UTF-8 编码下为 3 个字节</li><li>“<code>s</code> 你怎么坏掉了？” ——<code>str</code> 本来就是字节数组</li><li>“我可能用了假的 <code>encode()</code>”——你不应该对 <code>str</code> 变量做 <code>encode</code>，它本来就是编码后的</li><li>“喵喵喵？”——这是正常的，只是因为 Python 2 没有把 Unicode 字符显示成中文字符，用 print 就没问题了：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> s</div><div class="line">Hello, 世界</div></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li>永远记住 str 其实是 bytes，字符串应该用 unicode，尤其是包含中文时</li><li>如果能说服你的老板和同事，尽快把 Python 2 升级到 3</li></ol><p>最后，如果你需要写出兼容 Python 2\3 的程序，<a href="http://python-future.org/compatible_idioms.html#strings-and-bytes" target="_blank" rel="external">这篇文档</a>可以给你一些帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Tips:&lt;/strong&gt; 如果您已经充分理解问题是什么，请直接跳到 &lt;a href=&quot;#问题出在哪里&quot;&gt;#问题出在哪里&lt;/a&gt; 一节。&lt;/p&gt;
&lt;h2 id=&quot;字符串和编码&quot;&gt;&lt;a href=&quot;#字符串和编码&quot; class=&quot;headerlink&quot; title=&quot;字符串和编码&quot;&gt;&lt;/a&gt;字符串和编码&lt;/h2&gt;&lt;p&gt;先从概念说起，字符串和它的编码是两个不同的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;字符串&lt;/strong&gt;是一段文字本身，可以是中文可以是英文，以及各种语言&lt;/li&gt;
&lt;li&gt;字符串的&lt;strong&gt;编码&lt;/strong&gt;是计算机存储、处理字符串的方式；作为一种&lt;strong&gt;数据&lt;/strong&gt;，它和其他数据一样，都是以一串0和1组成的，通常我们用字节数组来表示它。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;字符串经过&lt;strong&gt;编码（encode）&lt;/strong&gt; 就成为了一堆数据，反过来，数据经过&lt;strong&gt;解码（decode）&lt;/strong&gt; 就变回我们认识的字符串。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/09/encode_decode.png&quot; alt=&quot;encode_decode&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>用 Bandit 做 Python 代码静态安全分析</title>
    <link href="https://ericfu.me/bandit-the-python-static-analyzer/"/>
    <id>https://ericfu.me/bandit-the-python-static-analyzer/</id>
    <published>2017-08-16T01:50:47.000Z</published>
    <updated>2017-08-21T00:55:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Bandit-是什么？"><a href="#Bandit-是什么？" class="headerlink" title="Bandit 是什么？"></a>Bandit 是什么？</h2><p>Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。</p><p>安装说明：参见 <a href="https://github.com/openstack/bandit" target="_blank" rel="external">GitHub 项目主页</a>。</p><a id="more"></a><h2 id="编写自定义的检查"><a href="#编写自定义的检查" class="headerlink" title="编写自定义的检查"></a>编写自定义的检查</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@bandit.checks('Call')</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prohibit_unsafe_deserialization</span><span class="params">(context)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'unsafe_load'</span> <span class="keyword">in</span> context.call_function_name_qual:</div><div class="line">        <span class="keyword">return</span> bandit.Issue(</div><div class="line">            severity=bandit.HIGH,</div><div class="line">            confidence=bandit.HIGH,</div><div class="line">            text=<span class="string">"Unsafe deserialization detected."</span></div><div class="line">        )</div></pre></td></tr></table></figure><ul><li><code>@bandit.checks(&#39;Call&#39;)</code>: 仅仅检查类型为 <code>Call</code> 的 AST Node</li><li><code>return bandit.Issue(...)</code>: 返回一个 Security Issue</li></ul><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>入口在 <code>cli/main.py</code> 的 <code>main()</code></p><p>先初始化了一堆参数，然后在这里创建了一个关键的 BanditManager 对象，之后的事情都是由它来完成的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">b_mgr = b_manager.BanditManager(b_conf, args.agg_type, args.debug,</div><div class="line">                                profile=profile, verbose=args.verbose,</div><div class="line">                                ignore_nosec=args.ignore_nosec)</div></pre></td></tr></table></figure><h3 id="扫描文件"><a href="#扫描文件" class="headerlink" title="扫描文件"></a>扫描文件</h3><p>紧接着就能看到这行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># initiate file discovery step within Bandit Manager</span></div><div class="line">b_mgr.discover_files(args.targets, args.recursive, args.excluded_paths)</div></pre></td></tr></table></figure><p>让我们跟进去，然后看看 <code>discover_files()</code> 都做了些什么：（代码太长就不贴了）</p><ul><li>处理 include/exclude 参数<ul><li>如果有 include 就只看这里面的文件，否则扫描所有文件</li><li>如果有 exclude，之后扫描的时候要去掉这些文件</li></ul></li><li>对所有指定的目标进行扫描<ul><li>如果设置了 recursive 选项，就递归地遍历子目录。</li></ul></li></ul><p>最后把遍历的结果排序并以列表的形式存放在 <code>self.files_list</code> 中。</p><h3 id="运行-Tests"><a href="#运行-Tests" class="headerlink" title="运行 Tests"></a>运行 Tests</h3><p>回到 <code>main()</code> 函数中，再往下看一点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># initiate execution of tests within Bandit Manager</span></div><div class="line">b_mgr.run_tests()</div></pre></td></tr></table></figure><p>看来这里是核心的一步，当然要走进去看看：（代码不贴了）</p><ul><li>枚举刚刚列表中的所有文件，读出来、并调用 <code>_parse_file()</code> 处理之。</li><li>如果处理失败了，也记下来，最后汇总输出会用到。</li></ul><p>继续跟进 <code>_parse_file()</code>，发现只是个包装，进入 <code>_execute_ast_visitor()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_execute_ast_visitor</span><span class="params">(self, fname, data, nosec_lines)</span>:</span></div><div class="line">    score = []</div><div class="line">    res = b_node_visitor.BanditNodeVisitor(fname, self.b_ma,</div><div class="line">                                           self.b_ts, self.debug,</div><div class="line">                                           nosec_lines, self.metrics)</div><div class="line"></div><div class="line">    score = res.process(data)</div><div class="line">    self.results.extend(res.tester.results)</div><div class="line">    <span class="keyword">return</span> score</div></pre></td></tr></table></figure><p>这个 BanditNodeVisitor 虽然没有继承标准库里的 ast.NodeVisitor 但实际上做的工作就是那样的——遍历所有 AST Node，同时对各个类型的 Node 执行对应的函数。</p><p>在 BanditNodeVisitor 中定义了很多类似 <code>visit_Call</code>, <code>visit_FunctionDef</code>, <code>visit_Str</code> 这样奇怪名字的函数，顾名思义就是对各个类型的 Node 所运行的函数。遍历的逻辑看 <code>visit</code> 函数。</p><p>以 <code>visit_Call</code> 为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visit_Call</span><span class="params">(self, node)</span>:</span></div><div class="line">    self.context[<span class="string">'call'</span>] = node</div><div class="line">    qualname = b_utils.get_call_name(node, self.import_aliases)</div><div class="line">    name = qualname.split(<span class="string">'.'</span>)[<span class="number">-1</span>]</div><div class="line"></div><div class="line">    self.context[<span class="string">'qualname'</span>] = qualname</div><div class="line">    self.context[<span class="string">'name'</span>] = name</div><div class="line"></div><div class="line">    self.update_scores(self.tester.run_tests(self.context, <span class="string">'Call'</span>))</div></pre></td></tr></table></figure><p>其实很简单，把对应的一些上下文信息 extract 出来并存到 <code>self.context</code>，然后用 <code>tester.run_tests</code> 执行所有对应 Call Node 的检查。</p><p>所以 <code>run_tests()</code> 的逻辑你应该能猜到个大概了：</p><ol><li>拿到所有类型为 <code>checktype</code> 的检查</li><li>对每个检查，以当前的 <code>context</code> 作为参数做检查，如果检查出问题就存起来</li></ol><h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><p>再回到 <code>main()</code> 函数中，再往下就是输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># trigger output of results by Bandit Manager</span></div><div class="line">sev_level = constants.RANKING[args.severity - <span class="number">1</span>]</div><div class="line">conf_level = constants.RANKING[args.confidence - <span class="number">1</span>]</div><div class="line">b_mgr.output_results(args.context_lines,</div><div class="line">                     sev_level,</div><div class="line">                     conf_level,</div><div class="line">                     args.output_file,</div><div class="line">                     args.output_format)</div></pre></td></tr></table></figure><p>Severity 和 Confidence 都是用来过滤的。最后输出到指定的形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Bandit-是什么？&quot;&gt;&lt;a href=&quot;#Bandit-是什么？&quot; class=&quot;headerlink&quot; title=&quot;Bandit 是什么？&quot;&gt;&lt;/a&gt;Bandit 是什么？&lt;/h2&gt;&lt;p&gt;Bandit 是一个用来检查 Python 代码中常见安全问题的工具，它会处理各个源代码文件，解析出 AST（抽象语法树），然后对 AST 节点执行一组对应的插件。当 Bandit 完成检查之后，它能生成一封安全报告。&lt;/p&gt;
&lt;p&gt;安装说明：参见 &lt;a href=&quot;https://github.com/openstack/bandit&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub 项目主页&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="python" scheme="https://ericfu.me/tags/python/"/>
    
      <category term="security" scheme="https://ericfu.me/tags/security/"/>
    
  </entry>
  
  <entry>
    <title>获得第三届阿里中间件性能大赛冠军</title>
    <link href="https://ericfu.me/aliware-performance-contest-first-prize/"/>
    <id>https://ericfu.me/aliware-performance-contest-first-prize/</id>
    <published>2017-07-18T16:48:53.000Z</published>
    <updated>2017-08-18T00:48:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。</p><p><img src="/images/2017/07/alibaba-praise.jpg" alt="alibaba-praise"></p><p>Emmm… 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。</p><a id="more"></a><h2 id="第一赛季：消息中间件"><a href="#第一赛季：消息中间件" class="headerlink" title="第一赛季：消息中间件"></a>第一赛季：消息中间件</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/middlewarerace2017/open-messaging-demo" target="_blank" rel="external">这里</a>）</p><p>设计一个消息中间件，它支持 Topic 和 Queue，每个 Topic 可以被一个或多个 Queue 订阅。显然，发往 Topic 的消息会被所有的订阅者接收到，而发往某个特定 Queue 的消息只会被它接收到。</p><p>消息不能丢失，且对于各个 Queue 消息要保持有序。在此前提下，吞吐量最优者获胜。</p><p>要注意的是，生产和消费是分两个阶段进行的：先进行生产，结束后再进行消费。</p></blockquote><p>我的思路很简单，对于每个 Topic 或者 Queue，把所有消息序列化后写入对应的一个文件中。消费时，从磁盘中顺序读取即可。</p><p>本题最后瓶颈落在了磁盘 IO 上，所以很多优化也变得无关紧要了。为了充分利用磁盘 IO，一定要确保两点：</p><ul><li>顺序写入和读取（参见我的<a href="https://ericfu.me/varnish-notes-from-archetect/">这篇文章</a>）</li><li>使用 Linux 内存映射（mmap）技术，也就是 Java nio 包里的 MappedByteBuffer</li></ul><p>此外，序列化的过程中尽可能减少内存拷贝，以及避免不必要的 String 序列化、反序列化。</p><p>做到以上几点，进入前 20 不是问题。初赛只要前 200 名就可以晋级了（吐槽：那很容易啊！！但是一开始不知道初赛成绩也要计入最后评分的）</p><p><a href="https://code.aliyun.com/fuyufjh/open-messaging-demo-zero/" target="_blank" rel="external">代码</a>托管在阿里云 code 上。</p><h2 id="第二赛季：数据库同步"><a href="#第二赛季：数据库同步" class="headerlink" title="第二赛季：数据库同步"></a>第二赛季：数据库同步</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/wanshao/IncrementalSync/" target="_blank" rel="external">这里</a>）</p><p>模拟数据库的主备复制。给定一批 binlog 文本数据，你的任务是重放所有 binlog 从而得到数据表的最新状态（假设原本状态是空的）。重放的操作包含 Insert/Update/Delete 三种操作，注意主键也可能被更新！</p><p>验证结果的方式是，客户端给定一个 PK 的区间，输出该区间的所有行。</p><p>注意，程序分 Server 和 Client 端，机器配置都很高（16核CPU），但是 JVM 的堆大小被限制为 1G 新生代 + 2G 老年代。此外，必须顺序读取 binlog。</p></blockquote><p>思路如下：并行化是一定要的，如何并行呢？答案（当然）是按主键哈希分桶。然而这样主键更新怎么处理？这是最大的难点。</p><p>为此我们想出了一种并行化的算法——可以说这就是最终获得冠军的原因。具体思路在决赛答辩 PPT 里写的很清楚啦。</p><p>答辩PPT（包括完整的解题思路）：</p><p><iframe src="//www.slideshare.net/slideshow/embed_code/key/jURs7pfS57Uz7d" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><p>这题思考了很久，最后交出的答卷含金量也很高。哈希分桶作为经典算法中常见的一种模式，在很多分布式系统中都有应用。</p><p>此外，比赛初期还没有加上“必须顺序读取”这个条件。如果没有这个条件又会有别的奇思妙想的算法来解决。留给读者自己思考（喵）</p><p><a href="https://code.aliyun.com/fuyufjh/IncrementalSyncSequential" target="_blank" rel="external">代码</a><br>托管在阿里云上，同时还有<br><a href="https://code.aliyun.com/fuyufjh/tianchi-middleware-doc" target="_blank" rel="external">设计文档</a> 也很有帮助（如果你觉得 PPT 还不够细致）。</p><h2 id="24-小时极客-PK-赛"><a href="#24-小时极客-PK-赛" class="headerlink" title="24 小时极客 PK 赛"></a>24 小时极客 PK 赛</h2><blockquote><p><strong>题目大意</strong>（完整的请点<a href="https://code.aliyun.com/fuyufjh/topkn_final" target="_blank" rel="external">这里</a>）</p><p>分页排序。给定一批数据，求解按顺序从小到大，顺序排名从第k下标序号之后连续的n个数据，类似于 <code>order by id limit k, n</code>，n 会很大，k &lt;= 100</p><p>数据是文本文件，每行是一个长度在 256 字符以内的字符串。排序的方式是：先按长度从小到大、再按字典序。</p><p>注意一共有 2 台 Worker 和 1 台 Master，Worker 上分别放了 5G 的数据，最后 Master 要请求到完整的排序结果。</p></blockquote><p>和之前的两场比赛不同，24小时赛的成绩是不计入最后评价的，而且是以个人名义参赛。奖品是去硅谷开（玩）会（耍）！（PS. 然而不能带女朋友，最后去了一帮基佬啊摔！）</p><p>外排序是很容易想到的思路，然而十分复杂，也不能很好的并行。那怎么办？依然是分桶，Bucket Sort。</p><p>假设字符串长度的分布是均匀的，字符出现的概率也是相近的，则我们可以用以下的值作为每个字符串的 key：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;length&gt; &lt;text[0]&gt; &lt;text[1]&gt; &lt;text[2]&gt;</div></pre></td></tr></table></figure><p>（如果长度小于 3 就用 ‘\0’ 填充一下）</p><p>这样我们就能对每个 key 做统计了——有多少个字符串是在这个桶里呢？假设这个结果放在数组 <code>count</code> 中，那再对 <code>count</code> 求一个累计和，就能用二分查找找到第 N 个数应该位于哪一个桶里。</p><p>同样的也可以找到第 N+K-1 个数应该位于哪个桶里。然后就简单了，把区间内的桶里的值都取出来，让 Master 排个序就好了。</p><p>把区间内的桶里的值都取出来？——这个过程可以扫描整个文件。也可以做个预处理的优化：把每个字符串的 key 和 offset 分别存到两个数组里，比如 <code>keys[]</code> 和 <code>offsets[]</code>，这样只要扫描数组就可以了！</p><p>然而我并没有想到这个优化，所以是第二名。哈哈哈！</p><p><a href="https://code.aliyun.com/fuyufjh/topkn_final?spm=a2111a.8458726.0.0.59556667DTd4xf" target="_blank" rel="external">代码</a>托管在阿里云上。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol><li>算法是好的，要多刷 HackerRank</li><li>阿里搞的这个比赛啊，Excited ！</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一不小心拿到了天池阿里中间件性能大赛的冠军，准确的说还有个 24 小时即刻挑战赛（个人赛）的亚军。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017/07/alibaba-praise.jpg&quot; alt=&quot;alibaba-praise&quot;&gt;&lt;/p&gt;
&lt;p&gt;Emmm… 过去太久了，不知道说什么感言了。以下是是比赛的题目以及我的解答，备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Data Streaming Algorithms Slideshare</title>
    <link href="https://ericfu.me/data-streaming-algorithms-slideshare/"/>
    <id>https://ericfu.me/data-streaming-algorithms-slideshare/</id>
    <published>2017-07-05T19:52:36.000Z</published>
    <updated>2017-08-19T07:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/2p95byDFIJLeWO" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><a id="more"></a><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol><li><a href="https://mapr.com/blog/some-important-streaming-algorithms-you-should-know-about/" target="_blank" rel="external">Some Important Streaming Algorithms You Should Know About</a></li><li><a href="https://www.slideshare.net/SandeepJoshi55/data-streaming-algorithms-65575952" target="_blank" rel="external">Data streaming algorithms</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/2p95byDFIJLeWO&quot; width=&quot;100%&quot; height=&quot;500&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;/div&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Paxos &amp; Raft 分布一致性算法 Slideshare</title>
    <link href="https://ericfu.me/paxos-raft-share/"/>
    <id>https://ericfu.me/paxos-raft-share/</id>
    <published>2017-05-11T04:47:39.000Z</published>
    <updated>2017-08-19T07:26:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/tPlG02EI7ijaKE" width="100%" height="500" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> </div></p><a id="more"></a><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol><li><a href="https://zh.wikipedia.org/zh-hans/Paxos%E7%AE%97%E6%B3%95" target="_blank" rel="external">Paxos算法 - 维基百科</a></li><li><a href="https://raft.github.io/raft.pdf" target="_blank" rel="external">In Search of an Understandable Consensus Algorithm<br>(Extended Version)</a></li><li><a href="http://codemacro.com/2014/10/15/explain-poxos/" target="_blank" rel="external">图解分布式一致性协议Paxos - loop in code</a></li></ol><h4 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h4><ol><li><a href="https://github.com/renquinn/Paxos-Demo" target="_blank" rel="external">Demo - Paxos</a> （有小错误）</li><li><a href="thesecretlivesofdata.com/raft/">Demo - Raft</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/tPlG02EI7ijaKE&quot; width=&quot;100%&quot; height=&quot;500&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 700px;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;/div&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>性能测试：ArrayBlockingQueue vs. Go channel</title>
    <link href="https://ericfu.me/perf-test-blockingqueue-vs-channel/"/>
    <id>https://ericfu.me/perf-test-blockingqueue-vs-channel/</id>
    <published>2017-04-30T23:56:23.000Z</published>
    <updated>2017-08-19T07:28:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>测试方案：</p><ol><li>主线程将一个 Integer 对象发到 Channel 0</li><li>线程 i 将对象从 Channel i 不断搬运到 Channel i+1</li><li>最后一个线程从 Channel N-1 中拿到对象，做加和</li></ol><p>为了保证公平，Go 中自行封装一个 Integer 而不是用 int 型；考虑到实际中大多数情况下 channel 里走的都是对象而非基本类型，这样是合理的。</p><a id="more"></a><p><strong>发现二者完成时间基本都在 3.8s ~ 4.0s 之间，可以说没有差异。</strong> ArrayBlockingQueue 的性能看来还是很高的。</p><p>PS. 尝试了容量不限的 ListBlockingQueue，时间在 5s 左右，也还可以接受。</p><p>测试代码在<a href="https://gist.github.com/fuyufjh/9657835df8202b29af8be1610c8327ad" target="_blank" rel="external">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;测试方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主线程将一个 Integer 对象发到 Channel 0&lt;/li&gt;
&lt;li&gt;线程 i 将对象从 Channel i 不断搬运到 Channel i+1&lt;/li&gt;
&lt;li&gt;最后一个线程从 Channel N-1 中拿到对象，做加和&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了保证公平，Go 中自行封装一个 Integer 而不是用 int 型；考虑到实际中大多数情况下 channel 里走的都是对象而非基本类型，这样是合理的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="golang" scheme="https://ericfu.me/tags/golang/"/>
    
      <category term="java" scheme="https://ericfu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>AWS Solution Architect 认证学习笔记</title>
    <link href="https://ericfu.me/aws-solution-architect-cert-associate/"/>
    <id>https://ericfu.me/aws-solution-architect-cert-associate/</id>
    <published>2017-04-30T22:22:55.000Z</published>
    <updated>2017-08-22T00:27:38.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/05/Solutions-Architect-Associate.png" alt="Solutions-Architect-Associate"></p><p>经过一个月的学习，成功通过了 AWS 考试认证。不得不说相比从零开始闷声摸索，学习带来的收获还是很大的，避免了一些 AWS 的大坑，也了解了一些 best practice 的系统设计。</p><p>可惜这个博客并不是在 AWS 上（原因可能是贫穷）。</p><p>以下是我看过的学习资料：</p><a id="more"></a><ul><li><a href="https://acloud.guru" target="_blank" rel="external">ACloudGuru 的课程</a>，需要购买</li><li>阅读各个 AWS Service FAQ</li><li><a href="http://jayendrapatil.com/" target="_blank" rel="external">Jayendra’s Blog</a>，知识点整理以及样题</li></ul><p>个人学习笔记（整理自 ACloudGuru 课程每章总结）</p><ul><li><a href="https://ericfu.me/aws-notes-iam/">AWS 学习笔记之 IAM</a></li><li><a href="https://ericfu.me/aws-notes-s3/">AWS 学习笔记之 S3</a></li><li><a href="https://ericfu.me/aws-notes-ec2/">AWS 学习笔记之 EC2</a></li><li><a href="https://ericfu.me/aws-notes-route53/">AWS 学习笔记之 Route53</a></li><li><a href="https://ericfu.me/aws-notes-database/">AWS 学习笔记之数据库</a></li><li><a href="https://ericfu.me/aws-notes-vpc/">AWS 学习笔记之 VPC</a></li><li><a href="https://ericfu.me/aws-notes-sqs-swf-sns/">AWS 学习笔记之 SQS/SWF/SNS 等</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/05/Solutions-Architect-Associate.png&quot; alt=&quot;Solutions-Architect-Associate&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过一个月的学习，成功通过了 AWS 考试认证。不得不说相比从零开始闷声摸索，学习带来的收获还是很大的，避免了一些 AWS 的大坑，也了解了一些 best practice 的系统设计。&lt;/p&gt;
&lt;p&gt;可惜这个博客并不是在 AWS 上（原因可能是贫穷）。&lt;/p&gt;
&lt;p&gt;以下是我看过的学习资料：&lt;/p&gt;
    
    </summary>
    
      <category term="series" scheme="https://ericfu.me/categories/series/"/>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
  <entry>
    <title>译：Varnish 架构师笔记</title>
    <link href="https://ericfu.me/varnish-notes-from-archetect/"/>
    <id>https://ericfu.me/varnish-notes-from-archetect/</id>
    <published>2017-04-30T22:06:55.000Z</published>
    <updated>2017-08-19T07:31:16.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>找到这篇文章是在阅读 Kafka 文档时，一个<a href="https://kafka.apache.org/documentation/#persistence" target="_blank" rel="external">名为 “Don’t fear the filesystem!”的段落</a>中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单：</p><ul><li>因为操作系统 PageCache 的存在，磁盘操作可能很快</li><li>虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快</li><li>CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要</li></ul><p><a href="http://varnish-cache.org/docs/trunk/phk/notes.html" target="_blank" rel="external">原文链接</a></p></blockquote><p>当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。</p><p>多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。</p><a id="more"></a><p>所以 Varnish 这个项目一开始并没能激起我很大的兴趣。但我渐渐意识到 Varnish 虽然是一个用户态应用，但却也是一个能充分发挥我长久以来积累的关于硬件和内核的经验知识的理想场所。目前 Varnish 的开发已经进展到alpha版本的阶段，而我觉得应当承认自己相当享受这一段历程。</p><h3 id="1975的编程方式到底出了什么问题？"><a href="#1975的编程方式到底出了什么问题？" class="headerlink" title="1975的编程方式到底出了什么问题？"></a>1975的编程方式到底出了什么问题？</h3><p>简而言之：计算机系统再也不应该被看作是由两个存储层次构成的了。</p><p>首先是主存：从水银延迟线，磁芯存储器，到现在可供随机访问的RAM。</p><p>然后是辅存：从纸带，磁带，到磁盘。最早的磁盘有一屋子大，然后缩小到洗衣机的尺寸，到今天硬盘可以被放进随身携带的 MP3 播放器中。</p><p>于是大家就按照这样的划分，在内存中分配变量，在磁盘中存取数据，以这样的方式进行编程工作。</p><p>还是拿 Squid 这个1975年风格的系统为例：你事先配置它的内存和硬盘用量，然后它会花费大把时间来追踪哪些HTTP对象驻留内存中，哪些存放在硬盘上，并且根据不同情况来调整它们的放置策略。</p><p>然而实际上呢，现今的计算机应被视为只使用一种统一的存储系统，这个系统完全基于硬盘（磁盘，固态硬盘或者其他什么东西），而传统的内存呢，在操作系统内核和虚拟内存硬件机制的帮助下可以被看作是硬盘的缓存。</p><p>回过头来看 Squid 的策略，它精心设计的存储管理机制实际上却陷入了与操作系统内核同样精巧的管理策略的激烈冲突。而就像所有内战一样，这样的冲突必然一事无成。</p><p>我们可以尝试从细节角度来看整个流程：一开始 Squid 会请求内存用来创建了一个 HTTP 对象，它往往会在创建之初被频繁访问多次，然后闲置一段时间。而后当内核接收到其他内存分配请求时，会将这些它认为闲置的内存数据放到硬盘交换分区去，而把这些回收的内存页用于更活跃的任务。在Squid下一次访问这一对象时，操作系统又会把暂存在交换分区的数据取回来供它使用。 这些内核对于内存的操作对于 Squid 是透明的，在它看来这个 HTTP 对象就像从没离开过内存一样，而实际上物理内存页得到了更有效的使用。</p><p>这就是虚拟内存机制。</p><p>如果事情到此为止的话就好了，但接下来1975年式的编程风格就出现了。</p><p>一段时间之后，Squid 也和内核一样注意到了这个对象闲置了，于是打算把它放到硬盘上去，省出一些内存来给更频繁访问的数据使用。所以它打开一个文件把这个对象写了进去。</p><p>打开慢镜头来看这个写入的过程：</p><p>Squid 通过系统调用 write 将 HTTP 对象的虚拟内存地址传递给内核。<br>由于物理页已经被内核交换出去，这个内存访问将引发一个缺页中断。<br>为了重新载入这个内存页，内核不得不交换出另一个正在使用的内存页（很可能包含另一 Squid 的对象），修复页表，然后继续执行系统调用。<br>Squid 对这些一无所知，它自以为只是进行了一次再普通不过的访存操作而已。</p><p>接下来 Squid 可以终于拿这块内存放别的数据了。而当这个对象再次被访问到时，Squid 又不得不把它从硬盘中取回来。首先它需要空闲的内存来存放这个对象，于是它根据某种淘汰算法选中另一个最近不常用的对象，把它写到硬盘上去（上面那些步骤重演了一遍）。然后打开文件读出这次所需的那个对象，最后通过网络套接字发送出去。</p><p>这一切显然充满了各种无用功。</p><p>让我们看看 Varnish 是怎么做的</p><p>Varnish 会直接请求大块虚拟内存，并由操作系统将这个内存空间映射到一个硬盘文件。当需要访问某个 HTTP 对象时，只需要正确索引这个对象相应的虚拟内存地址，剩下的交给操作系统就好了。当内核需要回收一些内存页时，它会自行决定将一些 Varnish 的内存数据写回到映射的文件中。而当 Varnish 再次访问这一块虚拟内存时，内核自然会腾出内存页来将文件中的数据读回使用。</p><p>仅此而已。</p><p>Varnish 不去尝试控制哪些数据应该被缓存在内存中，哪些应该放到硬盘上去。内核代码和硬件会处理这些事情，而且处理得很漂亮。</p><p>此外，与 Squid 不同的是 Varnish 只需要一个文件而不是为每个 HTTP 对象创建单独的文件。没有任何理由需要在 HTTP 对象和文件系统对象间建立一一对应的关系，也没有理由把时间浪费在文件系统的命名空间处理上。Varnish 需要处理的只是虚拟内存的指针和所需对象的长度值而已。</p><p>虚拟内存的出现为数据大于物理内存的场景提供了一种便利的机制，但人们似乎并没有领悟这一点。</p><h3 id="更多的缓存"><a href="#更多的缓存" class="headerlink" title="更多的缓存"></a>更多的缓存</h3><p>CPU 的时钟频率目前看来基本止步于4GHz了。即便如此，为了避免内存读写的瓶颈，硬件工程师们不得不使用使用一级，二级，有时候甚至是三级 CPU cache（现在我们可以认为 RAM 是第四级缓存了），此外还有写缓冲，流水线，页模式读取等各种技术，而这些都是为了加快访存来匹配CPU的处理速度。</p><p>虽然时钟频率方面受限，但硅工艺的进步缩小了器件尺寸，集成了更多的晶体管。所以多核 CPU 的设计逐渐成为主流，但这从编程模型角度看来实在是很糟糕的一件事。</p><p>虽然多核系统存在已久，但编写能够利用上多个 CPU 的代码依然是一件棘手的事。而要在多核系统上写出高性能的程序就更是如此了。</p><p>比如我们有两个计数器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">unsigned</span> n_foo;</div><div class="line"><span class="keyword">unsigned</span> n_bar;</div></pre></td></tr></table></figure><p>在一个 CPU 上执行了 <code>n_foo++</code> 的操作会使得CPU读取 <code>n_foo</code> 的值然后写回。</p><p>读取一个内存位置首先会检查它是否在 CPU L1 cache 中命中，这挺难的除非它刚刚被使用过。接下来是 L2 cache，我们不妨假设依然没有命中吧。</p><p>如果是在一个单核系统上，CPU 会去内存读取数据然后就完事。但在多核系统中，我们必须去检查其他CPU核心是否缓存并修改了 <code>n_foo</code> 的数值。我们会发起一个特殊的总线事务做这种检查，如果其他 CPU 答复说它确实持有这样一份拷贝，它就需要将它写回到内存中。如果硬件设计良好，我们可能可以通过总线监听获得这份新数据，不然的话就需要去内存里读取它。</p><p>我们终于可以修改 <code>n_foo</code> 的值了，但其实修改完了之后一般不会直接将它写回到内存中。为了之后操作的快速存取，很可能我们会把它缓存起来。</p><p>现在假设另一个 CPU 需要执行 <code>n_bar++</code> 的操作，它能够直接进行吗？答案是否定的。因为缓存的单位并不是字节而是 cache-line（典型值是 8-128 个字节）。所以当第一个 CPU 忙着处理 <code>n_foo</code> 时，第二个 CPU 必须等待，因为虽然它想获取的是另一个变量，但却不幸落在了同一个 cache-line 中。</p><p>明白了吧？没错，这有点丑。</p><h3 id="我们该怎么办"><a href="#我们该怎么办" class="headerlink" title="我们该怎么办"></a>我们该怎么办</h3><p><strong>尽一切可能，减少内存操作。</strong></p><p>下面是 Varnish 的一些做法。</p><p>当需要处理一个 HTTP 请求或响应时，我们会持有一组指针和一个内存工作区。我们并不需要在处理每个 HTTP 报头时都调用 malloc，而是一次性分配整个工作区的内存，然后按需从中获取所需空间。而当我们一次性释放全部报头时，只要将指针重置到工作区的起始位置即可。</p><p>当需要将 HTTP 报头从一个请求拷贝到另一个请求（或从从一个响应复制到另一个响应）时，并不需要进行字符串拷贝，而只要拷贝指针。如果源报头在这个过程中不会被不释放或改写，这样做是非常安全的。比如从客户端请求到后台请求的拷贝就是这样一个例子。</p><p>但在一些新构建的报头生命周期长于源报头的场景中，就需要另外分配内存了。例如当我们会缓存新 HTTP 对象时，首先就计算整个报头所需空间，然后通过一次 malloc 调用来获取内存。</p><p>另外我们会尽可能重用那些正被缓存的内存数据。</p><p>比如 Varnish 的 worker 线程是以<em>最近最忙的</em>方式调度的，也即是说一个 worker 线程空闲后会被放回到队列的最前端，使得它更有机会马上去处理下一个新请求，这样它的数据，栈空间和变量等很可能可以在 CPU 缓存中被重用，而不是再次从RAM中读取。</p><p>同时对于 worker 线程经常使用的数据，我们会把它们分配在每个线程的栈变量中，并且确保它们占据完整的内存页。这样我们就可以尽可能避免 cache-line 的竞争。</p><p>如果对你来说这些听起来都很陌生，我可以告诉你它们是确实有效的：Varnish 处理一个命中缓存的请求最多只需18个系统调用，而且其中不少只是为了获得时间戳来满足统计的需要。</p><p>这些技术并不新鲜，我们已经在内核开发中使用了10多年，现在该轮到你们来学习了:-)</p><p>如此，欢迎进入 Varnish，一个 2006风格架构的程序。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;找到这篇文章是在阅读 Kafka 文档时，一个&lt;a href=&quot;https://kafka.apache.org/documentation/#persistence&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;名为 “Don’t fear the filesystem!”的段落&lt;/a&gt;中提到的。文档指出，我们总是思维定势地以为磁盘很慢，内存很快。然而今天的计算机体系结构中，并非这么简单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因为操作系统 PageCache 的存在，磁盘操作可能很快&lt;/li&gt;
&lt;li&gt;虽然磁盘 IOPS 难以提高，但吞吐量在不断上升；换句话说，顺序读写磁盘非常快&lt;/li&gt;
&lt;li&gt;CPU Cache 常常被忽略了，了解 CPU Cache 对提升内存读写性能至关重要&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;http://varnish-cache.org/docs/trunk/phk/notes.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当你开始深入 Varnish 的源代码后，应该会发觉它与你日常所见的一般应用软件有着明显不同，而这绝非偶然。&lt;/p&gt;
&lt;p&gt;多年以来我的绝大部分时间花费在 FreeBSD 的内核开发上，而每每涉足用户空间编程，却总是毫无例外地发现那里的人们还在以1975年的方式工作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="os" scheme="https://ericfu.me/tags/os/"/>
    
  </entry>
  
  <entry>
    <title>HackerRank: Spanning Tree Fraction 题解</title>
    <link href="https://ericfu.me/hackerrank-spanning-tree-fraction/"/>
    <id>https://ericfu.me/hackerrank-spanning-tree-fraction/</id>
    <published>2017-04-20T08:50:21.000Z</published>
    <updated>2017-08-19T07:44:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>题目：<a href="https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction" target="_blank" rel="external">https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction</a><br>一张连通图G=(V,E)上，每条边有a和b两个整数，求一个生成树使得Sum(a) / Sum(b) 最大，输出这个最大值的分数形式 p/q</p></blockquote><p>设 $\frac{\sum{a_i}}{\sum{b_i}} \ge c$ ，经过变换可得，$\sum{(a_i - b_i c)} \ge 0$，这种形式下 <code>ai - bi * c</code> 就退化为一条边的 cost，能方便得用 Prim 或 Kruskal 算法求出 cost 最大的生成树。</p><a id="more"></a><p>因为我们知道 c 的取值是受限制的——显然不能任意大。根据题意，我们要求 c 的最大可能值 max(c)，借助二分查找：如果 c 取值 <code>(L+R)/2</code> 时找不到生成树满足 sum cost &gt;= 0，说明 max(c) 在右半边；反之，如果 c 的取值 <code>(L+R)/2</code> 时可行，说明在左半边（因为 C 是实数，这里说的都是闭区间）。 </p><p>Prim 和 Kruskal 算法的实现也是一个难点。本题中 Kruskal 算法更简单，从小到大取所有的边，利用并查集可以快速判断这条边是否已经被连通了，若还为连通就要选取此边。</p><p>最后的 p/q 怎么求呢？显然从实数 c 变回 p/q 是不可能的。假如循环 N 次，最后一次循环可以认为已经收敛了，最后一次计算中途的 $\sum{a_i}$ 和 $\sum{b_i}$ 就是最优 case 下的取值，将 $\frac{\sum{a_i}}{\sum{b_i}}$ 化简、消除公约数即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;numeric&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> n, m;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">100002</span>;</div><div class="line"><span class="keyword">int</span> u[N], v[N], a[N], b[N];</div><div class="line"></div><div class="line"><span class="keyword">double</span> w[N];</div><div class="line"><span class="keyword">int</span> p[N];</div><div class="line"></div><div class="line"><span class="keyword">int</span> <span class="built_in">set</span>[N];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_set</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (<span class="built_in">set</span>[x] == x) <span class="keyword">return</span> x;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">set</span>[x] = find_set(<span class="built_in">set</span>[x]);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">union_set</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</div><div class="line">    a = find_set(a);</div><div class="line">    b = find_set(b);</div><div class="line">    <span class="keyword">if</span> (a == b) <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    <span class="built_in">set</span>[a] = b;</div><div class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">int</span> A, B;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">double</span> c)</span> </span>&#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) w[i] = a[i] - b[i] * c;</div><div class="line">    sort(p, p + m, [](<span class="keyword">int</span> i, <span class="keyword">int</span> j)&#123; <span class="keyword">return</span> w[i] &gt; w[j]; &#125;);</div><div class="line">    iota(<span class="built_in">set</span>, <span class="built_in">set</span> + n, <span class="number">0</span>);</div><div class="line">    A = B = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> e = p[i];</div><div class="line">        <span class="keyword">if</span> (union_set(u[e], v[e])) &#123;</div><div class="line">            A += a[e];</div><div class="line">            B += b[e];</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> A &gt;= B * c;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</div><div class="line">    <span class="keyword">while</span> (a &amp;&amp; b) &#123;</div><div class="line">        <span class="keyword">if</span> (a &gt; b) a %= b;</div><div class="line">        <span class="keyword">else</span> b %= a;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> a | b;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</div><div class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d %d %d"</span>, &amp;u[i], &amp;v[i], &amp;a[i], &amp;b[i]);</div><div class="line">    &#125;</div><div class="line">    iota(p, p + m, <span class="number">0</span>);</div><div class="line">    <span class="keyword">double</span> lo = <span class="number">0</span>, hi = <span class="number">1e5</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> t = <span class="number">0</span>; t &lt; <span class="number">100</span>; t++) &#123;</div><div class="line">        <span class="keyword">double</span> c = (hi + lo) * <span class="number">0.5</span>;</div><div class="line">        <span class="keyword">if</span> (check(c)) lo = c;</div><div class="line">        <span class="keyword">else</span> hi = c;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> g = gcd(A, B);</div><div class="line">    <span class="built_in">printf</span>(<span class="string">"%d/%d\n"</span>, A/g, B/g);</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;题目：&lt;a href=&quot;https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.hackerrank.com/contests/w31/challenges/spanning-tree-fraction&lt;/a&gt;&lt;br&gt;一张连通图G=(V,E)上，每条边有a和b两个整数，求一个生成树使得Sum(a) / Sum(b) 最大，输出这个最大值的分数形式 p/q&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;设 $\frac{\sum{a_i}}{\sum{b_i}} \ge c$ ，经过变换可得，$\sum{(a_i - b_i c)} \ge 0$，这种形式下 &lt;code&gt;ai - bi * c&lt;/code&gt; 就退化为一条边的 cost，能方便得用 Prim 或 Kruskal 算法求出 cost 最大的生成树。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="algorithm" scheme="https://ericfu.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>AWS 学习笔记之 SQS/SWF/SNS 等</title>
    <link href="https://ericfu.me/aws-notes-sqs-swf-sns/"/>
    <id>https://ericfu.me/aws-notes-sqs-swf-sns/</id>
    <published>2017-04-11T02:34:49.000Z</published>
    <updated>2017-08-19T07:46:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SQS-Simple-Queue-Service"><a href="#SQS-Simple-Queue-Service" class="headerlink" title="SQS - Simple Queue Service"></a>SQS - Simple Queue Service</h2><p>SQS 是 AWS 的消息队列服务，用于暂存消息并等待接收者处理。</p><ul><li>不保证 FIFO，可能乱序到达</li><li>Visibility Timeout 最长 12 小时</li><li>保证每条消息至少被传递一次（At least once），这意味着某些情况下可能传递多次，所以你写程序的时候要注意处理重复的消息</li><li>每条消息最大 256 KB<ul><li>然而，依据 64 KB 的 Chunk 数量计费</li><li>所以一个 256 KB 的消息可能产生 4 次费用：4 * 64 KB chunks</li></ul></li><li>SQS 的消息没有优先级；如果你需要优先级，最佳实践是创建多个 SQS 队列</li></ul><a id="more"></a><h2 id="SWF-Simple-Workflow-service"><a href="#SWF-Simple-Workflow-service" class="headerlink" title="SWF - Simple Workflow service"></a>SWF - Simple Workflow service</h2><p>SWF 提供 Workflow 服务，比如 Amazon 用它来处理客户订单的流程——下单、支付、配货、发货……</p><ul><li>一个 Workflow 可以长达 1 年，而 SQS 的消息最多保留 14 天</li><li>SWF 的 API 面向 task，而 SQS 的 API 面向 message</li><li>SWF 保证一个 task 只会分配一次、不会重复；SQS 不保证这一点</li><li>SWF 会跟踪每个 task 和 event 的处理；SQS 没有这个功能</li></ul><p>SWF 有以下 3 种 Actors：</p><ul><li>Workflow Starter - 启动 workflow 的程序，比如电商网站的下单操作</li><li>Decider - 控制 task 的执行流程，如果某个步骤完成／失败，decider 决定下一步做什么</li><li>Activity Worker - 执行任务，可以是人类</li></ul><h2 id="SNS-Simple-Notification-Service"><a href="#SNS-Simple-Notification-Service" class="headerlink" title="SNS - Simple Notification Service"></a>SNS - Simple Notification Service</h2><p>SNS 提供消息通知服务，例如当 CloudTrail 发出警告时给用户发 Email 通知。</p><p>SNS 的订阅者可以是以下这些：</p><ul><li>HTTP / HTTPS</li><li>Email / Email (JSON)</li><li>SQS</li><li>Lambda</li><li>Application</li></ul><p>SNS vs SQS：</p><ul><li>相同点：都是消息服务</li><li>不同点：SNS 是 Push，而 SQS 是 Pull (Poll)</li></ul><h2 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h2><ul><li>API Gateway 很便宜，并能自动伸缩</li><li>API Gateway 可以通过 cache 提升性能，减轻后端服务器的负载</li><li>API Gateway 可以 throttle 流量从而防止被攻击</li><li>可以把所有的访问记到 CloudWatch</li><li>如果你用到了跨域 AJAX，记得开启 CORS (Cross-Origin Resource Sharing)</li></ul><h2 id="Elastic-Transcoder"><a href="#Elastic-Transcoder" class="headerlink" title="Elastic Transcoder"></a>Elastic Transcoder</h2><p>提供云转码服务，将原始视频格式转换到不同的格式，从而方便在电脑、平板或手机上播放。AWS 提供了很多预设的格式，你不用自己调参数，直接选择对应的设备就可以了。</p><p>根据转码的时间以及分辨率收费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;SQS-Simple-Queue-Service&quot;&gt;&lt;a href=&quot;#SQS-Simple-Queue-Service&quot; class=&quot;headerlink&quot; title=&quot;SQS - Simple Queue Service&quot;&gt;&lt;/a&gt;SQS - Simple Queue Service&lt;/h2&gt;&lt;p&gt;SQS 是 AWS 的消息队列服务，用于暂存消息并等待接收者处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不保证 FIFO，可能乱序到达&lt;/li&gt;
&lt;li&gt;Visibility Timeout 最长 12 小时&lt;/li&gt;
&lt;li&gt;保证每条消息至少被传递一次（At least once），这意味着某些情况下可能传递多次，所以你写程序的时候要注意处理重复的消息&lt;/li&gt;
&lt;li&gt;每条消息最大 256 KB&lt;ul&gt;
&lt;li&gt;然而，依据 64 KB 的 Chunk 数量计费&lt;/li&gt;
&lt;li&gt;所以一个 256 KB 的消息可能产生 4 次费用：4 * 64 KB chunks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SQS 的消息没有优先级；如果你需要优先级，最佳实践是创建多个 SQS 队列&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
  <entry>
    <title>AWS 学习笔记之 VPC</title>
    <link href="https://ericfu.me/aws-notes-vpc/"/>
    <id>https://ericfu.me/aws-notes-vpc/</id>
    <published>2017-04-09T18:36:00.000Z</published>
    <updated>2017-04-09T22:57:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h2><ul><li>把 VPC 想象成一个逻辑上的数据中心</li><li>包含一个 IGW （Internet Gateway）或者 Virtual Private Gateway，Route Tables，Network ACLs，Subnets，Security Groups</li><li>1 个 Subnet = 1 个可用区</li><li>Security Group 是有状态的，Network ACL 是无状态的</li><li>VPC 可以连接（peer）起来，甚至可以连接不同 AWS 账号的 VPC</li><li>不能 transitive peer！如果 A 和 B 相连，B 和 C 相连，A 和 C 是<strong>不</strong>联通的，必须手动连接 A 和 C</li></ul><a id="more"></a><h3 id="NAT-Instance"><a href="#NAT-Instance" class="headerlink" title="NAT Instance"></a>NAT Instance</h3><ul><li>创建 NAT instance 的时候要关掉 Source/Destination Check</li><li>NAT instance 必须放在 public subnet 里</li><li>必须有一个 Elastic IP</li><li>必须有一个从 private subnet 到 NAT instance 的路由</li><li>NAT instance 支持的流量取决于 instance size，如果不够用只能增加 instance size</li><li>如果要 HA，可以利用 Autoscaling Group 为不同 AZ 的 subnet 创建 NAT instance</li></ul><h3 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h3><ul><li>很新，很可能不出现在考试中</li><li>可以自动伸缩，最大支持 10Gbps</li><li>不用管补丁</li><li>不用管 security group</li><li>不用手工禁用 Source/Destination Check</li><li>自动分配 IP 地址</li><li>记得要更新 Route table</li></ul><h3 id="Network-ACLs"><a href="#Network-ACLs" class="headerlink" title="Network ACLs"></a>Network ACLs</h3><ul><li>VPC 创建的时候会自动创建一个 default network ACL，允许所有 outbound 和 inbount 流量</li><li>你可以创建自定义的 network ACL，默认情况下，新创建的 network ACL 阻止所有连接（为了安全考虑）</li><li>VPC 里的每个 subnet 必须要指定一个 network ACL，如果你没有显式的指定 network ACL，那就是用 default network ACL</li><li>一个 subnet 只能指定一个 network ACL；但 network ACL 可以被指定给多个 subnet。注意，当你把一个 network ACL 指定给某个 subnet 的时候，subnet 之前设定的 network ACL 就被挤掉了，不能共存（以上两点和 Route table 很相似）</li><li>Network ACL 的每条 rule 都有一个序号，序号决定了 rule 执行的循序</li><li>Network ACL 包含两张表：inbound rules 和 outbound rules，每个 rule 可以是 allow 或者 deny</li><li>Network ACL 是无状态的。也就是说，对允许进入的流量也可能会被拒绝出去，反之亦然（区别于 security group）</li><li>可以用 Network ACL 来 block 某些 IP 地址（段），security group 则不行</li></ul><h3 id="NAT-vs-Bastions"><a href="#NAT-vs-Bastions" class="headerlink" title="NAT vs Bastions"></a>NAT vs Bastions</h3><ul><li>NAT 用来给 private subnet 里的机器提供 internet 访问</li><li>Bastion 用来安全地管理 private subnet 里的机器，也可以称为跳板机</li></ul><h3 id="容灾架构"><a href="#容灾架构" class="headerlink" title="容灾架构"></a>容灾架构</h3><ul><li>如果你想保证容灾性，至少保证 2 个 public subnets 和 2 个 private subnets，保证它们不在一个可用区</li><li>保证 ELB 横跨你的多个可用区</li><li>对于 Bastion instance，把它放在 autoscaling group 里，保证至少有 2 个节点工作，用 Route53 来做 fail over</li><li>NAT instance 就比较麻烦了，每个 public subnet 里需要放一个，各自分配一个 IP 地址，而且你要写一个脚本来做 fail over。如果可能的话，用 NAT gateway 来代替</li></ul><h3 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h3><p>用来监控 VPC 里的网络流量。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;VPC&quot;&gt;&lt;a href=&quot;#VPC&quot; class=&quot;headerlink&quot; title=&quot;VPC&quot;&gt;&lt;/a&gt;VPC&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;把 VPC 想象成一个逻辑上的数据中心&lt;/li&gt;
&lt;li&gt;包含一个 IGW （Internet Gateway）或者 Virtual Private Gateway，Route Tables，Network ACLs，Subnets，Security Groups&lt;/li&gt;
&lt;li&gt;1 个 Subnet = 1 个可用区&lt;/li&gt;
&lt;li&gt;Security Group 是有状态的，Network ACL 是无状态的&lt;/li&gt;
&lt;li&gt;VPC 可以连接（peer）起来，甚至可以连接不同 AWS 账号的 VPC&lt;/li&gt;
&lt;li&gt;不能 transitive peer！如果 A 和 B 相连，B 和 C 相连，A 和 C 是&lt;strong&gt;不&lt;/strong&gt;联通的，必须手动连接 A 和 C&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
  <entry>
    <title>AWS 学习笔记之数据库</title>
    <link href="https://ericfu.me/aws-notes-database/"/>
    <id>https://ericfu.me/aws-notes-database/</id>
    <published>2017-04-05T06:42:21.000Z</published>
    <updated>2017-04-05T06:47:59.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="AWS-提供的数据库"><a href="#AWS-提供的数据库" class="headerlink" title="AWS 提供的数据库"></a>AWS 提供的数据库</h3><ul><li>RDS - OLTP<ul><li>SQL Server</li><li>MySQL</li><li>PostgreSQL</li><li>Oracle</li><li>Aurora</li><li>MariaDB</li></ul></li><li>DynamoDB - NoSQL</li><li>RedShift - OLAP</li><li>Elasticache - In Memory Caching<ul><li>Memcached</li><li>Redis</li></ul></li></ul><a id="more"></a><h3 id="RDS-Relational-Database-Service"><a href="#RDS-Relational-Database-Service" class="headerlink" title="RDS - Relational Database Service"></a>RDS - Relational Database Service</h3><p>RDS 提供一系列关系型数据库的托管服务。</p><p>RDS 的两种 Replica 方式的区别：</p><ul><li><strong>Multi-AZ</strong>：备份数据到多个可用区， 当一个可用区 fail 时，会自动 failover 到另一个可用区</li><li><strong>Read Replica</strong>：创建只读的 Replica 数据库，减轻单个 DB instance 的读取压力。一个 instance 至多可以有 5 个 read replica</li></ul><h3 id="Aurora-数据库"><a href="#Aurora-数据库" class="headerlink" title="Aurora 数据库"></a>Aurora 数据库</h3><p>Aurora 是 AWS 提供的 MySQL 兼容的关系型数据库。</p><ul><li>每个可用区有 2 份拷贝；考虑到至少有 3 个可用区，数据至少有 6 份拷贝</li><li>Aurora 能自动处理不多余 2 份拷贝的数据丢失，而不影响写可用性；最多容忍不多余 3 份的数据丢失，而不影响读可用性。</li><li>Aurora 的存储具有自我修复（self-healing）功能，它不断扫描数据块和磁盘，如果有错误就自动修复。</li></ul><p>有两种 Aurora Replicas 可供选择：</p><ul><li>Aurora Replicas (至多 15 个)</li><li>MySQL Read Replicas (至多 5 个)</li></ul><h3 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h3><p>DynamoDB 是 AWS 提供的 NoSQL 数据库服务。它有以下特性：</p><ul><li>数据存储在 SSD 介质上</li><li>跨 3 个地理隔离的数据中心</li><li>最终一致性读（默认，高性能）</li><li>支持强一致性读（可选）</li></ul><p>DynamoDB 相比于 RDS 具有很强的<strong>伸缩性</strong>，可以随时 scale 数据库而不用关机；RDS 就没这么容易了，你可能要换更大的 instance，或者增加 read replica。</p><h3 id="Redshift-数据库"><a href="#Redshift-数据库" class="headerlink" title="Redshift 数据库"></a>Redshift 数据库</h3><ul><li>单节点模式：最大 160GB</li><li>多节点模式：<ul><li>Leader Node: 管理客户端连接、接受查询</li><li>Compute Node: 存储数据、执行查询和计算任务，最多 128 个</li></ul></li></ul><h3 id="Elasticache-缓存服务"><a href="#Elasticache-缓存服务" class="headerlink" title="Elasticache 缓存服务"></a>Elasticache 缓存服务</h3><p>Elasticache 通过在内存中缓存信息，来减轻应用程序访问数据库的负担。它目前支持两种 caching engines:</p><ul><li>Memcached</li><li>Redis</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;AWS-提供的数据库&quot;&gt;&lt;a href=&quot;#AWS-提供的数据库&quot; class=&quot;headerlink&quot; title=&quot;AWS 提供的数据库&quot;&gt;&lt;/a&gt;AWS 提供的数据库&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RDS - OLTP&lt;ul&gt;
&lt;li&gt;SQL Server&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;li&gt;Oracle&lt;/li&gt;
&lt;li&gt;Aurora&lt;/li&gt;
&lt;li&gt;MariaDB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DynamoDB - NoSQL&lt;/li&gt;
&lt;li&gt;RedShift - OLAP&lt;/li&gt;
&lt;li&gt;Elasticache - In Memory Caching&lt;ul&gt;
&lt;li&gt;Memcached&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="aws" scheme="https://ericfu.me/tags/aws/"/>
    
  </entry>
  
  <entry>
    <title>译：TensorFlow 与深度学习</title>
    <link href="https://ericfu.me/tfdl-contents/"/>
    <id>https://ericfu.me/tfdl-contents/</id>
    <published>2017-04-04T22:05:54.000Z</published>
    <updated>2017-08-22T00:28:31.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2017/04/tensorflow-and-deep-learning-cover.png" alt="tensorflow-and-deep-learning-cove"></p><blockquote><p>花了一周翻译了 Google Cloud Platform 上的教学课程 <em>TensorFlow and deep learning, without a PhD</em>，分享给大家。</p></blockquote><p>这篇文章对于初学者是很友好的，即使你对机器学习没有任何掌握也能跟着教程做完全部课程，并且 TensorFlow 的接口很简单，甚至不用掌握矩阵运算也能轻松使用。当然，如果你有一些线性代数和机器学习的先验知识，很多概念都是相通的，理解起来要更容易。</p><a id="more"></a><p>有些专有词汇在现实讨论中就是用英文，文中也酌情保留了。</p><ol><li><a href="https://ericfu.me/tensorflow-overview/">概述</a></li><li><a href="https://ericfu.me/tfdl-install-tensorflow/">安装 TensorFlow</a></li><li><a href="https://ericfu.me/tensorflow-3-train-neural-network/">训练神经网络</a></li><li><a href="https://ericfu.me/tfdl-4-one-layer-neural-network/">单层神经网络</a></li><li><a href="https://ericfu.me/tfdl-5-gradient-descent/">梯度下降</a></li><li><a href="https://ericfu.me/tfdl-6-jump-into-the-code/">阅读代码</a></li><li><a href="https://ericfu.me/tfdl-7-add-layers/">增加 Layer</a></li><li><a href="https://ericfu.me/tfdl-8-special-care-for-deep-networks/">特别注意之处</a></li><li><a href="https://ericfu.me/tfdl-9-learning-rate-decay/">学习率衰减</a></li><li><a href="https://ericfu.me/tfdl-10-dropout-and-overfitting/">Dropout 和过拟合</a></li><li><a href="https://ericfu.me/tfdl-11-convolutional-network/">卷积网络：理论</a></li><li><a href="https://ericfu.me/tfdl-12-convolutional-network-lab/">卷积网络：实验</a></li><li><a href="https://ericfu.me/tfdl-13-the-99-percent-challenge/">挑战 99% 准确率</a></li><li><a href="https://ericfu.me/tfdl-14-congratulations/">恭喜！</a></li></ol><p>或者，用 GitBook 阅读，点<a href="https://www.gitbook.com/book/fuyufjh/tensorflow-and-deep-learning-chinese" target="_blank" rel="external">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/2017/04/tensorflow-and-deep-learning-cover.png&quot; alt=&quot;tensorflow-and-deep-learning-cove&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;花了一周翻译了 Google Cloud Platform 上的教学课程 &lt;em&gt;TensorFlow and deep learning, without a PhD&lt;/em&gt;，分享给大家。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇文章对于初学者是很友好的，即使你对机器学习没有任何掌握也能跟着教程做完全部课程，并且 TensorFlow 的接口很简单，甚至不用掌握矩阵运算也能轻松使用。当然，如果你有一些线性代数和机器学习的先验知识，很多概念都是相通的，理解起来要更容易。&lt;/p&gt;
    
    </summary>
    
    
      <category term="selected" scheme="https://ericfu.me/tags/selected/"/>
    
      <category term="tensorflow" scheme="https://ericfu.me/tags/tensorflow/"/>
    
  </entry>
  
</feed>
